var store = [{
        "title": "우분투(Ubuntu) 환경에 패키지(package)로 OpenJDK(Java) 설치하기",
        "excerpt":"OpenJDK는 Java 애플리케이션(application) 구축을 위한 오픈소스(open source) 기반의 JDK(Java Development Kit)입니다.    JDK는 JVM(Java Virtual Machine), JRE(Java Runtime Environment)와 함께 Java 프로그래밍에 필요한 핵심 기술 패키지(package)입니다. JDK는 2개로 나뉘는데, 하나는 BCL(Oracle Binary Code License)의 Oracle JDK, 하나는 GNU GPL v2(GNU General Public License)의 OpenJDK입니다.    이 포스트에서는 우분투(Ubuntu) 환경에서 package로 OpenJDK를 설치하는 방법을 소개합니다.      Oracle JDK와 OpenJDK에 대한 자세한 정보는 Oracle JDK와 OpenJDK의 차이점 포스트를 참고하시기 바랍니다.    선행조건(PREREQUISITE)     Ubuntu 환경이 필요합니다.      Ubuntu 설치 방법은 우분투(Ubuntu) 서버(Server) 16.04 설치하기 또는 우분투(Ubuntu) 서버(Server) 18.04 설치하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   Ubuntu 16.04.4 LTS (Xenial Xerus) Server (64-bit)   OpenJDK 1.8.0_222   요약(SUMMARY)     apt 명령어로 OpenJDK 설치   OpenJDK 설치 확인   Ubuntu 환경변수에 OpenJDK Java 경로 설정   (선택사항) apt 명령어로 OpenJDK 삭제   내용(CONTENTS)  1. apt 명령어로 OpenJDK 설치  $ sudo apt update &amp;&amp; sudo apt install openjdk-8-jdk -y   2. OpenJDK 설치 확인  $ java -version openjdk version \"1.8.0_222\" OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10) OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode)   3. Ubuntu 환경변수에 OpenJDK Java 경로(path) 설정  3.1. 설치된 Java path 확인  $ which java /usr/bin/java   3.2. OpenJDK Java path 추가     루트(root) 계정    $ sudo vi /etc/profile 또는 # vi $HOME/.bashrc      일반 계정    $ vi $HOME/.profile      root 계정으로 OpenJDK Java path를 추가합니다.   $ sudo vi /etc/profile   ---------------------------------------------------------------------------------------------------- JAVA_HOME=$(dirname $(dirname $(update-alternatives --list javac))) JAVA=${JAVA_HOME}/bin CLASSPATH=.:${JAVA_HOME}/lib/tools.jar PATH=${PATH}:${JAVA}  export JAVA_HOME JAVA CLASSPATH PATH ----------------------------------------------------------------------------------------------------      수정 내역 적용을 위해 아래 명령어를 입력합니다.   $ source /etc/profile   4. (선택사항) apt 명령어로 OpenJDK 삭제     ’–auto-remove’ 옵션을 추가하면, 사용하지 않는 관련 package를 모두 삭제합니다.   4.1. apt remove 명령어로 OpenJDK 삭제     설정 파일을 유지하며 OpenJDK를 삭제합니다.   $ sudo apt remove openjdk* $ sudo apt remove --auto-remove openjdk*   4.2. apt purge 명령어로 OpenJDK 삭제     설정 파일과 함께 OpenJDK를 삭제합니다. (단, 사용자 홈 디렉터리의 설정 파일은 유지됩니다.)   $ sudo apt purge openjdk* $ sudo apt purge --auto-remove openjdk*   마무리(CONCLUSION)  Ubuntu 환경에 package로 OpenJDK 설치를 완료했습니다.    JDK는 Java 프로그래밍에 필수 요소이며, 상당수의 open source 소프트웨어와 국내 SI(system integration) 프로젝트에서 JDK를 요구합니다.    정부 발주 프로젝트에 다수를 차지하는 SI 회사가 Java와 스프링 프레임워크(spring framework) 또는 Java와 전자정부표준프레임워크(eGov) 구성을 사용하기 때문에 Java의 인기는 시들지 않고 있습니다.    Java는 플랫폼에 독립적이고 수많은 개발자와 레퍼런스를 보유하고 있다는 장점과 속도 문제라는 단점을 가진 언어입니다.    국내 현업에서는 Java를 비롯한 여러 가지 언어가 자주 사용되므로, 개발 환경이나 작업 특성에 따라 적합한 언어를 선택할 수 있는 지식과 노하우가 필요합니다.   참고(REFERENCES)     https://openjdk.java.net/   https://ko.wikipedia.org/wiki/OpenJDK   https://namu.wiki/w/Java  ","categories": ["ubuntu"],
        "tags": ["openjdk","java","open source software","open source","oss","ubuntu"],
        "url": "https://lindarex.github.io/ubuntu/ubuntu-openjdk-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "Oracle JDK와 OpenJDK의 차이점",
        "excerpt":"Java 애플리케이션 구축을 위해서는 JDK(Java Development Kit)가 필수입니다.    이 포스트에서는 Oracle JDK와 OpenJDK의 차이점을 간단히 소개합니다.   내용(CONTENTS)  Oracle JDK와 OpenJDK의 차이점     Oracle JDK는 상용(유료)이지만, OpenJDK는 오픈소스기반(무료)입니다.   Oracle JDK의 라이선스는 Oracle BCL(Binary Code License) Agreement이지만, OpenJDK의 라이선스는 Oracle GPL v2입니다.   Oracle JDK는 LTS(장기 지원) 업데이트 지원을 받을 수 있지만, OpenJDK는 LTS 없이 6개월마다 새로운 버전이 배포됩니다.   Oracle JDK는 Oracle이 인수한 Sun Microsystems 플러그인을 제공하지만, OpenJDK는 제공하지 않습니다.   Oracle JDK는 OpenJDK 보다 CPU 사용량과 메모리 사용량이 적고, 응답시간이 높습니다.      Oracle JDK와 OpenJDK의 벤치마킹 결과는 Comparing JDK 8 performance 페이지를 참고하시기 바랍니다.    마무리(CONCLUSION)  Oracle은 2019년 1월부터 Java를 상용으로 사용하거나 지속적인 업데이트를 받으려는 기업 사용자에게 유료로 제공하고 있으며,  현재 2020년 2월 기준으로 Oracle JDK 8은 개인 또는 개발 용도의 사용은 무료지만, 2021년부터는 유료화로 전환될 것으로 알려져 있습니다.    JDK 유료화에 따라 개인 사용자를 비롯해 기업 사용자는 OpenJDK를 적용하기 위해 호환성, 안정성, 성능 등의 검증이 필요합니다. 또한, 배포 버전에 따른 생산성 확인과 보안 이슈 등을 충분히 인지하고 테스트할 필요가 있습니다.    더 자세한 내용은 아래 참고 페이지를 확인해 주시기 바랍니다.   참고(REFERENCES)     https://engineering.linecorp.com/ko/blog/line-open-jdk/   https://c10106.tistory.com/4075   https://jsonobject.tistory.com/395  ","categories": ["concepts"],
        "tags": ["oraclejdk","openjdk","open source software","open source","oss","java"],
        "url": "https://lindarex.github.io/concepts/difference-between-oraclejdk-openjdk/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 방화벽(Firewalld) 설치 및 설정하기",
        "excerpt":"방화벽(firewalld)은 IPv4, IPv6, 이더넷 브리지 및 IPSet의 방화벽(firewall) 설정을 지원하는 리눅스(linux) firewall 관리 도구이며, linux 커널 netfilter 프레임워크의 프런트 엔드 역할을 하는 RHEL 7 제품군의 기본 firewall 관리 소프트웨어입니다.    우분투(ubuntu)의 기본 firewall 시스템은 UFW(uncomplicated firewall)이지만, firewalld를 설치하여 사용할 수 있습니다.    이 포스트에서는 ubuntu 환경에서 패키지로 firewalld를 설치하고 설정하는 방법을 소개합니다.      UFW 설정 방법은 우분투(Ubuntu) 환경에 방화벽(UFW) 설정하기 포스트를 참고하시기 바랍니다.    선행조건(PREREQUISITE)     ubuntu 환경이 필요합니다.      ubuntu 설치 방법은 우분투(Ubuntu) 서버(Server) 16.04 설치하기 또는 우분투(Ubuntu) 서버(Server) 18.04 설치하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   Ubuntu 16.04.4 LTS (Xenial Xerus) Server (64-bit)   요약(SUMMARY)     apt 명령어로 firewalld 설치   firewalld 설치 확인   firewalld 설정   내용(CONTENTS)  1. apt 명령어로 firewalld 설치  $ sudo apt update &amp;&amp; sudo apt install firewalld -y   2. firewalld 설치 확인  $ sudo firewall-cmd --version 0.4.4.5   3. firewalld 설정  3.1. firewalld에  rule 추가     아래 예제는 영구적으로 public zone에 TCP 8080 포트를 추가하는 명령어입니다.    $ sudo firewall-cmd --permanent --zone=public --add-port=8080/tcp   3.2. firewalld에 rule 적용     아래 명령어를 실행하기 전에는 추가한 rule이 적용되지 않습니다.    $ sudo firewall-cmd --reload   3.3. firewalld에 설정된 모든 값 조회  $ sudo firewall-cmd --list-all public   target: default   icmp-block-inversion: no   interfaces:   sources:   services: ssh dhcpv6-client   ports: 8080/tcp   protocols:   masquerade: no   forward-ports:   source-ports:   icmp-blocks:   rich rules:   마무리(CONCLUSION)  ubuntu 환경에 패키지로 firewalld 설치 및 설정을 완료했습니다.    firewalld는 CentOS 7부터 이전의 Iptables를 대체해 새롭게 선보인 패킷 필터링 firewall 프로그램입니다.   다음 포스트에서는 우분투(Ubuntu) 환경에 방화벽(UFW) 설정하기를 소개하겠습니다.   참고(REFERENCES)     http://manpages.ubuntu.com/manpages/bionic/man1/firewall-cmd.1.html   https://computingforgeeks.com/install-and-use-firewalld-on-ubuntu-18-04-ubuntu-16-04/  ","categories": ["ubuntu"],
        "tags": ["firewalld","firewall","open source software","open source","oss","ubuntu"],
        "url": "https://lindarex.github.io/ubuntu/ubuntu-firewalld-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 서버(Server) 16.04 설치하기",
        "excerpt":"우분투(ubuntu)는 데비안(debian) 리눅스(linux) 기반으로, debian에 비해 사용자 편의성에 초점을 맞춘 linux 배포판이며 컴퓨터 운영체제(OS, operating system)입니다.    새로운 버전은 6개월마다, 장기 지원 버전(LTS, long term support)은 2년에 한 번씩 출시되고, 다양한 언어를 지원하고 낮은 사양의 컴퓨터에서도 작동하도록 설계되어 있습니다.    이 포스트에서는 VMware Workstation에 ubuntu server 16.04 LTS를 설치하는 방법을 소개합니다.   선행조건(PREREQUISITE)     VMware Workstation이 설치되어 있어야 합니다.   테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   Ubuntu 16.04.6 LTS (Xenial Xerus) Server (64-bit)      ubuntu server 16.04 설치 시 필요한 시스템 요구사항은 https://help.ubuntu.com/16.04/serverguide/preparing-to-install.html를 확인해 주시기 바랍니다.    요약(SUMMARY)     ubuntu server 16.04.6 ISO 파일 내려받기   VMware workstation의 VM(virtual machine) 설정   ubuntu server 16.04.6 설치   linux 명령어로 ubuntu VM 상태 확인   내용(CONTENTS)  1. ubuntu server 16.04.6 ISO 파일 내려받기     웹브라우저로 ubuntu Releases 페이지를 엽니다.         목록의 ‘16.04.6’를 클릭하여 ubuntu 16.04.6 LTS (Xenial Xerus) 페이지로 이동합니다.         Server install image 영역의 ‘64-bit PC (AMD64) server install image’를 클릭하여 ISO 파일을 내려받습니다.      http://mirror.kakao.com/ubuntu-releases/16.04.6/ubuntu-16.04.6-server-amd64.iso를 통해 바로 내려받을 수 있습니다.   ubuntu-16.04.6-server-amd64.iso 파일 사이즈는 약 893 MB입니다.       2. VMware workstation의 VM(Virtual Machine) 설정     VMware workstation을 실행합니다.   아래 메뉴를 통해 ‘New Virtual Machine Wizard’ 팝업을 엽니다.      상단 메뉴 &gt; File &gt; ‘New Virtual Machine…’ 또는 Ctrl + N          상세 설정을 위해 ‘Custom (advanced)’를 선택하고 ‘Next’를 클릭합니다.         VMware Workstation 버전을 선택하고 ‘Next’를 클릭합니다.         상세 설정을 위해 ‘I will install the operating system later.’를 선택하고 ‘Next’를 클릭합니다.         Guest operating system은 ‘Linux’, Version은 ‘ubuntu 64-bit’를 선택하고 ‘Next’를 클릭합니다.         Virtual machine name을 입력하고, Location에 vmdk 파일이 저장될 경로를 입력하거나 ‘Browse..’를 클릭해 위치를 선택하고 ‘Next’를 클릭합니다.      VMDK란 Virtual Machine Disk의 약자이며, 자세한 정보는 https://en.wikipedia.org/wiki/VMDK를 참고하시기 바랍니다.          processors ‘1’, cores per processor ‘1’을 입력하고 ‘Next’를 클릭합니다.         권장값인 ‘2048’MB를 입력하고 ‘Next’를 클릭합니다.      권장값은 자신의 PC 메모리(Host memory)에 따라 다를 수 있습니다.          ‘Use network address translation (NAT)’를 선택하고 ‘Next’를 클릭합니다.      Network type에 대한 자세한 정보는 VMware Workstation의 가상 네트워크(Virtual Network) 알아보기 포스트를 참고하시기 바랍니다.          권장값인 ‘LSI Logic (Recommended)’를 선택하고 ‘Next’를 클릭합니다.         권장값인 ‘SCSI Logic (Recommended)’를 선택하고 ‘Next’를 클릭합니다.         ‘Create a new virtual disk’를 선택하고 ‘Next’를 클릭합니다.         Maximum disk size를 ‘20.0’을 입력하고 ‘Split virtual disk into multiple files’를 선택하고 ‘Next’를 클릭합니다.         Disk file 명을 입력하고 ‘Next’를 클릭합니다.         ‘Customize Hardware..’를 클릭하여 Hardware 팝업을 엽니다.         앞에서 설정한 내용을 확인합니다. (Memory, Processors)            Use ISO image file 항목의 ‘Browse..’를 클릭해 내려받은 ubuntu 16.04.6 LTS (Xenial Xerus) ISO 파일을 선택합니다.         앞에서 설정한 내용을 확인합니다. (Network Adapter, USB Controller, Sound Card, Printer, Display)                     모든 설정을 확인하고 ‘Finish’를 클릭합니다.      3. ubuntu server 16.04.6 설치     아래 메뉴를 통해 ubuntu VM을 구동합니다.      상단 메뉴 &gt; 녹색 플레이 버튼 또는 Ctrl + B          설치할 때 사용하고자 하는 언어를 선택합니다.         ‘Install ubuntu Server’를 선택하여 설치를 시작합니다.         ubuntu Server에 사용하고자 하는 언어를 설정합니다. ‘English’를 선택합니다.         Location을 설정합니다. ‘other’를 선택한 후, ‘Asia’의 ‘Korea, Republic of’를 선택합니다.               Locale을 설정합니다. ‘United States’를 선택합니다.         Keyboard layout을 설정합니다. ‘No’를 선택한 후, ‘English (US)’를 선택합니다.               설치가 진행됩니다.         호스트네임(Hostname)을 입력합니다.         사용자 이름(User name)을 입력합니다.            비밀번호(Password)를 입력합니다.            Home directory의 암호화를 설정합니다. ‘No’를 선택합니다.         설치가 진행됩니다.         타임존(Time zone)을 설정합니다.         Disk partition을 설정합니다. ‘Guided - use entire disk and set up LVM’을 선택합니다.                     설치가 진행됩니다.         HTTP proxy를 설정합니다.         설치가 진행됩니다.         Upgrade 주기를 설정합니다.         추가로 설치할 패키지를 선택합니다.      Xshell을 비롯해 SSH 클라이언트 사용을 위해 OpenSSH server 패키지를 추가합니다.          설치가 진행됩니다.         GRUB boot loader를 설정합니다. ‘Yes’를 선택합니다.         설치가 진행됩니다.         설치 과정의 마지막입니다. ‘Continue’를 선택합니다.         재부팅됩니다.         정상 설치 후 로그인 프롬프트 상태입니다.      4. linux 명령어(command)로 ubuntu VM 상태 확인      사용자 이름과 비밀번호를 입력하여 로그인합니다.         free command로 메모리 상태를 확인합니다.         df command로 디스크 사용량을 확인합니다.         ifconfig command로 네트워크 정보를 확인합니다.         ping command로 네트워크 상태를 확인합니다.         ‘apt update’ command로 패키지 인덱스를 업데이트합니다.      ‘apt update’ command는 업그레이드 가능한 패키지 정보를 업데이트하는 것이며, 실제로 패키지를 업데이트하지 않습니다.          ‘apt upgrade’ command로 업그레이드 가능한 모든 패키지를 업데이트합니다.            마무리(CONCLUSION)  VMware Workstation에 Ubuntu 16.04.6 LTS (Xenial Xerus) Server 설치를 완료했습니다.    2020년 2월 12일 W3Techs.com 통계에 따르면, ubuntu 점유율은 linux 배포판을 사용하는 웹사이트 중 1위(38.9%)로, 2위인 debian(18.4%)의 2배 이상 차이가 납니다. 또한 대부분의 오픈소스 소프트웨어는 Ubuntu를 지원하고, 국내외 사용자와 레퍼런스가 많습니다.    linux 입문자 또는 오픈소스 소프트웨어에 관심이 있다면, Ubuntu를 사용하며 linux command를 숙지하고 다양한 커뮤니티에서 활동하는 것을 권합니다.   참고(REFERENCES)     https://ubuntu.com/   http://mirror.kakao.com/ubuntu-releases/   https://ubuntu.com/tutorials/tutorial-install-ubuntu-server#1-overview   https://ko.wikipedia.org/wiki/우분투_(운영_체제)   ","categories": ["ubuntu"],
        "tags": ["ubuntu","16.04","vmware workstation","open source software","open source","oss"],
        "url": "https://lindarex.github.io/ubuntu/ubuntu-1604-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 서버(Server) 18.04 설치하기",
        "excerpt":"우분투(ubuntu) server 18.04는 ubuntu server 16.04와 비교해 새로운 기능과 개선 사항으로 많은 변화가 있으며, 릴리스의 코드 이름은 ‘Bionic Beaver’입니다.    이 포스트에서는 VMware Workstation에 ubuntu server 18.04 LTS를 설치하는 방법을 소개합니다.   선행조건(PREREQUISITE)     VMware Workstation이 설치되어 있어야 합니다.   테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   Ubuntu 18.04.3 LTS (Bionic Beaver) Server (64-bit)      ubuntu server 18.04 설치 시 필요한 시스템 요구사항은 https://help.ubuntu.com/lts/serverguide/preparing-to-install.html를 확인해 주시기 바랍니다.    요약(SUMMARY)     ubuntu server 18.04.3 ISO 파일 내려받기   VMware workstation의 VM(virtual machine) 설정   ubuntu server 18.04.3 설치   리눅스 명령어로 ubuntu VM 상태 확인   내용(CONTENTS)  1. ubuntu server 18.04.3 ISO 파일 내려받기     웹브라우저로 ubuntu Releases 페이지를 엽니다.         목록의 ‘18.04.3’을 클릭하여 ubuntu 18.04.3 LTS (Bionic Beaver) 페이지로 이동합니다.         Server install image 영역의 ‘64-bit PC (AMD64) server install image’를 클릭하여 ISO 파일을 내려받습니다.      http://mirror.kakao.com/ubuntu-releases/18.04.3/ubuntu-18.04.3-live-server-amd64.iso를 통해 바로 내려받을 수 있습니다.   ubuntu-18.04.3-live-server-amd64.iso 파일 사이즈는 약 868 MB입니다.       2. VMware workstation의 VM(Virtual Machine) 설정     VMware workstation을 실행합니다.   아래 메뉴를 통해 ‘New Virtual Machine Wizard’ 팝업을 엽니다.      상단 메뉴 &gt; File &gt; ‘New Virtual Machine…’ 또는 Ctrl + N          상세 설정을 위해 ‘Custom (advanced)’를 선택하고 ‘Next’를 클릭합니다.         VMware Workstation 버전을 선택하고 ‘Next’를 클릭합니다.         상세 설정을 위해 ‘I will install the operating system later.’를 선택하고 ‘Next’를 클릭합니다.         Guest operating system은 ‘Linux’, Version은 ‘ubuntu 64-bit’를 선택하고 ‘Next’를 클릭합니다.         Virtual machine name을 입력하고, Location에 vmdk 파일이 저장될 경로를 입력하거나 ‘Browse..’를 클릭해 위치를 선택하고 ‘Next’를 클릭합니다.      VMDK란 Virtual Machine Disk의 약자이며, 자세한 정보는 https://en.wikipedia.org/wiki/VMDK를 참고하시기 바랍니다.          processors ‘1’, cores per processor ‘1’을 입력하고 ‘Next’를 클릭합니다.         권장값인 ‘2048’MB를 입력하고 ‘Next’를 클릭합니다.      권장값은 자신의 PC 메모리(Host memory)에 따라 다를 수 있습니다.          ‘Use network address translation (NAT)’를 선택하고 ‘Next’를 클릭합니다.      Network type에 대한 자세한 정보는 VMware Workstation의 가상 네트워크(Virtual Network) 알아보기 포스트를 참고하시기 바랍니다.          권장값인 ‘LSI Logic (Recommended)’를 선택하고 ‘Next’를 클릭합니다.         권장값인 ‘SCSI Logic (Recommended)’를 선택하고 ‘Next’를 클릭합니다.         ‘Create a new virtual disk’를 선택하고 ‘Next’를 클릭합니다.         Maximum disk size를 ‘20.0’을 입력하고 ‘Split virtual disk into multiple files’를 선택하고 ‘Next’를 클릭합니다.         Disk file 명을 입력하고 ‘Next’를 클릭합니다.         ‘Customize Hardware..’를 클릭하여 Hardware 팝업을 엽니다.         앞에서 설정한 내용을 확인합니다. (Memory, Processors)            Use ISO image file 항목의 ‘Browse..’를 클릭해 내려받은 ubuntu 18.04.3 LTS (Bionic Beaver) ISO 파일을 선택합니다.         앞에서 설정한 내용을 확인합니다. (Network Adapter, USB Controller, Sound Card, Printer, Display)                     모든 설정을 확인하고 ‘Finish’를 클릭합니다.      3. ubuntu server 18.04.3 설치     아래 메뉴를 통해 ubuntu VM을 구동합니다.      상단 메뉴 &gt; 녹색 플레이 버튼 또는 Ctrl + B          설치가 진행됩니다.         사용하고자 하는 언어를 선택합니다.         Keyboard layout을 설정합니다. ‘English (US)’를 선택합니다.         Network를 설정합니다.      VMware Workstation에 Ubuntu를 설치할 경우, 자동으로 DHCP(Dynamic Host Configuration Protocol)로 설정됩니다.   DHCP에 대한 자세한 정보는 https://ko.wikipedia.org/wiki/동적_호스트_구성_프로토콜를 참고하시기 바랍니다.          Proxy를 설정합니다.         ubuntu archive mirror address를 설정합니다.         Filesystem을 설정합니다. ‘Use An Entire Disk and Set Up LVM’을 선택합니다.                  사용자 프로필을 설정합니다.         SSH 설치 여부를 선택합니다.      Xshell을 비롯해 SSH 클라이언트 사용을 위해 OpenSSH server 패키지를 설치합니다.          추가로 설치할 패키지를 선택합니다.         설치가 진행됩니다.            설치가 완료되었습니다. ‘Reboot’를 선택합니다.         재부팅됩니다.         정상 설치 후 로그인 프롬프트 상태입니다.      4. 리눅스 명령어(command)로 ubuntu VM 상태 확인      사용자 이름과 비밀번호를 입력하여 로그인합니다.         free command로 메모리 상태를 확인합니다.         df command로 디스크 사용량을 확인합니다.         ifconfig command로 네트워크 정보를 확인합니다.         ping command로 네트워크 상태를 확인합니다.         ‘apt update’ command로 패키지 인덱스를 업데이트합니다.      ‘apt update’ command는 업그레이드 가능한 패키지 정보를 업데이트하는 것이며, 실제로 패키지를 업데이트하지 않습니다.          ‘apt upgrade’ command로 업그레이드 가능한 모든 패키지를 업데이트합니다.         마무리(CONCLUSION)  VMware Workstation에 Ubuntu 18.04.3 LTS (Bionic Beaver) Server 설치를 완료했습니다.    다음 포스트에서는 우분투(Ubuntu) 서버(Server) 초기 설정하기를 소개하겠습니다.   참고(REFERENCES)     https://ubuntu.com/   http://mirror.kakao.com/ubuntu-releases/   https://ubuntu.com/tutorials/tutorial-install-ubuntu-server#1-overview   https://ko.wikipedia.org/wiki/우분투_(운영_체제)   ","categories": ["ubuntu"],
        "tags": ["ubuntu","18.04","vmware workstation","open source software","open source","oss"],
        "url": "https://lindarex.github.io/ubuntu/ubuntu-1804-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "VMware Workstation의 가상 네트워크(Virtual Network) 알아보기",
        "excerpt":"VMware Workstation은 가상 머신(VM, virtual machine)을 생성하고 가상 네트워크(virtual network)를 구성할 수 있습니다.    이 포스트에서는 VMware Workstation의 virtual network에 대해 소개합니다.   선행조건(PREREQUISITE)     VMware Workstation이 설치되어 있어야 합니다.   테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   요약(SUMMARY)     Bridged Networking   NAT Networking   Host-Only Networking      내용(CONTENTS)  1. Bridged Networking     Bridged Networking은 호스트(host) 시스템(system)의 네트워크(network) 어댑터(adapter)를 사용하여 VM을 network에 연결합니다.   Bridged Networking을 적용한 VM은 물리적 system으로 인식되고, 공유기는 개별적으로 IP를 할당합니다.   공유기를 통해 외부 통신을 합니다.   동일한 network에 있다는 전제하에, Bridged Networking을 적용한 VM은 host system을 비롯해 동일 network 내의 다른 system에도 엑세스가 가능합니다.   VMware Workstation 설치 시 Bridged Network(VMnet0)가 설정됩니다.         출처 :: https://docs.vmware.com/en/VMware-Workstation-Player-for-Windows/15.0/com.vmware.player.win.using.doc/GUID-BAFA66C3-81F0-4FCA-84C4-D9F7D258A60A.html    2. NAT Networking     NAT(Network Address Translation) Networking을 적용한 VM은 외부 network 통신을 위한 IP를 할당받지 않습니다.   host system에 별도의 개인 network가 설정되고, NAT Networking을 적용한 VM은 가상 DHCP 서버(server)로부터 내부 network 대역을 할당받습니다.   host system을 통해 외부 통신을 합니다.   NAT Network는 하나만 존재합니다.   VMware Workstation 설치 시 NAT Network(VMnet8)가 설정됩니다.         출처 :: https://docs.vmware.com/en/VMware-Workstation-Player-for-Windows/15.0/com.vmware.player.win.using.doc/GUID-89311E3D-CCA9-4ECC-AF5C-C52BE6A89A95.html    3. Host-Only Networking     Host-Only Networking을 적용한 VM은 기본적으로 외부 통신, host system과 통신이 불가능합니다.   Host-Only Networking을 적용한 VM은 VMware Workstation에서 생성한 VM만 통신이 가능합니다.   VMware Workstation 설치 시 Host-Only Network(VMnet1)가 설정됩니다.         출처 :: https://docs.vmware.com/en/VMware-Workstation-Player-for-Windows/15.0/com.vmware.player.win.using.doc/GUID-93BDF7F1-D2E4-42CE-80EA-4E305337D2FC.html    마무리(CONCLUSION)  VMware Workstation의 virtual network에 대해 알아봤습니다.    위에 기재한 networking 개념은 VMware Workstation뿐만 아니라 모든 운영체제(OS, operating system), 특히 IaaS network 설정에 필요합니다.    대부분 로컬 테스트를 할 때, VMware Workstation의 virtual network 설정을 수정하는 경우는 드물지만, 클라우드(cloud) 기반 프로젝트에서는 기본적인 network 개념을 충분히 이해하고 테스트할 필요가 있습니다.    더 자세한 내용은 아래 참고 페이지를 확인해 주시기 바랍니다.   참고(REFERENCES)     https://docs.vmware.com/en/VMware-Workstation-Player-for-Windows/15.0/com.vmware.player.win.using.doc/GUID-D9B0A52D-38A2-45D7-A9EB-987ACE77F93C.html   ","categories": ["vmware-workstation"],
        "tags": ["vmware workstation","virtual network","bridged","nat","host-only"],
        "url": "https://lindarex.github.io/vmware-workstation/vmware-workstation-virtual-network/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 패키지(Package)로 젠킨스(Jenkins) 설치하기",
        "excerpt":"젠킨스(Jenkins)는 MIT license로 오픈소스(open source) 소프트웨어이며, 소프트웨어 개발 시 지속적 통합(CI, continuous integration) 서비스(service)를 제공하는 자동화 서버(server)입니다.    jenkins는 모든 프로젝트의 빌드(build)와 배포(deploy), 자동화를 지원하며, 수백 개의 플러그인(plugin)을 제공합니다.    이 포스트에서는 우분투(ubuntu) 환경에서 패키지(package)로 jenkins를 설치하는 방법을 소개합니다.   선행조건(PREREQUISITE)     ubuntu 환경에 Java가 설치되어 있어야 합니다.            jenkins 버전(version)에 따른 필요 Java version                    2.164 (2019-02) and newer: Java 8 or Java 11           2.54 (2017-04) and newer: Java 8           1.612 (2015-05) and newer: Java 7                           방화벽 설정이 필요합니다.            TCP 8080 포트가 개방되어 있어야 합니다.              Java 설치 방법은 우분투(Ubuntu) 환경에 OpenJDK(Java) 설치하기 포스트를 참고하시기 바랍니다.       jenkins version에 따른 Java version에 대한 자세한 정보는 https://jenkins.io/doc/administration/requirements/java/를 확인해 주시기 바랍니다.       방화벽 설정 방법은 우분투(Ubuntu) 환경에 방화벽(Firewalld) 설치 및 설정하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   Ubuntu 16.04.4 LTS (Xenial Xerus) Server (64-bit)   jenkins 2.121.1   OpenJDK 1.8.0_171   요약(SUMMARY)     jenkins debian packages repository 설정   apt 명령어로 jenkins 설치   (선택사항) Java 경로 설정 및 jenkins의 기본 포트 변경   systemctl 명령어로 jenkins 실행   웹브라우저로 jenkins 접속   내용(CONTENTS)  1. jenkins debian packages repository key 추가  $ sudo wget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add -   2. jenkins debian packages repository 추가  $ sudo sh -c 'echo deb https://pkg.jenkins.io/debian-stable binary/ &gt; /etc/apt/sources.list.d/jenkins.list'   3. apt install 명령어로 jenkins 설치  $ sudo apt update &amp;&amp; sudo apt install jenkins -y     4. (선택사항) Java 경로 설정  4.1. 설치된 Java 경로 확인  $ sudo which java /usr/bin/java   4.2. Java 경로 추가  $ sudo vi /etc/init.d/jenkins   -------------------------------------------------------------------------------- PATH=\"/bin:/usr/bin:/sbin:/usr/sbin:/usr/bin/java\"  --------------------------------------------------------------------------------   5. (선택사항) 기본 포트 변경  $ sudo vi /etc/default/jenkins     -------------------------------------------------------------------------------- JENKINS_PORT=\"8080\"   JENKINS_AJP_PORT=\"9091\"   JENKINS_USER=\"root\"   --------------------------------------------------------------------------------   6. systemctl 명령어로 jenkins service 관리  6.1. jenkins service 설정 반영  $ sudo systemctl daemon-reload   6.2. jenkins service 시작  $ sudo systemctl start jenkins.service   6.3. jenkins service 중지  $ sudo systemctl stop jenkins.service   6.4. jenkins service 재시작  $ sudo systemctl restart jenkins.service   6.5. jenkins service 설정 재적용  $ sudo systemctl reload jenkins.service   6.6. jenkins service 상태 조회  $ sudo systemctl status jenkins.service   6.7. jenkins service 활성화(부팅 시 자동 시작)  $ sudo systemctl enable jenkins.service   6.8. jenkins service 비활성화  $ sudo systemctl disable jenkins.service   6.9. jenkins service 및 관련 프로세스 모두 중지  $ sudo systemctl kill jenkins.service   7. 웹브라우저로 jenkins 접속     http://[MY-IP]:8080   마무리(CONCLUSION)  ubuntu 환경에 package로 jenkins 설치를 완료했습니다.    jenkins 초기 설정은 젠킨스(Jenkins) 초기 설정하기 포스트를 참고하시기 바랍니다.    다음 포스트에서는 우분투(Ubuntu) 환경에 WAR 파일로 젠킨스(Jenkins) 설치하기를 소개하겠습니다.   참고(REFERENCES)     https://jenkins.io/   http://pkg.jenkins-ci.org/debian-stable/  ","categories": ["jenkins"],
        "tags": ["jenkins","ci","continuous integration","open source software","open source","oss","ubuntu"],
        "url": "https://lindarex.github.io/jenkins/ubuntu-jenkins-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "젠킨스(Jenkins) 초기 설정하기",
        "excerpt":"이 포스트에서는 우분투(ubuntu) 환경에 젠킨스(Jenkins)를 설치한 후 초기 설정하는 방법을 소개합니다.   선행조건(PREREQUISITE)     ubuntu 환경에 Jenkins가 설치되어 있어야 합니다.      Jenkins 설치 방법은 우분투(Ubuntu) 환경에 패키지로 젠킨스(Jenkins) 설치하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     Ubuntu 16.04.6 LTS (Xenial Xerus) Server (64-bit)   OpenJDK 1.8.0_242   Jenkins 2.204.2   요약(SUMMARY)     Jenkins 접속 및 활성화   Jenkins 추천 플러인 설치   관리자(Admin) 계정 등록   Jenkins 접속 URL 확인   OpenJDK Java 설정   내용(CONTENTS)  1. Jenkins 접속      2. Jenkins 활성화     아래 명령어로 Jenkins 활성화에 필요한 암호(Password)를 조회합니다.   $ sudo cat /var/lib/jenkins/secrets/initialAdminPassword 91623591371f4f57bf6814a674bfeda9      3. Jenkins 플러인(plugin) 설치     ‘Install suggested plugins’를 선택합니다.         plugin 설치를 진행합니다.      4. 관리자(Admin) 계정 등록      5. Jenkins 접속 URL 확인     포트(port) 번호를 포함한 전체 URL을 기재합니다.         Jenkins 사용을 위한 준비를 완료했습니다.         초기 설정 완료 후 Jenkins 첫 페이지로 이동합니다.      6. OpenJDK Java 설정     아래 메뉴를 통해 ‘Global Tool Configuration’ 페이지로 이동합니다.      Jenkins &gt; 왼쪽 상단 메뉴 &gt; Jenkins 관리 &gt; Global Tool Configuration                  JDK 설정              ‘JDK Installations’ 버튼을 클릭한 후, ‘Add JDK’ 버튼을 클릭합니다.       ‘Install automatically’을 체크해제하여, JDK name과 JAVA_HOME을 기재합니다.                  JAVA_HOME은 아래 명령어로 조회할 수 있습니다.         $ echo $JAVA_HOME  /usr/lib/jvm/java-8-openjdk-amd64                     마무리(CONCLUSION)  Jenkins 설치 후 초기 설정을 완료했습니다.    다음 포스트에서는 우분투(Ubuntu) 환경에 WAR 파일로 젠킨스(Jenkins) 설치하기를 소개하겠습니다.   참고(REFERENCES)     https://jenkins.io/   ","categories": ["jenkins"],
        "tags": ["jenkins","ci","continuous integration","open source software","open source","oss"],
        "url": "https://lindarex.github.io/jenkins/jenkins-initial-setting/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 패키지(Package)로 PostgreSQL 설치하기",
        "excerpt":"PostgreSQL은 BSD(berkeley software distribution) 또는 MIT 라이선스(license)와 유사한 PostgreSQL license가 적용된 오픈소스(open source) 소프트웨어로 배포(release) 됩니다.    PostgreSQL은 확장 가능성 및 표준 준수를 강조하는 객체-관계형 데이터베이스 관리 시스템(ORDBMS, object-relational database management system)으로, 트랜잭션(transaction)과 ACID(Atomicity, Consistency, Isolation, Durability)를 지원합니다.    macOS 서버의 경우 PostgreSQL이 기본 데이터베이스(database)이며, MS Windows와 리눅스(linux)에서도 이용할 수 있습니다.    이 포스트에서는 우분투(ubuntu) 환경에서 패키지(package)로 PostgreSQL을 설치하는 방법을 소개합니다.   선행조건(PREREQUISITE)     ubuntu 환경에 Java가 설치되어 있어야 합니다.   방화벽 설정이 필요합니다.            TCP 5432 포트가 개방되어 있어야 합니다.              Java 설치 방법은 우분투(Ubuntu) 환경에 OpenJDK(Java) 설치하기 포스트를 참고하시기 바랍니다.       방화벽 설정 방법은 우분투(Ubuntu) 환경에 방화벽(Firewalld) 설치 및 설정하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   Ubuntu 18.04.3 LTS (Bionic Beaver) Server (64-bit)   PostgreSQL 11.5 (Ubuntu 11.5-3.pgdg18.04+1)   OpenJDK 1.8.0_222   요약(SUMMARY)     PostgreSQL debian packages repository 설정   apt 명령어로 PostgreSQL 설치   PostgreSQL 설정   systemctl 명령어로 PostgreSQL 실행   내용(CONTENTS)  1. PostgreSQL debian packages repository 추가  $ sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" &gt; /etc/apt/sources.list.d/pgdg.list'   2. PostgreSQL debian packages repository key 추가  $ sudo wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add -   3. apt install 명령어로 PostgreSQL 설치  $ sudo apt update &amp;&amp; sudo apt install postgresql postgresql-contrib -y   4. PostgreSQL 설정      일반 사용자 계정으로 진행합니다.    4.1. PGDATA 디렉터리(directory) 생성  $ export LINDAREX_WORKSPACE=${HOME}/workspace $ mkdir -p ${LINDAREX_WORKSPACE}/pgsql/data   4.2. PGDATA directory 경로(path) 추가  $ sudo vi /etc/profile   -------------------------------------------------------------------------------- export LINDAREX_WORKSPACE=/home/ubuntu/workspace export PGDATA=${LINDAREX_WORKSPACE}/pgsql/data --------------------------------------------------------------------------------      위 workspace(LINDAREX_WORKSPACE) directory path는 사용자 계정에 따라 다릅니다.       수정 내역 적용을 위해 아래 명령어를 입력합니다.    $ source /etc/profile      4.3. Database 초기화     아래 명령어로 Database 초기화를 실행합니다.   $ /usr/lib/postgresql/11/bin/initdb   4.4. PostgreSQL 설정 수정     아래 설정으로 외부 접속이 가능하게 합니다.   $ sudo vi /etc/postgresql/11/main/postgresql.conf   -------------------------------------------------------------------------------- listen_addresses = '*' --------------------------------------------------------------------------------      아래 설정으로 모든 사용자(비밀번호 사용)가 접속이 가능하게 합니다.   $ sudo vi /etc/postgresql/11/main/pg_hba.conf   -------------------------------------------------------------------------------- ## Add at the bottom host    all             all             0.0.0.0/0               password --------------------------------------------------------------------------------   5. systemctl 명령어로 PostgreSQL 서비스(service) 관리  5.1. PostgreSQL service 설정 반영  $ sudo systemctl daemon-reload   5.2. PostgreSQL service 시작  $ sudo systemctl start postgresql.service   5.3. PostgreSQL service 중지  $ sudo systemctl stop postgresql.service   5.4. PostgreSQL service 재시작  $ sudo systemctl restart postgresql.service   5.5. PostgreSQL service 설정 재적용  $ sudo systemctl reload postgresql.service   5.6. PostgreSQL service 상태 조회  $ sudo systemctl status postgresql.service   5.7. PostgreSQL service 활성화(부팅 시 자동 시작)  $ sudo systemctl enable postgresql.service   5.8. PostgreSQL service 비활성화  $ sudo systemctl disable postgresql.service   5.9. PostgreSQL service 및 관련 프로세스 모두 중지  $ sudo systemctl kill postgresql.service   마무리(CONCLUSION)  ubuntu 환경에 package로 PostgreSQL 설치를 완료했습니다.    2020년 3월 기준, open source 소프트웨어인 PostgreSQL은 세계 점유율 4위로, open source 프로젝트 커뮤니티 기반의 database 중에는 1위이며, 대량 Insert 처리와 DW(data warehouse)에 강한 모습을 보이면서 국내에서도 점유율이 증가하는 추세입니다.    PostgreSQL은 pgAdmin, psql cli(command line interface, 명령줄 인터페이스) 등으로 PostgreSQL을 관리하며, 실제 기능적인 면에서 Oracle과 유사한 것이 많기 때문에, Oracle 사용자들이 가장 쉽게 테스트하고 적용할 수 있습니다.    더 자세한 내용은 아래 참고 페이지를 확인해 주시기 바랍니다.      database ranking에 대한 자세한 정보는 https://db-engines.com/en/ranking, https://db-engines.com/en/ranking_trend를 확인해 주시기 바랍니다.       data warehouse에 대한 자세한 정보는 https://ko.wikipedia.org/wiki/데이터_웨어하우스를 확인해 주시기 바랍니다.    참고(REFERENCES)     https://www.postgresql.org/   https://www.postgresql.org/download/linux/ubuntu/   https://ko.wikipedia.org/wiki/PostgreSQL   https://d2.naver.com/helloworld/227936  ","categories": ["postgresql"],
        "tags": ["postgresql","database","open source software","open source","oss","ubuntu"],
        "url": "https://lindarex.github.io/postgresql/ubuntu-postgresql-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 SonarQube 설치하기",
        "excerpt":"SonarQube는 정적 코드 분석기로, LGPL(lesser gnu general public license)로 오픈소스(open source) 소프트웨어이며, 20개 이상의 프로그래밍 언어의 버그와 code smells, 코드 커버리지, 유닛 테스트, 코딩 표준, 중복 코드, 코드 복잡도, 주석 및 보안 취약점의 보고서를 제공하고, 자동 리뷰를 수행하여 지속적인 코드 품질 검사를 위한 플랫폼입니다.    SonarQube는 Maven, Ant, Gradle, MSBuild 및 CI(continuous integration) 도구인 Atlassian Bamboo, Jenkins, Hudson 등과의 연동을 제공합니다.    이 포스트에서는 우분투(ubuntu) 환경에서 SonarQube를 설치하는 방법을 소개합니다.   선행조건(PREREQUISITE)     ubuntu 환경에 PostgreSQL이 설치되어 있어야 합니다.   방화벽 설정이 필요합니다.            TCP 9000 포트가 개방되어 있어야 합니다.              PostgreSQL 설치 방법은 우분투(Ubuntu) 환경에 PostgreSQL 설치하기 포스트를 참고하시기 바랍니다.       방화벽 설정 방법은 우분투(Ubuntu) 환경에 방화벽(Firewalld) 설치 및 설정하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   Ubuntu 18.04.3 LTS (Bionic Beaver) Server (64-bit)   SonarQube 7.9.1   PostgreSQL 11.5 (Ubuntu 11.5-3.pgdg18.04+1)   OpenJDK 1.8.0_222   요약(SUMMARY)     SonarQube 파일 내려받기   PostgreSQL 설정   SonarQube 설정   스크립트로 SonarQube 서비스 관리   웹브라우저로 SonarQube 접속   내용(CONTENTS)      아래 명령어로 workspace 디렉터리를 생성합니다.    $ export LINDAREX_WORKSPACE=${HOME}/workspace $ mkdir -p ${LINDAREX_WORKSPACE}   1. SonarQube 파일 내려받기  $ wget -P ${LINDAREX_WORKSPACE} https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-7.9.1.zip   2. 내려받은 파일 압축 해제  $ unzip -q ${LINDAREX_WORKSPACE}/sonarqube-7.9.1.zip -d ${LINDAREX_WORKSPACE}   3. Symbolic link 설정  $ ln -s ${LINDAREX_WORKSPACE}/sonarqube-7.9.1 ${LINDAREX_WORKSPACE}/sonarqube   4. PostgreSQL 설정     SonarQube와 연동 될 사용자 계정과 데이터베이스(Database)를 생성합니다.   4.1. postgres 계정 로그인  $ sudo su - postgres   4.2. psql utility 실행  $ psql postgres   4.3. 사용자 생성  postgres=# create user sonar;   4.4. 사용자 Role 설정  postgres=# alter role sonar with createdb;   4.5. 사용자 비밀번호 설정  postgres=# alter user sonar with encrypted password 'sonar-password'; postgres=# alter user postgres password 'postgres-password';      생성한 사용자 정보를 조회합니다.    postgres=# \\du      4.6. Database 생성  postgres=# create database sonar owner sonar;   4.7. Privileges 설정  postgres=# grant all privileges on database sonar to sonar;      생성한 Database를 조회합니다.    postgres=# \\l      4.8. psql utility 종료  postgres=# \\q   4.9. postgres 계정 로그아웃  $ exit   5. SonarQube 설정     위에서 생성한 PostgreSQL 사용자 정보와 Database 정보, SonarQube UI의 포트를 설정합니다.   $ vi ${LINDAREX_WORKSPACE}/sonarqube/conf/sonar.properties   -------------------------------------------------------------------------------- sonar.jdbc.username=sonar sonar.jdbc.password=sonar-password sonar.jdbc.url=jdbc:postgresql://{MY-IP}/sonar sonar.web.port=9000 --------------------------------------------------------------------------------      OpenJDK(Java) 경로를 설정합니다.   $ vi ${LINDAREX_WORKSPACE}/sonarqube/conf/wrapper.conf   -------------------------------------------------------------------------------- wrapper.java.command=/home/rex/workspace/tool/java11/bin/java --------------------------------------------------------------------------------   6. Max map count 설정  $ sudo vi /etc/profile   -------------------------------------------------------------------------------- sudo sysctl -w vm.max_map_count=262144 --------------------------------------------------------------------------------      SonarQube를 Linux에 설치 시, 아래 사항이 요구됩니다.         vm.max_map_count :: 262144 이상     fs.file-max :: 65536 이상     file descriptors :: 65536 이상     threads :: 4096 이상         SonarQube 설치 시 필요 요구사항에 대한 자세한 정보는 https://docs.sonarqube.org/latest/requirements/requirements/를 확인해 주시기 바랍니다.       수정 내역 적용을 위해 아래 명령어를 입력합니다.    $ sudo source /etc/profile      7. 스크립트로 SonarQube 서비스(service) 관리  7.1. SonarQube service 시작  $ ${LINDAREX_WORKSPACE}/sonarqube/bin/linux-x86-64/sonar.sh start   7.2. SonarQube service 중지  $ ${LINDAREX_WORKSPACE}/sonarqube/bin/linux-x86-64/sonar.sh stop   7.3. SonarQube service 재시작  $ ${LINDAREX_WORKSPACE}/sonarqube/bin/linux-x86-64/sonar.sh restart   7.3. SonarQube service 상태 조회  $ ${LINDAREX_WORKSPACE}/sonarqube/bin/linux-x86-64/sonar.sh status   8. 웹브라우저로 SonarQube 접속     http://[MY-IP]:9000   마무리(CONCLUSION)  ubuntu 환경에 SonarQube 설치를 완료했습니다.    SonarQube를 활용하기 위해서는 추가적인 설정이 필요합니다.   SonarQube 하나만으로는 아무것도 할 수 없습니다.   Java 프로젝트(project)의 코드 정적 분석을 할 경우, project 설정 파일(build.gradle, pom.xml 등)에 SonarScanner 설정을 추가하고, CI와 연동하거나 로컬 환경에 SonarQube 설정을 해야 합니다.   대부분 현업에서는 Jenkins 등의 CI, GitHub 등의 버전 관리(version control) 시스템과 연동하여 SonarScanner를 통해 소스 코드를 분석합니다.    더 자세한 내용은 아래 참고 페이지를 확인해 주시기 바랍니다.   참고(REFERENCES)     https://www.sonarqube.org/   https://docs.sonarqube.org/latest/   http://www.sonarqube.org/downloads/  ","categories": ["sonarqube"],
        "tags": ["sonarqube","open source software","open source","oss","ubuntu"],
        "url": "https://lindarex.github.io/sonarqube/ubuntu-sonarqube-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 SCM Manager 설치하기",
        "excerpt":"SCM Manager는 사설 버전 관리 레파지토리(private source code and version control repository)로 BSD(berkeley software distribution) 라이선스(license)가 적용된 오픈소스(open source) 소프트웨어입니다.    SCM Manager는 Git과 Mercurial, Subversion을 지원하고, 다양한 플러그인과 RESTFul API를 제공하며, 설치 방법으로는 Standalone 방식(tar, RPM, DEB package)과 WebApp(war) 방식을 제공합니다.    이 포스트에서는 우분투(ubuntu) 환경에서 standalone 방식으로 SCM Manager를 설치하는 방법을 소개합니다.   선행조건(PREREQUISITE)     ubuntu 환경에 Java가 설치되어 있어야 합니다.   방화벽 설정이 필요합니다.            TCP 8080 포트가 개방되어 있어야 합니다.              Java 설치 방법은 우분투(Ubuntu) 환경에 OpenJDK(Java) 설치하기 포스트를 참고하시기 바랍니다.       방화벽 설정 방법은 우분투(Ubuntu) 환경에 방화벽(Firewalld) 설치 및 설정하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   Ubuntu 18.04.3 LTS (Bionic Beaver) Server (64-bit)   SCM Manager 1.6.0   OpenJDK 1.8.0_222   요약(SUMMARY)     SCM Manager 파일 내려받기   SCM Manager 설정   스크립트로 SCM Manager 살행   웹브라우저로 SCM Manager 접속   내용(CONTENTS)      아래 명령어로 workspace 디렉터리를 생성합니다.    $ export LINDAREX_WORKSPACE=${HOME}/workspace $ mkdir -p ${LINDAREX_WORKSPACE}   1. SCM Manager 파일 내려받기  $ wget -P ${LINDAREX_WORKSPACE} https://maven.scm-manager.org/nexus/content/repositories/releases/sonia/scm//scm-server/1.60/scm-server-1.60-app.tar.gz   2. 내려받은 파일 압축 해제  $ tar zxf ${LINDAREX_WORKSPACE}/scm-server-1.60-app.tar.gz -C ${LINDAREX_WORKSPACE}   3. SCM Manager 설정      SCM Manager UI의 포트를 설정합니다.   $ vi ${LINDAREX_WORKSPACE}/scm-server/conf/server-config.xml   -------------------------------------------------------------------------------- &lt;SystemProperty name=\"jetty.port\" default=\"8080\" /&gt; --------------------------------------------------------------------------------   4. SCM Manager 실행  $ cd ${LINDAREX_WORKSPACE}/scm-server/bin/ $ nohup ./scm-server &gt; /dev/null &amp;   5. 웹브라우저로 SCM Manager 접속     http://[MY-IP]:8080      SCM Manager의 기본 계정은 ‘scmadmin’, 비밀번호도 ‘scmadmin’ 입니다.    마무리(CONCLUSION)  ubuntu 환경에 standalone 방식으로 SCM Manager 설치를 완료했습니다.    SCM Manager 외에도 GitHub, GitLab, Bitbucket, GitStack 등 다양한 private git 호스팅 서비스(service)가 존재합니다.   타 service와 비교 시, SCM Manager의 장점은 간단히 설치하여 사용 가능하고, 설정하기 쉽다는 것입니다.   또한, git 외에도 svn과 mercurial 서버(server)까지 동시에 사용할 수 있으며, 기존에 사용 중이던 svn server를 그대로 이용할 수 있습니다.    더 자세한 내용은 아래 참고 페이지를 확인해 주시기 바랍니다.   참고(REFERENCES)     https://www.scm-manager.org/   https://www.scm-manager.org/download/  ","categories": ["scm-manager"],
        "tags": ["scm manager","version control","open source software","open source","oss","ubuntu"],
        "url": "https://lindarex.github.io/scm-manager/ubuntu-scm-manager-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 AWS CLI 설치하기",
        "excerpt":"AWS CLI(command line interface, 명령줄 인터페이스)는 AWS 서비스(service)를 관리하는 통합 도구입니다.   Python 2.6.5 이상이 필요하며, pip을 사용하여 AWS CLI를 설치합니다.    이 포스트에서는 우분투(ubuntu) 환경에서 AWS를 사용하기 위한 AWS CLI를 설치하는 방법을 소개합니다.      pip은 파이썬(Python)으로 작성된 패키지(package) 소프트웨어를 설치 및 관리하는 package 관리 시스템입니다.    선행조건(PREREQUISITE)     ubuntu 환경이 필요합니다.      Ubuntu 설치 방법은 우분투(Ubuntu) 서버(Server) 16.04 설치하기 또는 우분투(Ubuntu) 서버(Server) 18.04 설치하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   Ubuntu 18.04.3 LTS (Bionic Beaver) Server (64-bit)   AWS CLI v1.16.261   Python v2.7.17   요약(SUMMARY)     apt 명령어로 pip 설치   pip 명령어로 AWS CLI 설치   내용(CONTENTS)  1. apt 명령어로 pip 설치  $ sudo apt install python-setuptools python-pip -y   2. pip 명령어로 AWS CLI 설치  $ pip install awscli      AWS CLI 설치 후 세션 재접속이 필요합니다.    마무리(CONCLUSION)  ubuntu 환경에 AWS CLI 설치를 완료했습니다.    다음 포스트에서는 AWS CLI를 이용한 AWS S3 사용 방법을 소개하겠습니다.   참고(REFERENCES)     https://aws.amazon.com/ko/cli/  ","categories": ["aws"],
        "tags": ["aws","cli","ubuntu"],
        "url": "https://lindarex.github.io/aws/ubuntu-aws-cli-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "AWS EC2 우분투(Ubuntu) 환경에 BOSH director 설치하기",
        "excerpt":"Cloud Foundry BOSH는 소규모 및 대규모 클라우드(cloud) 애플리케이션(application)의 배포와 라이프 사이클 관리, release engineering을 통합하는 오픈소스(open source) 소프트웨어 프로젝트입니다.  BOSH는 수백 개의 가상머신(VM, virtual machine)에 application을 프로비저닝(provisioning)하고 배포할 수 있으며, 모니터링과 오류 복구, application 업데이트를 수행합니다.    BOSH는 Cloud Foundry PaaS(CFAR) 배포를 위해 개발되었지만, 거의 모든 소프트웨어를 배포하는 데에도 사용할 수 있습니다. 예를 들면, Hadoop 또는 Jenkins 등의 오픈소스 소프트웨어를 BOSH release로 작성하여 배포할 수 있으며 대규모 분산 시스템(system)에 적합합니다.    또한 BOSH는 Amazon Web Services EC2, Google Cloud Platform, Microsoft Azure, OpenStack, VMware vSphere 및 Alibaba Cloud와 같은 다양한 IaaS(infrastructure as a service) provider를 지원하며, Apache CloudStack, VirtualBox 등의 IaaS provider 지원을 위해 CPI(cloud provider interface)를 제공합니다.    이 포스트에서는 AWS EC2 우분투(ubuntu) 환경에서 BOSH 사용을 위한 BOSH director를 설치하는 방법을 소개합니다.      CFAR에 대해서는 본문의 CF CLI 설치에서 설명합니다.    선행조건(PREREQUISITE)     AWS 환경이 필요합니다.            VPC 설정이 필요합니다.       Subnet 설정이 필요합니다.       Internet gateway 설정이 필요합니다.       NAT Gateway 설정이 필요합니다.       Routing table 설정이 필요합니다.       security group 설정이 필요합니다.       Quota 설정이 필요합니다.           AWS EC2 Inception VM이 필요합니다.            ubuntu 환경이 필요합니다.              BOSH director 설치를 위한 AWS 환경 설정 및 AWS EC2 Inception VM 생성 방법은 다음 포스트에서 소개하겠습니다.       AWS, GCP, MS Azure, OpenStack, VMware vSphere 등 IaaS 환경에 BOSH director를 설치하여 BOSH를 사용하거나, 로컬(local) 환경에 BOSH-LITE를 설치하여 BOSH를 사용할 수 있습니다.       Provisioning이란 사용자의 요구에 맞게 system 자원을 할당, 배치, 배포해 두었다가 필요 시 system을 즉시 사용할 수 있는 상태로 미리 준비해 두는 것을 의미합니다.       Provisioning에 대한 자세한 정보는 https://ko.wikipedia.org/wiki/프로비저닝, https://en.wikipedia.org/wiki/Provisioning_(telecommunications)를 확인해 주시기 바랍니다.       Ubuntu 설치 방법은 우분투(Ubuntu) 서버(Server) 16.04 설치하기 또는 우분투(Ubuntu) 서버(Server) 18.04 설치하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     AWS EC2   Ubuntu 18.04.3 LTS (Bionic Beaver) Server (64-bit)   요약(SUMMARY)     BOSH CLI 설치   (선택사항) CF CLI 설치   CF UAA CLI 설치   Credhub CLI 설치   BOSH director 설치   BOSH UAA 통합 인증   BOSH director 설정   BOSH jumpbox 설정   Credhub 설정   내용(CONTENTS)      아래 명령어로 환경변수를 설정하고, workspace 디렉터리(directory)를 생성합니다.    $ export LINDAREX_INCEPTION_USER_NAME='ubuntu' $ export LINDAREX_BOSH_WORKSPACE=/home/${LINDAREX_INCEPTION_USER_NAME}/workspace $ export LINDAREX_BOSH_DIRECTOR='micro-bosh' $ export LINDAREX_BOSH_IAAS='aws' $ mkdir -p ${LINDAREX_BOSH_WORKSPACE}   1. BOSH CLI 설치하기     BOSH CLI는 v1 버전과 v2 버전이 존재하며, 이 포스트에서는 v2 버전을 기준으로 설명합니다.   1.1. 종속 패키지 설치     자신이 사용하는 ubuntu 버전에 따라 아래 명령어로 종속 패키지(package dependencies)를 설치합니다.      ubuntu 18.04 (Bionic)    $ sudo apt update $ sudo apt install -y build-essential zlibc zlib1g-dev ruby ruby-dev openssl libxslt1-dev libxml2-dev libssl-dev libreadline7 libreadline-dev libyaml-dev libsqlite3-dev sqlite3      ubuntu 16.04 (Xenial) 또는 ubuntu 14.04 (Trusty)    $ sudo apt update $ sudo apt install -y libcurl4-openssl-dev gcc g++ build-essential zlibc zlib1g-dev ruby ruby-dev openssl libxslt-dev libxml2-dev libssl-dev libreadline6 libreadline6-dev libyaml-dev libsqlite3-dev sqlite3   1.2. BOSH CLI 파일(file) 내려받기  $ curl -Lo ${LINDAREX_BOSH_WORKSPACE}/bosh https://github.com/cloudfoundry/bosh-cli/releases/download/v6.2.1/bosh-cli-6.2.1-linux-amd64   1.3. chmod(change mode) 명령어로 file 권한 변경(실행 권한 부여)  $ chmod +x ${LINDAREX_BOSH_WORKSPACE}/bosh   1.4. 실행 file을 사용자 프로그램 경로(path)로 옮기기  $ sudo mv ${LINDAREX_BOSH_WORKSPACE}/bosh /usr/local/bin/bosh   1.5. BOSH CLI 설치 확인  $ bosh -v version 6.2.1-a28042ac-2020-02-10T18:40:57Z  Succeeded   2. (선택사항) CF CLI 설치하기     CF CLI에서 CF는 정확히 표현하면 CFAR(cloud foundry application runtime)입니다.   CF CLI는 CFAR 명령어를 사용하기 위한 설치입니다.   CFAR은 BOSH로 배포하며, 배포 방법은 다음 포스트에서 소개하겠습니다.      CFAR은 CFCR(cloud foundry container runtime)이 생기기 전까지 CF(cloud foundry)로 불렸었고, CFCR도 Cloud Foundry Foundation incubating project 초반에는 kubo로 불렸습니다.       최근 컨테이너 플랫폼(container platform)의 성장과 함께 Cloud Foundry도 kubernetes를 이용한 container runtime을 제공하면서, application runtime인 CF를 CFAR로, container runtime인 kubo를 CFCR로 명칭을 변경했습니다.    2.1. CF CLI Debian packages repository key 추가  $ wget -q -O - https://packages.cloudfoundry.org/debian/cli.cloudfoundry.org.key | sudo apt-key add -   2.2. CF CLI Debian packages repository 추가  $ echo \"deb https://packages.cloudfoundry.org/debian stable main\" | sudo tee /etc/apt/sources.list.d/cloudfoundry-cli.list   2.3. apt install 명령어로 CF CLI 설치  $ sudo apt update &amp;&amp; sudo apt install cf-cli -y   2.4. CF CLI 설치 확인  $ cf --version cf version 6.49.0+d0dfa93bb.2020-01-07   3. CF UAA CLI 설치하기     UAA는 사용자 계정 및 인증 서버(User Account and Authentication server)입니다.   UAA는 CFAR과 BOSH에 각각 존재하며, CF UAA CLI를 통해 모두 사용할 수 있습니다.   3.1. Rubygems으로 CF UAA CLI 설치  $ sudo gem install cf-uaac   3.2. CF UAA CLI 설치 확인  $ uaac -v UAA client 4.2.0   4. Credhub CLI 설치     CredHub는 비밀번호, 인증서(certificates), 인증 기관(certificate authorities), ssh 키, rsa 키와 같은 credentials 정보를 관리합니다.   4.1. Credhub CLI file 내려받기  $ wget -P ${LINDAREX_BOSH_WORKSPACE} https://github.com/cloudfoundry-incubator/credhub-cli/releases/download/2.6.2/credhub-linux-2.6.2.tgz   4.2. 내려받은 file 압축 해제  $ tar zxf ${LINDAREX_BOSH_WORKSPACE}/credhub-linux-2.6.2.tgz -C ${LINDAREX_BOSH_WORKSPACE}   4.3. chmod 명령어로 file 권한 변경(실행 권한 부여)  $ chmod +x ${LINDAREX_BOSH_WORKSPACE}/credhub   4.4. 실행 file을 사용자 프로그램 path로 옮기기  $ sudo mv ${LINDAREX_BOSH_WORKSPACE}/credhub /usr/local/bin/credhub   4.5. Credhub CLI 설치 확인  $ credhub --version CLI Version: 2.6.2 Server Version: Not Found. Have you targeted and authenticated against a CredHub server?   5. BOSH director 설치  5.1. BOSH deployment repository clone 받기  $ git clone https://github.com/cloudfoundry/bosh-deployment.git ${LINDAREX_BOSH_WORKSPACE}/bosh-deployment      Cloud Foundry github repository에서 제공하는 BOSH deployment는 branch나 tag가 존재하지 않습니다. Clone 받는 당시의 버전으로 설치를 해야 하며, 간혹 오류를 포함한 버전도 배포되곤 합니다.    5.2. 배포 스크립트(script) 작성  $ vi ${LINDAREX_BOSH_WORKSPACE}/bosh-deployment/deploy-${LINDAREX_BOSH_IAAS}.sh   -------------------------------------------------------------------------------- #!/bin/bash  bosh create-env bosh.yml \\ \t--state=aws/state.json \\ \t--vars-store=aws/creds.yml \\ \t-o aws/cpi.yml \\ \t-o uaa.yml \\ \t-o credhub.yml \\ \t-o jumpbox-user.yml \\ \t-v internal_cidr='10.0.1.0/24' \\ \t-v internal_gw='10.0.1.1' \\ \t-v internal_ip='10.0.1.6' \\ \t-v director_name='micro-bosh' \\ \t-v access_key_id='AKIBI7UEWB42Q4THBBFQ' \\ \t-v secret_access_key='br3K3YQM7SwvyP/0G4AJozpSLUFwaBxy+KRFy+3q' \\ \t-v region='ap-northeast-2' \\ \t-v az='ap-northeast-2c' \\ \t-v default_key_name='lindarex-inception' \\ \t-v default_security_groups=[lindarex-security] \\ \t-v subnet_id='subnet-087f7203d0bcd1396' \\ \t-v private_key='~/.ssh/lindarex-inception.pem' --------------------------------------------------------------------------------      ‘-o’ 옵션은 YAML로 작성된 옵션 file의 path를 지정하여 배포 시에 적용합니다.        ‘-v’ 옵션은 변수를 설정하여 배포 시에 적용합니다.       배포 script를 상세하게 설명하겠습니다.            ‘create-env’ :: BOSH director를 배포하는 명령어입니다.       ‘bosh.yml’ :: 설정한 YAML file을 기반으로 Single VM을 생성합니다.       ’–state’ :: 배포 상태 file의 path입니다.       ’–vars-store’ :: Credentials 정보를 저장하는 YAML file의 path입니다.       ‘-o aws/cpi.yml’ :: AWS CPI를 적용합니다. IaaS 마다 CPI가 다릅니다.       ‘-o uaa.yml’ :: UAA를 적용합니다.       ‘-o credhub.yml’ :: Credhub을 적용합니다.       ‘-o jumpbox-user.yml’ :: Jumpbox user를 적용합니다. BOSH director VM에 SSH 접속을 할 수 있습니다.       ‘-v internal_cidr’ :: BOSH director VM에 적용할 네트워크(network)의 cidr입니다.       ‘-v internal_gw’ :: BOSH director VM에 적용할 network의 gateway ip입니다.       ‘-v internal_ip’ :: BOSH director VM에 적용할 network의 private ip입니다.       ‘-v director_name’ :: BOSH director 이름입니다. BOSH login할 때 사용합니다.       ‘-v access_key_id’ :: IaaS가 AWS인 경우 필요한 변수이며, AWS access key id입니다.       ‘-v secret_access_key’ :: IaaS가 AWS인 경우 필요한 변수이며, AWS secret access key입니다.       ‘-v region’ :: IaaS가 AWS인 경우 필요한 변수이며, AWS region입니다.       ‘-v az’ :: IaaS가 AWS인 경우 필요한 변수이며, AWS az(가용 영역, availability zone)입니다.       ‘-v default_key_name’ :: IaaS가 AWS인 경우 필요한 변수이며, AWS key name입니다.       ‘-v default_security_groups’ :: IaaS가 AWS인 경우 필요한 변수이며, AWS security_group입니다.       ‘-v subnet_id’ :: IaaS가 AWS인 경우 필요한 변수이며, BOSH director VM에 적용할 network의 subnet id입니다.       ‘-v private_key’ :: IaaS가 AWS인 경우 필요한 변수이며, AWS private key file의 path입니다.              위 배포 script는 AWS 서울 region에 배포되는 설정이지만, region과 az를 제외한 AWS 설정값은 잘못된 값이므로 참고만 하시기를 바랍니다.    5.3. chmod 명령어로 file 권한 변경(실행 권한 부여)  $ chmod +x ${LINDAREX_BOSH_WORKSPACE}/bosh-deployment/deploy-${LINDAREX_BOSH_IAAS}.sh   5.4. (선택사항) 배포 상태 file 삭제  $ rm -rf ${LINDAREX_BOSH_WORKSPACE}/bosh-deployment/${LINDAREX_BOSH_IAAS}/state.json $ rm -rf ${LINDAREX_BOSH_WORKSPACE}/bosh-deployment/${LINDAREX_BOSH_IAAS}/creds.yml      state.json file과 creds.yml file은 배포 성공 여부와 관계없이 ‘bosh create-env’ 명령어를 한 번이라도 실행하면 생성되는 file입니다.    5.5. 배포 script 실행  $ cd ${LINDAREX_BOSH_WORKSPACE}/bosh-deployment $ ./deploy-${LINDAREX_BOSH_IAAS}.sh   6. BOSH UAA 통합 인증  6.1. BOSH UAA target 설정  $ uaac target https://10.0.1.6:8443 --skip-ssl-validation Unknown key: Max-Age = 86400  Target: https://10.0.1.6:8443    6.2. Client credentials grant를 통해 UAA admin token 얻기  $ uaac token client get uaa_admin -s `bosh int ${LINDAREX_BOSH_WORKSPACE}/bosh-deployment/${LINDAREX_BOSH_IAAS}/creds.yml --path /uaa_admin_client_secret` Unknown key: Max-Age = 86400  Successfully fetched token via client credentials grant. Target: https://10.0.1.6:8443 Context: uaa_admin, from client uaa_admin    6.3. UAA admin token 파싱 및 clients 목록 조회  $ uaac token decode &amp;&amp; uaac clients  Note: no key given to validate token signature    jti: 1b82b1e6cfbb4606a182d5ab88c28b8d   sub: uaa_admin   authorities: clients.read password.write clients.secret clients.write uaa.admin scim.write scim.read   scope: clients.read password.write clients.secret clients.write uaa.admin scim.write scim.read   client_id: uaa_admin   cid: uaa_admin   azp: uaa_admin   revocable: true   grant_type: client_credentials   rev_sig: 34bac1ad   iat: 1573525863   exp: 1573569063   iss: https://10.0.1.6:8443/oauth/token   zid: uaa   aud: scim uaa_admin password clients uaa   admin     scope: uaa.none     resource_ids: none     authorized_grant_types: client_credentials     autoapprove:     authorities: bosh.admin     lastmodified: 1573525068032   bosh_cli     scope: openid bosh.admin bosh.read bosh.*.admin bosh.*.read bosh.teams.*.admin bosh.teams.*.read     resource_ids: none     authorized_grant_types: password refresh_token     autoapprove:     access_token_validity: 120     refresh_token_validity: 86400     authorities: uaa.none     lastmodified: 1573525068153   credhub-admin     scope: uaa.none     resource_ids: none     authorized_grant_types: client_credentials     autoapprove:     access_token_validity: 3600     authorities: credhub.write credhub.read     lastmodified: 1573525068271   credhub_cli     scope: credhub.read credhub.write     resource_ids: none     authorized_grant_types: password refresh_token     autoapprove:     access_token_validity: 60     refresh_token_validity: 1800     authorities: uaa.none     lastmodified: 1573525068386   director_to_credhub     scope: uaa.none     resource_ids: none     authorized_grant_types: client_credentials     autoapprove:     access_token_validity: 3600     authorities: credhub.write credhub.read     lastmodified: 1573525068492   hm     scope: uaa.none     resource_ids: none     authorized_grant_types: client_credentials     autoapprove:     authorities: bosh.admin     lastmodified: 1573525068590   uaa_admin     scope: uaa.none     resource_ids: none     authorized_grant_types: client_credentials     autoapprove:     authorities: clients.read password.write clients.secret clients.write uaa.admin scim.write scim.read     lastmodified: 1573525068689   7. BOSH director 설정  7.1. Local alias 설정  $ export BOSH_CLIENT='admin' $ export BOSH_CLIENT_SECRET=`bosh int ${LINDAREX_BOSH_WORKSPACE}/bosh-deployment/${LINDAREX_BOSH_IAAS}/creds.yml --path /admin_password` $ bosh alias-env ${LINDAREX_BOSH_DIRECTOR} -e 10.0.1.6 --ca-cert &lt;(bosh int ${LINDAREX_BOSH_WORKSPACE}/bosh-deployment/${LINDAREX_BOSH_IAAS}/creds.yml --path /director_ssl/ca)   7.2. BOSH 로그인  $ bosh -e ${LINDAREX_BOSH_DIRECTOR} l Successfully authenticated with UAA  Succeeded   7.3. BOSH director 정보 조회  $ bosh -e ${LINDAREX_BOSH_DIRECTOR} env Using environment '10.0.1.6' as client 'admin'  Name               micro-bosh UUID               b002986f-3a05-441a-be08-3a46dbe29a8a Version            270.5.0 (00000000) Director Stemcell  ubuntu-xenial/456.27 CPI                aws_cpi Features           compiled_package_cache: disabled                    config_server: enabled                    local_dns: enabled                    power_dns: disabled                    snapshots: disabled User               admin  Succeeded   7.4. (선택사항) 사용자 프로필에 BOSH director 정보 추가  $ vi $HOME/.profile   -------------------------------------------------------------------------------- export LINDAREX_INCEPTION_USER_NAME='ubuntu' export LINDAREX_BOSH_WORKSPACE=/home/${LINDAREX_INCEPTION_USER_NAME}/workspace export LINDAREX_BOSH_DIRECTOR='micro-bosh' export LINDAREX_BOSH_IAAS='aws' export BOSH_ENVIRONMENT='10.0.1.6' export BOSH_CLIENT='admin' export BOSH_CLIENT_SECRET=`bosh -e ${LINDAREX_BOSH_DIRECTOR} int ${LINDAREX_BOSH_WORKSPACE}/bosh-deployment/${LINDAREX_BOSH_IAAS}/creds.yml --path /admin_password` --------------------------------------------------------------------------------      수정 내역 적용을 위해 아래 명령어를 입력합니다.    $ source $HOME/.profile   8. BOSH jumpbox 설정  8.1. Jumpbox key 생성  $ cd ${LINDAREX_BOSH_WORKSPACE}/bosh-deployment $ bosh int ${LINDAREX_BOSH_IAAS}/creds.yml --path /jumpbox_ssh/private_key &gt; jumpbox.key   8.2. chmod 명령어로 file 권한 변경(소유자만 읽기 및 쓰기 권한 부여)  $ chmod 600 jumpbox.key   8.3. BOSH director VM에 SSH 접속  $ ssh jumpbox@${BOSH_ENVIRONMENT} -i ${LINDAREX_BOSH_WORKSPACE}/bosh-deployment/jumpbox.key The authenticity of host '10.0.1.6 (10.0.1.6)' can't be established. ECDSA key fingerprint is SHA256:Lc0OsEocqPAEgAk0c1X7Y7y+iNWqeFMGkfFFLRlA8ww. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '10.0.1.6' (ECDSA) to the list of known hosts. Unauthorized use is strictly prohibited. All access and activity is subject to logging and monitoring. Welcome to Ubuntu 16.04.6 LTS (GNU/Linux 4.15.0-54-generic x86_64)   * Documentation:  https://help.ubuntu.com  * Management:     https://landscape.canonical.com  * Support:        https://ubuntu.com/advantage  The programs included with the Ubuntu system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright.  Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law.  Last login: Tue Feb 18 07:23:17 2020 from 10.0.201.149 To run a command as administrator (user \"root\"), use \"sudo &lt;command&gt;\". See \"man sudo_root\" for details.  bosh/0:~$    8.4. BOSH director VM의 외부 통신 상태 확인  bosh/0:~$ sudo ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=53 time=30.8 ms 64 bytes from 8.8.8.8: icmp_seq=2 ttl=53 time=29.1 ms ^C --- 8.8.8.8 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1000ms rtt min/avg/max/mdev = 29.135/30.009/30.884/0.891 ms   bosh/0:~$ wget https://wordpress.org/latest.zip      아래 명령어로 BOSH director VM의 SSH 접속을 종료할 수 있습니다.    bosh/0:~$ exit logout Connection to 10.0.1.6 closed.   9. Credhub 설정  9.1. Credhub api server 설정  $ credhub api \\ --ca-cert=&lt;(bosh int ${LINDAREX_BOSH_WORKSPACE}/bosh-deployment/${LINDAREX_BOSH_IAAS}/creds.yml --path /credhub_ca/ca) \\ --ca-cert=&lt;(bosh int ${LINDAREX_BOSH_WORKSPACE}/bosh-deployment/${LINDAREX_BOSH_IAAS}/creds.yml --path /uaa_ssl/ca) \\ --server=10.0.1.6:8844 Setting the target url: https://10.0.1.6:8844   9.2. Credhub 로그인  $ credhub login --client-name=credhub-admin --client-secret=`bosh int ${LINDAREX_BOSH_WORKSPACE}/bosh-deployment/${LINDAREX_BOSH_IAAS}/creds.yml --path /credhub_admin_client_secret` Login Successful   9.3. credentials 전체 조회  $ credhub find credentials: []   마무리(CONCLUSION)  AWS EC2 ubuntu 환경에 BOSH director 설치를 완료했습니다.    BOSH director 설치는 CFAR 및 CFCR 등 BOSH release 배포를 위한 기반 작업이며, 설치 진행을 위해서는 적지 않은 IaaS 설정이 필요합니다.    다음 포스트에서는 BOSH director 설치를 위한 AWS 환경 설정과 AWS EC2 Inception VM 생성 방법, BOSH components에 대한 설명과 BOSH release 작성 방법을 소개하겠습니다.   참고(REFERENCES)     https://bosh.io/docs/cli-v2-install/   https://github.com/cloudfoundry/bosh-cli/releases   https://docs.cloudfoundry.org/cf-cli/install-go-cli.html   https://github.com/cloudfoundry/cf-uaac   https://github.com/cloudfoundry-incubator/credhub-cli  ","categories": ["bosh"],
        "tags": ["bosh","cloud foundry","ubuntu","aws","ec2","open source software","open source","oss"],
        "url": "https://lindarex.github.io/bosh/ubuntu-bosh-director-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "Travis CI로 Jekyll 블로그를 GitHub Pages에 자동 배포하기",
        "excerpt":"이 포스트에서는 Travis CI(travis)를 이용해 Jekyll 블로그를 GitHub Pages에 자동 배포(deploy)하는 방법을 소개합니다.   선행조건(PREREQUISITE)     GitHub 계정이 필요합니다.   GitHub Pages의 브랜치(branch)는 빌드(build) 후 deploy되는 master branch, jekyll code를 올릴 sources branch로 구성합니다.   테스트 환경(TEST ENVIRONMENT)     Chrome v80.0.3987.116(공식 빌드) (64비트)   Firefox Browser v74.0b5 (64-비트)   요약(SUMMARY)     GitHub Personal access token 생성   Travis CI 설정   travis.yml 파일 작성   deploy 확인   내용(CONTENTS)  1. GitHub Personal access token 생성     https://github.com/에 접속합니다.       우측 상단의 Profile 아이콘을 선택하여 ‘Settings’ 페이지로 이동합니다.         좌측 중간에 Personal access tokens 메뉴를 클릭합니다.         GitHub 계정의 비밀번호를 입력합니다.         token 이름을 입력하고, token으로 접근 가능한 Scope(repo 전체)를 체크하고, 화면 하단의 ‘Generate token’을 클릭합니다.            생성된 token 값은 travis 설정에 필요하기 때문에 메모해 둡니다.      2. Travis CI 설정      https://github.com/apps/travis-ci에 접속합니다.       우측 상단의 ‘Install’을 클릭합니다.         ‘Only select repositories’ 영역의 ‘Select repositories’를 클릭하여 travis를 연결할 레파지토리(repository)를 선택하고 ‘Install’을 클릭합니다.         travis가 GitHub repository에 접근할 수 있도록 권한 부여에 승인합니다.            https://travis-ci.com/으로 이동하여 승인이 진행됩니다.         우측 하단의 ‘Settings’를 클릭합니다.         ‘Environment Variables’ 영역에서 아래와 같이 입력하고 ‘Add’를 클릭합니다.            Name :: LINDAREX_GITHUB_TOKEN       Value :: 메모해 두었던 token              위 Name은 임의로 입력해도 무관합니다. 단, 이후 travis 설정과 동일해야 합니다.       3. travis.yml 파일 작성      실제 생성하는 파일명은 ‘.travis.yml’입니다.       아래와 같이 ‘.travis.yml’ 파일을 작성하여 jekyll 최상위 디렉터리(index.html 파일이 있는 경로)에 저장합니다.   travis 설정 시 Environment Variable에 입력한 name과 아래 github_token 값이 동일해야 합니다.   language: ruby rvm: - 2.3.3  script: bundle install &amp;&amp; bundle exec jekyll build  exclude: [vendor]  deploy:   provider: pages   skip_cleanup: true   github_token: $LINDAREX_GITHUB_TOKEN   keep_history: true   local-dir: ./_site   target-branch: master   on:     branch: sources  branches:   only:   - sources    4. deploy 확인     설정 완료 후, travis 첫 페이지으로 이동합니다.         로컬의 sources branch를 원격 저장소로 push 하면, travis에서 build하여 master branch로 자동 deploy됩니다.         build 중입니다.         정상 deploy되었습니다.      마무리(CONCLUSION)  travis로 jekyll을 GitHub Pages에 자동 deploy하는 방법을 완료했습니다.    다음 포스트에서는 travis로 다른 언어를 deploy하는 방법을 소개하겠습니다.   참고(REFERENCES)     https://travis-ci.com/   ","categories": ["travis-ci"],
        "tags": ["travis ci","github pages","jekyll"],
        "url": "https://lindarex.github.io/travis-ci/travis-github-pages-jekyll-setting/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 패키지(Package)로 Azure CLI 설치하기",
        "excerpt":"Azure CLI(command line interface, 명령줄 인터페이스)는 Microsoft Azure(azure)에 리소스(resource)를 만들고 관리할 수 있는 도구입니다.    모든 azure 서비스(service)에서 사용할 수 있으며, azure를 빠르게 사용할 수 있도록 자동화에 초점을 두고 있습니다.    이 포스트에서는 우분투(ubuntu) 환경에서 패키지(package)로 azure cli를 설치하는 방법을 소개합니다.   선행조건(PREREQUISITE)     ubuntu 환경이 필요합니다.      Ubuntu 설치 방법은 우분투(Ubuntu) 서버(Server) 16.04 설치하기 또는 우분투(Ubuntu) 서버(Server) 18.04 설치하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   Ubuntu 18.04.3 LTS (Bionic Beaver) Server (64-bit)   요약(SUMMARY)     azure cli debian packages repository 설정   apt 명령어로 azure cli 설치   azure cli 로그인   (선택사항) apt 명령어로 azure cli 삭제   내용(CONTENTS)  1. 종속 package 설치      Ubuntu 18.04 (Bionic)    $ sudo apt update $ sudo apt install -y ca-certificates curl apt-transport-https lsb-release gnupg   2. Microsoft 서명 키 설치  $ curl -sL https://packages.microsoft.com/keys/microsoft.asc | \\     gpg --dearmor | \\     sudo tee /etc/apt/trusted.gpg.d/microsoft.asc.gpg &gt; /dev/null   3. azure cli debian packages repository 추가  $ AZ_REPO=$(lsb_release -cs) $ echo \"deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPO main\" | \\     sudo tee /etc/apt/sources.list.d/azure-cli.list   4. azure cli 설치  $ sudo apt update $ sudo apt install azure-cli -y   5. azure 로그인  $ az login To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code HGCW1YCF3 to authenticate.   [   {     \"cloudName\": \"AzureCloud\",     \"id\": \"cd85e66d-8955-67b9-cdb5-cb6b706eb6b9\",     \"isDefault\": true,     \"name\": \"Microsoft Azure-LindaRex\",     \"state\": \"Enabled\",     \"tenantId\": \"2a755963-e627-665a-b039-ba6576750ae1\",     \"user\": {       \"name\": \"lindarex@lindarex.github.io\",       \"type\": \"user\"     }   } ]      위 로그인 결과는 잘못된 값이므로 참고만 하시기를 바랍니다.    6. (선택사항) azure cli 삭제  $ sudo apt remove azure-cli -y $ sudo apt autoremove -y $ sudo rm /etc/apt/sources.list.d/azure-cli.list $ sudo rm /etc/apt/trusted.gpg.d/microsoft.asc.gpg   마무리(CONCLUSION)  ubuntu 환경에 package로 azure cli 설치를 완료했습니다.    다음 포스트에서는 azure 사용을 위한 service principal, resource group, network security group, virtual network, subnet, storage account 등의 설정과 Inception VM 생성 방법을 소개하겠습니다.   참고(REFERENCES)     https://docs.microsoft.com/ko-kr/cli/azure/?view=azure-cli-latest  ","categories": ["azure"],
        "tags": ["azure","cli","ubuntu"],
        "url": "https://lindarex.github.io/azure/ubuntu-azure-cli-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 패키지(Package)로 Elastic Stack 설치하기",
        "excerpt":"Elastic Stack을 소개하기에 앞서 ELK Stack을 먼저 소개하겠습니다.    ELK Stack은 Elasticsearch, Logstash, Kibana의 연동으로 텍스트, 숫자, 위치 기반 정보, 정형 및 비정형 데이터(data) 등 모든 유형의 data를 수집 및 변환하고 분석하여 시각화하는 오픈소스(open source) 소프트웨어입니다.    2015년에 ELK Stack에 경량의 단일 목적 data 수집기 제품군(Beats)을 도입하면서 Elastic Stack으로 명칭이 변경되었습니다.   즉, ELK Stack에 Beats가 추가되어 Elastic Stack으로 통합되었습니다.    이 포스트에서는 우분투(ubuntu) 환경에서 패키지(package)로 elastic stack를 설치하는 방법을 소개합니다.   선행조건(PREREQUISITE)     ubuntu 환경에 Java가 설치되어 있어야 합니다.   방화벽 설정이 필요합니다.            TCP 9200 포트, TCP 5601 포트가 개방되어 있어야 합니다.              Java 설치 방법은 우분투(Ubuntu) 환경에 OpenJDK(Java) 설치하기 포스트를 참고하시기 바랍니다.       방화벽 설정 방법은 우분투(Ubuntu) 환경에 방화벽(Firewalld) 설치 및 설정하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   Ubuntu 16.04.6 LTS (Xenial Xerus) Server (64-bit)   Elasticsearch 7.2.0   Logstash 7.2.0   Kibana 7.2.0   Filebeat 7.2.0   OpenJDK 1.8.0_212   요약(SUMMARY)     Elasticsearch 설치   Logstash 설치   Kibana 설치   Filebeat 설치   웹브라우저로 Kibana 접속   내용(CONTENTS)  1. Elasticsearch 설치     Elasticsearch는 Apache Lucene으로 구축된 JSON 기반의 분산형 오픈소스 RESTful 검색 분석 엔진이며, Logstash를 통해 수신된 data를 저장소에 저장하는 역할을 담당합니다.   1.1. Elasticsearch debian packages repository 추가  $ echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list   1.2. Elasticsearch debian packages repository key 추가  $ wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -   1.3. apt install 명령어로 Elasticsearch 설치  $ sudo apt update &amp;&amp; sudo apt install elasticsearch -y   1.4. systemctl 명령어로 Elasticsearch 서비스(service) 관리  1.4.1. Elasticsearch service 설정 반영  $ sudo systemctl daemon-reload   1.4.2. Elasticsearch service 시작  $ sudo systemctl start elasticsearch.service   1.4.3. Elasticsearch service 중지  $ sudo systemctl stop elasticsearch.service   1.4.4. Elasticsearch service 재시작  $ sudo systemctl restart elasticsearch.service   1.4.5. Elasticsearch service 설정 재적용  $ sudo systemctl reload elasticsearch.service   1.4.6. Elasticsearch service 상태 조회  $ sudo systemctl status elasticsearch.service   1.4.7. Elasticsearch service 활성화(부팅 시 자동 시작)  $ sudo systemctl enable elasticsearch.service   1.4.8. Elasticsearch service 비활성화  $ sudo systemctl disable elasticsearch.service   1.4.9. Elasticsearch service 및 관련 프로세스(process) 모두 중지  $ sudo systemctl kill elasticsearch.service   1.5. curl 명령어로 Elasticsearch service 확인  $ curl -X GET http://localhost:9200 {   \"name\" : \"lindarex-elk\",   \"cluster_name\" : \"elasticsearch\",   \"cluster_uuid\" : \"RXW_zgPsS9mWiB5CDi2aCQ\",   \"version\" : {     \"number\" : \"7.2.0\",     \"build_flavor\" : \"default\",     \"build_type\" : \"deb\",     \"build_hash\" : \"508c38a\",     \"build_date\" : \"2020-02-20T15:54:18.811730Z\",     \"build_snapshot\" : false,     \"lucene_version\" : \"8.0.0\",     \"minimum_wire_compatibility_version\" : \"6.8.0\",     \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"   },   \"tagline\" : \"You Know, for Search\" }   2. Logstash 설치     Logstash는 서버(server) 사이드 data 처리 파이프라인으로, 여러 다양한 소스에서 동시에 data를 수집 및 변환하여 Elasticsearch와 같은 stash 보관소로 전송합니다.   2.1. (선택사항) SSL certificate 생성  2.1.1. Hostname or FQDN 설정  $ cd /etc/ssl/ $ sudo openssl req -x509 -nodes -newkey rsa:2048 -days 365 -keyout logstash-forwarder.key -out logstash-forwarder.crt -subj /CN=server.lindarex.local   2.1.2. IP Address 설정  $ sudo vi /etc/ssl/openssl.cnf   -------------------------------------------------------------------------------- ... [ v3_ca ] subjectAltName = IP:192.168.10.20 ... --------------------------------------------------------------------------------   $ cd /etc/ssl/ $ sudo openssl req -x509 -days 365 -batch -nodes -newkey rsa:2048 -keyout logstash-forwarder.key -out logstash-forwarder.crt   2.1.3. SSL 변환  $ cd /etc/ssl/ $ sudo openssl pkcs8 -in logstash-forwarder.key  -topk8 -nocrypt -out logstash-forwarder.key.pem $ sudo chmod 644 /etc/ssl/logstash-forwarder.key.pem   2.2. apt install 명령어로 Logstash 설치  $ sudo apt install logstash -y   2.3. Logstash 구성  $ sudo vi /etc/logstash/conf.d/logstash.conf   -------------------------------------------------------------------------------- input {  beats {    port =&gt; 5044        # Set to False if you do not SSL // (선택사항) SSL 미사용 시에 'false' 설정    ssl =&gt; true       # Delete below lines if no SSL is used  // (선택사항) SSL 미사용 시에 아래 설정 삭제    ssl_certificate =&gt; \"/etc/ssl/logstash-forwarder.crt\"    ssl_key =&gt; \"/etc/ssl/logstash-forwarder.key.pem\"    } }  filter { if [type] == \"syslog\" {     grok {       match =&gt; { \"message\" =&gt; \"%{SYSLOGLINE}\" }     }      date { match =&gt; [ \"timestamp\", \"MMM  d HH:mm:ss\", \"MMM dd HH:mm:ss\" ] }   }  }  output {  elasticsearch {   hosts =&gt; localhost     index =&gt; \"%{[@metadata][beat]}-%{+YYYY.MM.dd}\"        } stdout {     codec =&gt; rubydebug        } } --------------------------------------------------------------------------------   2.4. systemctl 명령어로 Logstash service 관리  2.4.1. Logstash service 설정 반영  $ sudo systemctl daemon-reload   2.4.2. Logstash service 시작  $ sudo systemctl start logstash.service   2.4.3. Logstash service 중지  $ sudo systemctl stop logstash.service   2.4.4. Logstash service 재시작  $ sudo systemctl restart logstash.service   2.4.5. Logstash service 설정 재적용  $ sudo systemctl reload logstash.service   2.4.6. Logstash service 상태 조회  $ sudo systemctl status logstash.service   2.4.7. Logstash service 활성화(부팅 시 자동 시작)  $ sudo systemctl enable logstash.service   2.4.8. Logstash service 비활성화  $ sudo systemctl disable logstash.service   2.4.9. Logstash service 및 관련 process 모두 중지  $ sudo systemctl kill logstash.service   3. Kibana 설치     Kibana는 프런트 엔드 애플리케이션으로, Elasticsearch에서 인덱스 된 data 검색 및 다양한 차트와 그래프를 제공하고, 실시간으로 data를 분석하여 시각화를 담당합니다.   3.1. apt install 명령어로 Kibana 설치  $ sudo apt install kibana -y   3.2. Kibana 구성  $ sudo vi /etc/kibana/kibana.yml   -------------------------------------------------------------------------------- ... #server.host: \"localhost\" server.host: \"192.168.10.20\" #elasticsearch.hosts: [\"http://localhost:9200\"] elasticsearch.hosts: [\"http://localhost:9200\"] ... --------------------------------------------------------------------------------   3.3. systemctl 명령어로 Kibana service 관리  3.3.1. Kibana service 설정 반영  $ sudo systemctl daemon-reload   3.3.2. Kibana service 시작  $ sudo systemctl start kibana.service   3.3.3. Kibana service 중지  $ sudo systemctl stop kibana.service   3.3.4. Kibana service 재시작  $ sudo systemctl restart kibana.service   3.3.5. Kibana service 설정 재적용  $ sudo systemctl reload kibana.service   3.3.6. Kibana service 상태 조회  $ sudo systemctl status kibana.service   3.3.7. Kibana service 활성화(부팅 시 자동 시작)  $ sudo systemctl enable kibana.service   3.3.8. Kibana service 비활성화  $ sudo systemctl disable kibana.service   3.3.9. Kibana service 및 관련 process 모두 중지  $ sudo systemctl kill kibana.service   4. Filebeat 설치     Filebeat는 경량 로그(log) 수집기로, SSH 터미널의 사용이 불가능한 상황(log를 생성하는 server나 가상 시스템, 컨테이너가 수백~수천 개에 이르는 경우)에 log와 파일을 경량화된 방식으로 전달하고 중앙 집중화하여, 작업을 보다 간편하게 만들어 주는 역할을 합니다.   4.1. apt install 명령어로 Filebeat 설치  $ sudo apt install filebeat -y   4.2. Filebeat 구성  $ sudo vi /etc/filebeat/filebeat.yml   -------------------------------------------------------------------------------- ... - type: log    # Change to true to enable this input configuration.   #enabled: false   enabled: true    # Paths that should be crawled and fetched. Glob based paths.   paths:     #- /var/log/*.log     - /var/log/syslog ... #-------------------------- Elasticsearch output ------------------------------ #output.elasticsearch:   # Array of hosts to connect to.   #hosts: [\"localhost:9200\"]    # Optional protocol and basic auth credentials.   #protocol: \"https\"   #username: \"elastic\"   #password: \"changeme\"  #----------------------------- Logstash output -------------------------------- output.logstash:    #hosts: [\"localhost:5044\"]   hosts: [\"192.168.10.20:5044\"]        # Comment out this line if you are not using SSL on Logstash server   #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]   ssl.certificate_authorities: [\"/etc/ssl/logstash-forwarder.crt\"] ... --------------------------------------------------------------------------------   4.3. systemctl 명령어로 Filebeat service 관리  4.3.1. Filebeat service 설정 반영  $ sudo systemctl daemon-reload   4.3.2. Filebeat service 시작  $ sudo systemctl start filebeat.service   4.3.3. Filebeat service 중지  $ sudo systemctl stop filebeat.service   4.3.4. Filebeat service 재시작  $ sudo systemctl restart filebeat.service   4.3.5. Filebeat service 설정 재적용  $ sudo systemctl reload filebeat.service   4.3.6. Filebeat service 상태 조회  $ sudo systemctl status filebeat.service   4.3.7. Filebeat service 활성화(부팅 시 자동 시작)  $ sudo systemctl enable filebeat.service   4.3.8. Filebeat service 비활성화  $ sudo systemctl disable filebeat.service   4.3.9. Filebeat service 및 관련 process 모두 중지  $ sudo systemctl kill filebeat.service   5. 웹브라우저로 Jenkins 접속     http://[MY-IP]:5601   마무리(CONCLUSION)  ubuntu 환경에 package로 elastic stack 설치를 완료했습니다.    다음 포스트에서는 elastic stack 사용 방법을 소개하겠습니다.   참고(REFERENCES)     https://www.elastic.co/kr/   https://ko.wikipedia.org/wiki/일래스틱서치  ","categories": ["elastic-stack"],
        "tags": ["elastic stack","elk stack","elasticsearch","logstash","kibana","filebeat","open source software","open source","oss","ubuntu"],
        "url": "https://lindarex.github.io/elastic-stack/ubuntu-elastic-stack-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 방화벽(UFW) 설정하기",
        "excerpt":"방화벽(UFW, uncomplicated firewall)은 데비안(debian) 계열 및 다양한 리눅스(linux) 환경에서 작동되고, GPL(GNU General Public License)이 적용되며 파이썬(python)으로 개발되었습니다.    ufw는 기본적으로 ubuntu 18.04 LTS 이후 버전에서 사용할 수 있습니다.   사용하기 쉬운 CLI(command line interface, 명령줄 인터페이스)를 사용하고, 프로그램 구성에 iptables를 사용하여 netfilter 방화벽(firewall)을 관리하는 프로그램입니다.    이 포스트에서는 ubuntu 환경에서 ufw를 설정하는 방법을 소개합니다.   선행조건(PREREQUISITE)     ubuntu 환경이 필요합니다.      Ubuntu 설치 방법은 VMware workstation에 ubuntu server 16.04 설치하기 또는 VMware workstation에 ubuntu server 18.04 설치하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   Ubuntu 18.04.3 LTS (Bionic Beaver) Server (64-bit)   요약(SUMMARY)     ufw 활성화 또는 비활성화   ufw 기본 정책(default rules) 조회   ufw default rules 허용 또는 차단   ufw rule 허용 또는 차단   ufw rule 삭제   서비스 명으로 ufw rule 허용 또는 차단   IP 주소로 ufw rule 허용 또는 차단   ufw ping(icmp) 허용 또는 차단   (선택사항) apt 명령어로 ufw 삭제   내용(CONTENTS)  1. ufw 활성화 또는 비활성화  1.1. ufw 활성화  $ sudo ufw enable      ufw은 기본적으로 비활성화되어 있습니다.    1.2. ufw 비활성화  $ sudo ufw disable   2. ufw 상태 조회  $ sudo ufw status verbose Status: inactive   3. ufw default rules 조회  $ sudo ufw show raw      아래 경로의 하위 파일을 조회하여 확인할 수 있습니다.    $ sudo cat /etc/ufw/user.rules *filter :ufw-user-input - [0:0] :ufw-user-output - [0:0] :ufw-user-forward - [0:0] :ufw-before-logging-input - [0:0] :ufw-before-logging-output - [0:0] :ufw-before-logging-forward - [0:0] :ufw-user-logging-input - [0:0] :ufw-user-logging-output - [0:0] :ufw-user-logging-forward - [0:0] :ufw-after-logging-input - [0:0] :ufw-after-logging-output - [0:0] :ufw-after-logging-forward - [0:0] :ufw-logging-deny - [0:0] :ufw-logging-allow - [0:0] :ufw-user-limit - [0:0] :ufw-user-limit-accept - [0:0] ### RULES ###  ### END RULES ###  ### LOGGING ### -A ufw-after-logging-input -j LOG --log-prefix \"[UFW BLOCK] \" -m limit --limit 3/min --limit-burst 10 -A ufw-after-logging-forward -j LOG --log-prefix \"[UFW BLOCK] \" -m limit --limit 3/min --limit-burst 10 -I ufw-logging-deny -m conntrack --ctstate INVALID -j RETURN -m limit --limit 3/min --limit-burst 10 -A ufw-logging-deny -j LOG --log-prefix \"[UFW BLOCK] \" -m limit --limit 3/min --limit-burst 10 -A ufw-logging-allow -j LOG --log-prefix \"[UFW ALLOW] \" -m limit --limit 3/min --limit-burst 10 ### END LOGGING ###  ### RATE LIMITING ### -A ufw-user-limit -m limit --limit 3/minute -j LOG --log-prefix \"[UFW LIMIT BLOCK] \" -A ufw-user-limit -j REJECT -A ufw-user-limit-accept -j ACCEPT ### END RATE LIMITING ### COMMIT   4. ufw default rules 허용 또는 차단  4.1. ufw default rules 허용  $ sudo ufw default allow   4.2. ufw default rules 차단  $ sudo ufw default deny   5. ufw rule 허용 또는 차단  5.1. TCP 8080 포트(port) 허용  $ sudo ufw allow 8080/tcp   5.2. TCP 8080 port 차단  $ sudo ufw deny 8080/tcp   5.3. UDP 22 port 허용  $ sudo ufw allow 22/udp   5.4. UDP 22 port 차단  $ sudo ufw deny 22/udp   5.5. TCP/UDP 53 port 허용  $ sudo ufw allow 53   5.6. TCP/UDP 53 port 차단  $ sudo ufw deny 53      53 port는 DNS 사용 port입니다.    6. ufw rule 삭제  6.1. TCP 8080 port 차단 rule 삭제  $ sudo ufw delete deny 8080/tcp   6.2. UDP 22 port 차단 rule 삭제  $ sudo ufw delete deny 22/udp   6.3. TCP/UDP 53 port 차단 rule 삭제  $ sudo ufw delete deny 53   7. 서비스(service) 명으로 ufw rule 허용 또는 차단      아래 명령어로 service 목록을 조회할 수 있습니다.    $ cat /etc/services      7.1. SSH service 허용  $ sudo ufw allow ssh   7.2. SSH service 차단  $ sudo ufw deny ssh   8. IP 주소(address)로 ufw rule 허용 또는 차단  8.1. IP address 허용  $ sudo ufw allow from 192.168.10.20   8.2. IP address 차단  $ sudo ufw deny from 192.168.10.20   8.3. IP address subnet(net mask) 허용  $ sudo ufw allow from 192.168.10.0/24   8.4. IP address subnet(net mask) 차단  $ sudo ufw deny from 192.168.10.0/24   8.5. IP address와 port 허용  $ sudo ufw allow from 192.168.10.20 to any port 22   8.6. IP address와 port 차단  $ sudo ufw deny from 192.168.10.20 to any port 22   8.7. IP address와 port, protocol 허용  $ sudo ufw allow from 192.168.10.20 to any port 22 proto tcp   8.8. IP address와 port, protocol 차단  $ sudo ufw deny from 192.168.10.20 to any port 22 proto tcp   9. ufw ping(icmp) 허용 또는 차단      ufw는 기본적으로 ping 요청을 허용합니다.    9.1. ufw ping(icmp) 허용  $ sudo vi /etc/ufw/before.rules   ... # ok icmp codes -A ufw-before-input -p icmp --icmp-type destination-unreachable -j ACCEPT -A ufw-before-input -p icmp --icmp-type source-quench -j ACCEPT -A ufw-before-input -p icmp --icmp-type time-exceeded -j ACCEPT -A ufw-before-input -p icmp --icmp-type parameter-problem -j ACCEPT -A ufw-before-input -p icmp --icmp-type echo-request -j ACCEPT ...   9.2. ufw ping(icmp) 차단  $ sudo vi /etc/ufw/before.rules   ... # ok icmp codes -A ufw-before-input -p icmp --icmp-type destination-unreachable -j DROP -A ufw-before-input -p icmp --icmp-type source-quench -j DROP -A ufw-before-input -p icmp --icmp-type time-exceeded -j DROP -A ufw-before-input -p icmp --icmp-type parameter-problem -j DROP -A ufw-before-input -p icmp --icmp-type echo-request -j DROP ...      ‘ACCEPT’를 ‘DROP’으로 변경합니다.    10. (선택사항) apt 명령어로 ufw 삭제     설정 파일을 유지하며 ufw를 삭제합니다.   $ sudo apt remove ufw $ sudo apt remove --auto-remove ufw      설정 파일과 함께 ufw를 삭제합니다. (단, 사용자 홈 디렉터리의 설정 파일은 유지됩니다.)     $ sudo apt purge ufw $ sudo apt purge --auto-remove ufw           마무리(CONCLUSION)  ubuntu 환경에 ufw 설정을 완료했습니다.    다음 포스트에서는 우분투(Ubuntu) 환경에 iptables 설정하기를 소개하겠습니다.   참고(REFERENCES)     https://help.ubuntu.com/community/UFW   http://manpages.ubuntu.com/manpages/bionic/en/man8/ufw.8.html  ","categories": ["ubuntu"],
        "tags": ["ufw","firewall","open source software","open source","oss","ubuntu"],
        "url": "https://lindarex.github.io/ubuntu/ubuntu-ufw-setting/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "클라우드 파운드리(Cloud Foundry, CFAR) 소개",
        "excerpt":"클라우드 파운드리(Cloud Foundry)는 과거에 PaaS(platform as a service)와 재단을 가리키는 용어였지만, 현재는 PaaS를 뜻하는 Cloud Foundry는 CFAR(cloud foundry application runtime)로 대체되었습니다.    CFAR은 개방형 기여(contribution) 및 개방형 거버넌스 모델(governance model)을 갖춘 Apache License 2.0으로 배포된 오픈소스 클라우드 애플리케이션 플랫폼(open source cloud application platform)입니다.    사용자가 업체(vendor)에 종속되지 않도록 유연성을 제공하고, 애플리케이션(application)을 더 빠르고 쉽게 구축(build)하여 테스트(test)하고 배포(deploy)하며 확장할 수 있는 기능을 제공합니다.    이 포스트에서는 CFAR을 소개합니다.   내용(CONTENTS)     CFAR은 클라우드(IaaS) 환경, 개발 프레임워크(Buildpack) 및 application 서비스(Service)를 선택할 수 있으며, application을 빠르고 쉽게 build, test하고 deploy 하며 확장할 수 있는 오픈소스(open source) 멀티 클라우드(cloud) application PaaS입니다.   CFAR의 컨테이너(container) 기반 아키텍처는 아마존 웹 서비스(AWS), 마이크로소프트 애저(Azure), 구글 컴퓨트 플랫폼(GCP), 오픈스택(OpenStack), VM웨어(VMware) vSphere 등의 다양한 cloud 환경 위에서 다양한 언어로 application을 실행할 수 있도록 합니다.   CFAR은 초기 개발부터 test, 그리고 deploy에 이르는 완전한 application 수명 주기(lifecycle)를 지원하기 때문에 지속적 배포(CD, continuous delivery)에 적합합니다.   CFAR은 루비(Ruby), 고언어(Go), 자바(Java)로 개발되었으며, BOSH deployment 스크립트(script)를 이용하여 IaaS에 deploy 합니다.      국내에서는 한국정보화진흥원(NIA, National Information Society Agency)에서 2014년부터 CFAR을 이용한 PaaS 아키텍처와 기능 분석을 시작했고, 2016년에 CFAR과 개발 도구 및 운영 도구를 패키징한 PaaS-TA라는 open source PaaS 플랫폼(platform)을 공개했습니다.    2016년 버전 1.0을 시작으로 다양한 서비스(service)와 버전 업그레이드를 통해 코스콤, KT, LG CNS, SK 등 민간 기업과 협력하면서 2019년에 버전 5.0까지 배포되었습니다.       open source project인 CFAR을 상용화한 Pivotal Cloud Foundry(PCF)와 IBM Cloud(IBM Bluemix) 등은 전 세계적으로 TOP 10 안에 드는 PaaS platform이며, Public, Private 또는 하이브리드(Hybrid) IaaS 환경에서 모두 활용 가능합니다.       IaaS(infrastructure as a service)란 서버(server) 자원(resource), 네트워크(network), 스토리지(storage) 등 인프라(infra) resources을 쉽고 편하게 이용할 수 있는 cloud service 형태로 제공하는 것을 의미하며, PaaS와 SaaS(software as a service)의 기반입니다.    server 가상화, 데스크톱 가상화 등의 기술로 구현되며, 대표적으로 AWS EC2(Elastic Cloud Compute), MS Azure, Google compute engine, OpenStack 등이 있습니다.       PaaS란 platform as a service의 약자로 application 개발을 위한 infra를 구축하고 유지보수하는 복잡함 없이, application을 개발하고 실행 및 관리할 있는 platform을 제공하는 service를 의미합니다.    대표적인 상용 PaaS로는 Pivotal Cloud Foundry, Google App Engine, Oracle Cloud Platform, Heroku 등이 있습니다.       SaaS는 cloud 환경에서 동작하는 application을 service 형태로 제공하는 것으로 software as a service의 약자입니다.    주문형 소프트웨어(on-demand software)라고도 하며, 대표적으로 Google Gmail, Google docs, Dropbox 등이 있습니다.       Buildpack이란 application을 CFAR로 배포(push)할 때, application에 대한 프레임워크(framework) 및 런타임(runtime) 지원을 제공합니다.    CFAR은 push 되는 application을 검사하여 적합한 buildpack을 감지하고, 이 buildpack은 내려받을 종속성(dependencies)을 결정하고 바인딩(binding) 된 service와 통신하도록 application 구성을 설정합니다.    buildpack에 대한 자세한 정보는 https://docs.cloudfoundry.org/buildpacks/를 확인해 주시기 바랍니다.       바인딩(binding)이란 일반적으로 하나를 다른 것으로 매핑시키는 것을 의미합니다.    Cloud Foundry에서 binding은 service instance를 push 된 application에 연결 또는 연관(association)시키는 것이며, 반대는 unbinding입니다.    1. 역사(history)     CFAR은 VMware에 의해 개발되어 Pivotal Software로 넘어간 후, 2015년 1월에 Cloud Foundry 재단이 비영리 독립 리눅스(linux) 재단 협업 project의 하나로 설립되어, Cloud Foundry 소프트웨어(소스 코드 및 모든 관련 상표)는 open source 소프트웨어 재단의 소유가 되었습니다.   현재 CFAR 개발은 Cloud Foundry 재단이 관리 및 통제하고 있습니다.   2. 아키텍처(architecture)     3. 지원되는 런타임 및 프레임워크(runtimes and frameworks)                  언어(language)       런타임(runtimes)       프레임워크(frameworks)                       자바(Java)       Java 6 ~ 8       스프링 프레임워크(Spring Frameworks) 3.x, 4.x                 루비(Ruby)       Ruby 1.8 ~ 2.2       레일즈(Rails), Sinatra                 Node.js       V8 자바스크립트 엔진 (구글 크롬)       Node.js                 스칼라(Scala)       Scala 2.x       플레이(play) 2.x, 리프트(lift)                 파이썬(Python)       Python 2.7.10 ~ 3.5.1       Python                 PHP       PHP 5.5 ~ 7.0       PHP                 Go       Go 1.1.1 ~ 1.4.2       Go              출처 :: https://ko.wikipedia.org/w/index.php?title=클라우드_파운드리&amp;action=edit&amp;section=4    4. 구성요소(component)     각 구성요소에 대한 최신 배포(release) 버전과 저장소(repository), 개별 설치 방법은 생략합니다.   4.1. Router     라우터(router)는 들어오는 트래픽을 클라우드 컨트롤러(Cloud Controller) 컴포넌트(component) 또는 디에고 셀(Diego Cell)에서 실행되는 호스팅 된 application으로 라우팅합니다.   router는 주기적으로 Diego BBS(bulletin board system)를 조회하여 현재 application이 실행 중인 셀(cell)과 container를 결정하고, 이 정보로 각 cell 가상 머신(VM, virtual machine)의 IP 주소(address)와 cell container의 호스트 측 포트(port) 번호를 기반으로 새로운 라우팅 테이블을 다시 계산합니다.      router에 대한 자세한 정보는 https://docs.cloudfoundry.org/concepts/architecture/router.html을 확인해 주시기 바랍니다.       Diego cell에 대한 자세한 정보는 https://docs.cloudfoundry.org/concepts/architecture/#diego-cell을 확인해 주시기 바랍니다.    4.2. OAuth2 UAA server     OAuth2 UAA(user account and authentication) server와 로그인(login) server는 함께 작동하여 ID 관리 기능을 제공합니다.   UAA는 OAuth2 제공자(provider)로서 CFAR 사용자를 대신하여 클라이언트(client) application이 사용할 수 있는 토큰(token)을 발행합니다.   UAA는 login server와 연동하여 CFAR 자격 증명(credentials)으로 사용자를 인증 할 수 있으며, 이러한 credentials나 다른 credentials를 사용하여 SSO service를 제공할 수 있습니다.   UAA에는 사용자 계정을 관리하고 OAuth2 client를 등록하기 위한 엔드 포인트(endpoints)와 다양한 관리 기능이 있습니다.   CFAR에는 기본적으로 두 개의 UAA 인스턴스(instance)가 있는데, 하나는 BOSH Director 용이며 다른 하나는 BOSH 배포(CFAR도 BOSH 배포에 포함) 용도로 사용됩니다.   하나의 runtime 또는 service에 login 하면, UAA를 사용하여 인증하는 다른 runtime 및 service에는 login 되지 않고, 각 runtime 또는 service에 별도로 login 해야 합니다.      UAA에 대한 자세한 정보는 https://docs.cloudfoundry.org/concepts/architecture/uaa.html을 확인해 주시기 바랍니다.    4.3. Cloud Controller     클라우드 컨트롤러(CC, cloud controller)는 application push를 지시합니다.   application을 CFAR로 push 하기 위해 CC를 대상(target)으로 하고, CC-Bridge components를 통해 Diego Brain에 지시하여 개별 Diego Cell을 조정한 후, application을 준비(stage)하고 실행합니다.   CC는 client가 시스템(system)에 접근할 수 있도록 REST API endpoints를 제공하고, 조직(orgs), 공간(spaces), 서비스(services), 사용자 역할(user roles) 등의 데이터베이스(database)를 유지하고 관리합니다.      CC에 대한 자세한 정보는 https://docs.cloudfoundry.org/concepts/architecture/cloud-controller.html을 확인해 주시기 바랍니다.    4.4. Blobstore     Blobstore는 큰 바이너리(binary) 파일(file)을 저장할 수 있는 repository입니다.   application code packages, buildpacks, droplets를 포함하고 있으며, 내부 server 또는 외부 S3, S3 호환 endpoints로 구성할 수 있습니다.      droplets이란 application을 CFAR로 push하고 buildpack을 사용하여 deploy 하면 생성되는 CFAR의 실행 단위입니다.    4.5. Diego Cell     application instances와 application tasks, staging tasks는 모두 Diego Cell VM에서 Garden container로 실행됩니다.   Diego cell rep component는 container의 lifecycle과 container에서 실행되는 프로세스(process)를 관리하고, container 상태를 Diego BBS에 보고하며, 로그(log)와 측정지표(metrics)를 Loggregator로 보냅니다.      Garden에 대한 자세한 정보는 https://docs.cloudfoundry.org/concepts/architecture/garden.html을 확인해 주시기 바랍니다.    4.6. Service Brokers     application은 database나 third-party SaaS provider와 같은 service에 의존하는데, service를 프로비저닝(provisioning)하고 application에 binding 할 경우, 해당 service의 service broker는 service instance를 제공하는 역할을 수행합니다.      프로비저닝(provisioning)이란 사용자의 요구에 맞게 system resource를 할당하고 배치하여 배포해 두었다가, 필요할 때 system을 즉시 사용할 수 있는 상태로 미리 준비해 두는 것을 의미합니다.       CFAR의 service에 대한 자세한 정보는 http://docs.cloudfoundry.org/services/index.html을 확인해 주시기 바랍니다.    4.7. Internal networking     CFAR component VM은 HTTP와 HTTPS 프로토콜(protocol)을 통해 내부적으로 서로 통신하며, Diego BBS에 저장된 임시 메시지(message)와 데이터(data)를 공유합니다.   BOSH Director는 deploy 된 모든 VM에 BOSH DNS server를 설치하고, 모든 VM은 다른 모든 VM에 대한 최신 DNS 레코드(record)를 같은 기반으로 유지하여 VM 간의 service 검색을 가능하게 합니다.   BOSH DNS는 여러 VM을 사용할 수 있을 때, 상태가 양호한 VM을 임의로 선택하여 client 측 로드 밸런싱(load balancing)을 제공합니다.   route-emitter component는 NATS protocol을 사용하여 최신 라우팅 테이블을 router로 전송합니다.   Diego BBS는 cell과 application 상태, 할당되지 않은 작업, heartbeat messages 등과 같이 자주 업데이트되는 data와 일회성 data, 수명이 긴 distributed locks를 저장하며, Go MySQL 드라이버를 사용하여 MySQL에 data를 저장합니다.      BBS(Bulletin Board System)에 대한 자세한 정보는 https://docs.cloudfoundry.org/concepts/diego/diego-architecture.html#bbs를 확인해 주시기 바랍니다.       BOSH DNS에 대한 자세한 정보는 https://bosh.io/docs/dns/를 확인해 주시기 바랍니다.    4.8. Loggregator     Loggregator(log aggregator) system은 application log를 스트리밍하여 개발자에게 전송합니다.      Loggregator에 대한 자세한 정보는 https://docs.cloudfoundry.org/loggregator/architecture.html을 확인해 주시기 바랍니다.    4.9. Metrics Collector     Metrics Collector는 component로부터 metrics와 통계치(statistics)를 수집하고, 운영자는 이 정보를 사용하여 Cloud Foundry deployment를 모니터링할 수 있습니다.   마무리(CONCLUSION)  CFAR에 대한 소개와 architecture, component 등을 살펴보았습니다.    CFAR을 비롯해서 Cloud, PaaS, IaaS, SaaS 등의 기술 트랜드는 선택이 아닌 필수가 되었습니다. 대기업 및 금융권 등 현업에서는 Cloud 전환 프로젝트가 빠르게 진행 중이고, 공공 기관에서도 위에 소개한 PaaS-TA를 이용한 다양한 과제가 진행되고 있습니다.    CFAR에 대한 더 자세한 내용은 앞으로 다양한 포스트로 소개할 예정입니다. 다음 포스트에서는 클라우드 파운드리(Cloud Foundry) 프로젝트(projects)를 소개하겠습니다.   참고(REFERENCES)     https://www.cloudfoundry.org/   https://docs.cloudfoundry.org/concepts/architecture/   https://ko.wikipedia.org/wiki/클라우드_파운드리   http://www.ciokorea.com/news/37345   https://www.trustradius.com/platform-as-a-service-paas   https://www.updatedreviews.in/top-10-best-paas-providers   https://www.slant.co/topics/3478/~best-platform-as-a-service-providers-that-have-a-free-plan   https://www.devteam.space/blog/10-top-paas-providers/   ","categories": ["cfar"],
        "tags": ["cloud foundry","cf","cfar","paas","open source software","open source","oss"],
        "url": "https://lindarex.github.io/cfar/cloud-foundry-cfar-introduction/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "클라우드 파운드리(Cloud Foundry) 프로젝트(projects) 소개",
        "excerpt":"이 포스트에서는 클라우드 파운드리(Cloud Foundry) 프로젝트(projects)를 간단히 소개합니다.   내용(CONTENTS)  1. BOSH     BOSH는 복잡한 분산 시스템의 release engineering, 배포(deploy) 및 애플리케이션(application) 수명 주기(lifecycle) 관리를 위한 클라우드(cloud) 오픈소스(open source) 소프트웨어입니다.   BOSH는 가상 머신(VM, virtual machine)이나 컨테이너(container), 베어 메탈(bare metal) 등 어디에 deploy 하든 상관없이 동일한 접근 방식을 취하고, 대부분의 application을 deploy 할 때 BOSH가 사용됩니다.   BOSH는 백그라운드에서 작동하며, 구성요소(component)가 변경될 때, application을 최신 상태로 유지하고, 환경이 올바르게 구성되었는지 확인하여 환경이 정의되었을 때 실행 방식을 조정하도록 재구성합니다.   BOSH는 스템셀(stemcell), 릴리스(release) 및 배포 매니페스트(deployment manifest)를 사용하여 cloud 환경의 무결성(integrity)을 유지합니다.            stemcell                    VM을 생성하는 데 사용되는 골든 운영체제 이미지(golden operating system image)와 유사합니다.           배포에 포함된 기본 운영체제(OS, operating system)를 다른 application 패키지(package)와 분리합니다.                       release                    stemcell 위에 놓인 레이어(layer)로 어떤 application을 deploy하고 어떻게 구성해야 하는지 BOSH에 application을 패키지화하여 전달합니다.           release는 구성 속성(configuration properties) 및 템플릿(templates), 시작(startup) scripts, 소스 코드(source code), 바이너리(binary) 아티팩트(artifacts)를 포함하며, 재현 가능한 방식(reproducible)으로 application을 build하고 deploy 하는 데 필요한 모든 것이 포함됩니다.                       deployment manifest                    어떤 cloud 또는 인프라(infra)에 어떤 BOSH release를 deploy하고, 어떻게 구성해야 하는지 설명하는 yaml file입니다.           BOSH는 manifest file을 사용하여 대상 infra에 deploy하고, VM 또는 container의 상태를 모니터링하고, 필요시에 복구합니다.                           BOSH는 CPI(cloud provider interface) 모델(model)을 포함하며, 이는 멀티(multi) cloud 기능(capability)의 핵심입니다.   BOSH는 CPI를 사용하여 AWS, Azure, GCP, OpenStack, VMware vSphere 등을 포함한 여러 cloud 환경에 application을 deploy 할 수 있습니다.   CPI는 BOSH가 infra와 상호 작용하여 stemcell, VM 및 디스크(disk)를 생성하고 관리하는 데 사용하는 API입니다.   BOSH는 초기에 CFAR을 배포하기 위해 생성되었지만, 현재는 다른 환경에서 모든 종류의 application을 package하고 관리하기 위해 사용되고 있습니다.      Bare metal이란 소프트웨어가 설치되어 있지 않은 단일 테넌트(single-tenant) 물리적 하드웨어를 의미하며, 가상화 및 cloud 호스팅(hosting)과 구별하기 위해 사용됩니다.    bare metal에 대한 자세한 정보는 https://en.wikipedia.org/wiki/Bare-metal_server를 확인해 주시기 바랍니다.    2. CFAR(cloud foundry application runtime)     CFAR은 클라우드 환경과 개발 프레임워크, application 서비스(service)를 선택할 수 있으며, application을 빠르고 쉽게 구축하고 테스트하여 deploy 할 수 있는, 완전한 application lifecycle을 지원하는 open source multi cloud application PaaS(platform as a service)입니다.      CFAR에 대한 자세한 설명은 클라우드 파운드리(Cloud Foundry, CFAR) 소개 포스트를 참고하시기 바랍니다.    3. CFCR(cloud foundry container runtime)     CFCR은 kubernetes를 사용하여 container를 더 세밀하게 제어하고 관리할 수 ​​있는 기능을 제공합니다.   CFCR은 과거에 Cloud Foundry Foundation 내의 incubating project인 Project Kubo였는데, container platform의 성장과 함께 container runtime인 kubo를 CFCR로 명칭을 변경했습니다.   CFCR은 2017년 11월에 kubernetes 적합성 인증(Certified Kubernetes)을 받았고, BOSH를 사용하여 cloud platform에서 고가용성(high availability)의 kubernetes 클러스터(cluster)를 인스턴스화하고 deploy 및 관리할 수 있는 기능을 제공합니다.   kubernetes와 BOSH의 조합으로 BOSH를 통해 deploy 및 lifecycle를 관리하면, kubernetes cluster의 스케일링(scaling)과 VM healing, 롤링 업그레이드(rolling upgrade)를 할 수 있습니다.      Certified Kubernetes란 CNCF(Cloud Native Computing Foundation)의 Kubernetes Software Conformance Certification Program으로 kubernetes의 목표인 일관성과 휴대성을 위한 검토와 인증에 대한 적합성 시험 결과를 CNCF로 제출하여, CNCF로부터 준수 구현을 공식적으로 인증받은 것을 의미합니다.    Certified Kubernetes에 대한 자세한 정보는 https://www.cncf.io/certification/software-conformance/를 확인해 주시기 바랍니다.    4. Quarks     Project Quarks(quarks)는 Cloud Foundry Foundation 내의 incubating project로, CFAR을 VM 대신 container로 패키징하여 kubernetes에 쉽게 deploy 할 수 있는 기능을 제공합니다.   container형 CFAR은 BOSH로 deploy 한 CFAR과 동일한 기능을 제공하고, 더 적은 infra 용량으로 kubernetes 운영자에게 익숙한 환경을 제공합니다.   quarks는 기존 BOSH release를 kubernetes에 설치하기 위해 docker 이미지(image) 또는 helm charts로 변환합니다.   kubernetes는 CFAR instance의 자동 확장 및 실패한 노드(node)를 복구하는 기능을 제공합니다.   kubernetes를 기본 infra로 사용하면 quarks를 모든 private 또는 public cloud에 쉽게 deploy 할 수 있습니다.      helm은 Kubernetes의 package 관리자이며, helm을 사용하면 application 및 리소스(resources)를 Kubernetes cluster에 쉽게 설치할 수 있습니다.    helm charts는 Kubernetes resources 세트를 Kubernetes cluster에 설치하기 위한 helm package이며, helm charts에는 chart.yaml, templates, values.yaml 및 의존성(dependencies)이 포함됩니다.    helm에 대한 자세한 정보는 https://helm.sh/를 확인해 주시기 바랍니다.    5. Eirini     Project Eirini(eirini)는 Cloud Foundry Foundation 내의 incubating project로, CFAR을 위한 플러그형 스케줄링을 가능하게 하며 application container instance를 조정하기 위해 Diego/Garden 또는 Kubernetes 중에서 선택할 수 있습니다.   eirini는 기존 Kubernetes cluster infra를 재사용하여 CFAR에 의해 deploy 된 application을 hosting 합니다.   개발자가 buildpack을 사용하여 application을 push 하면, kubernetes 내부에서 일회성 스테이징 작업이 포드(pod)로 실행되고, 드롭릿(droplet)이 동일한 방식으로 생성하여 업로드됩니다. 이 과정에서 application은 docker image로 다운로드되어 kubernetes에 deploy 됩니다.   eirini는 CFAR 운영자가 kubernetes를 container 스케줄러로 선택할 수 있도록 하여, kubernetes에 익숙한 조직이 CFAR을 보다 쉽게 이용할 수 있도록 합니다.   6. Open Service Broker API     Open Service Broker API project를 통해 독립적인 application vendor와 SaaS provider는 CFAR과 kubernetes와 같은 cloud native platform에서 실행되는 워크로드(workload)에 대한 백 앤드 service를 쉽게 제공할 수 있습니다.   Open Service Broker API는 많은 platform과 service provider가 채택하였고, Google, IBM, Pivotal, Red Hat, SAP 등 다수의 cloud 회사들이 기여했습니다.   Open Service Broker API 사양의 service broker는 service lifecycle을 관리하고, platform은 service broker와 상호 작용하여 service를 provisioning 및 access하고 관리합니다.   마무리(CONCLUSION)  Cloud Foundry projects를 살펴보았습니다.    BOSH는 Cloud Foundry projects에 기반이 되는 프로젝트이고, CFAR을 제외한 나머지 CFCR, Quarks, Eirini 등의 공통 키워드는 Kubernetes입니다. 더 말이 필요 없는 kubernetes…    다음 포스트에서는 kubernetes 소개와 CFAR을 설치하는 방법을 소개하겠습니다.   참고(REFERENCES)     https://www.cloudfoundry.org/   https://www.cncf.io/   https://github.com/cncf/k8s-conformance  ","categories": ["cfar"],
        "tags": ["cloud foundry","cf","cfar","cfcr","bosh","quarks","eirini","kubernetes","paas","open source software","open source","oss"],
        "url": "https://lindarex.github.io/cfar/cloud-foundry-projects-introduction/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 iptables 설정하기",
        "excerpt":"iptables은 방화벽(firewall) 구성이나 NAT(network address translation)에 사용되는데, 리눅스(linux) 커널(kernel) firewall이 제공하는 테이블(table)과 체인(chain), 규칙(rule)을 시스템 관리자가 구성합니다.    각각 다른 kernel 모듈(module)과 프로그램들은 다른 프로토콜(protocol)을 위해 사용되는데, iptables는 IPv4에, ip6tables는 IPv6에, arptables는 ARP에, ebtables는 이더넷(ethernet) 프레임에 적용됩니다.   linux 시스템(system)에서 iptables는 /usr/sbin/iptables에 설치되며, iptables의 후속 버전은 nftables입니다.    이 포스트에서는 ubuntu 환경에서 iptables를 설정하는 방법을 소개합니다.      nftables이란 iptables에 비해 코드 중복이 적고 처리량이 더 많은 iptables의 후속 버전이며, linux kernel 3.13부터 사용 가능합니다. nftables에 대한 자세한 정보는 https://en.wikipedia.org/wiki/Nftables를 확인해 주시기 바랍니다.    선행조건(PREREQUISITE)     ubuntu 환경이 필요합니다.      Ubuntu 설치 방법은 우분투(Ubuntu) 서버(Server) 16.04 설치하기 또는 우분투(Ubuntu) 서버(Server) 18.04 설치하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   Ubuntu 18.04.4 LTS (Bionic Beaver) Server (64-bit)   요약(SUMMARY)     iptables 확인 및 초기화   iptables 조회   iptables 용어   iptables 사용 방법   내용(CONTENTS)     iptables는 kernel 2.4 이전 버전에서 사용되던 ipchains를 대신하는 firewall 도구이며, NetFilter 프로젝트에서 개발했습니다.   iptables는 kernel에서 netfilter 패킷(packet) 필터링(filtering) 기능을 사용자 공간에서 제어하는 수준으로 사용하고, protocol 상태 추적, packet 애플리케이션(application) 계층(layer) 검사, 속도 제한, filtering 정책(policy) 등의 기능을 제공합니다.   ubuntu 18.04는 기본적으로 iptables와 함께 UFW(Uncomplicated Firewall, ufw)를 제공합니다.   iptables를 설정하기 전에 ufw를 비활성화하거나 제거하는 것을 추천합니다.   iptables 설정에는 ROOT 권한을 요구하기 때문에, 이 포스트에서는 ROOT 사용자가 설정한다는 가정하에 설명합니다.      ufw에 대한 자세한 설명은 우분투(Ubuntu) 환경에 방화벽(UFW) 설정하기 포스트를 참고하시기 바랍니다.    1. iptables 확인 및 초기화     ROOT 권한 필요    1.1. iptables 확인  # which iptables /sbin/iptables   # iptables -V iptables v1.6.1   1.2. iptables 초기화     모든 chain에 설정된 모든 rule을 제거합니다.   # iptables -F      기본 chain을 제외한 나머지 모든 chain에 설정된 모든 rule을 제거합니다.   # iptables -X   2. iptables 조회     아래 명령어의 결과는 iptables의 기본 설정 상태입니다.       기본 조회입니다.   # iptables -L chain INPUT (policy ACCEPT) target     prot opt source               destination  chain FORWARD (policy ACCEPT) target     prot opt source               destination  chain OUTPUT (policy ACCEPT) target     prot opt source               destination      아래는 조회에 사용되는 ‘-L’ command의 옵션 목록입니다.            ‘-v’ 또는 ‘–verbose’ :: 각 chain의 packet과 byte counters 정보, 각 rule에 일치하는 packet과 byte counters 정보 및 특정 rule에 적용되는 인터페이스(interface) 등을 조회합니다.       ‘-n’ 또는 ‘–numeric’ :: 기본 호스트(host) 이름, 네트워크(network) 이름 또는 서비스(service) 형식이 아닌 IP 주소(address)와 포트(port) 번호(number)로 표시합니다.       ‘-x’ 또는 ‘–exact’ :: packet과 byte counters 정보를 정확한 값으로 확장하여 표시합니다.       ’–line-numbers’ :: chain의 각 rule의 시작 부분에 숫자를 추가합니다.           상세 조회입니다.   # iptables -L -v Chain INPUT (policy ACCEPT 10 packets, 676 bytes)  pkts bytes target     prot opt in     out     source               destination  Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)  pkts bytes target     prot opt in     out     source               destination  Chain OUTPUT (policy ACCEPT 5 packets, 812 bytes)  pkts bytes target     prot opt in     out     source               destination      IP address와 port number로 표시하여 조회합니다.   # iptables -L -n Chain INPUT (policy ACCEPT) target     prot opt source               destination  Chain FORWARD (policy ACCEPT) target     prot opt source               destination  Chain OUTPUT (policy ACCEPT) target     prot opt source               destination      정확한 값으로 확장하여 조회합니다.   # iptables -L -x Chain INPUT (policy ACCEPT) target     prot opt source               destination  Chain FORWARD (policy ACCEPT) target     prot opt source               destination  Chain OUTPUT (policy ACCEPT) target     prot opt source               destination      rule의 시작 부분에 숫자를 추가하여 조회합니다.   # iptables -L --line-numbers Chain INPUT (policy ACCEPT) num  target     prot opt source               destination  Chain FORWARD (policy ACCEPT) num  target     prot opt source               destination  Chain OUTPUT (policy ACCEPT) num  target     prot opt source               destination      전체 rule을 출력합니다.   # iptables -S -P INPUT ACCEPT -P FORWARD ACCEPT -P OUTPUT ACCEPT      전체 rule을 상세하게 출력합니다.   # iptables -S -v -P INPUT ACCEPT -c 117 8240 -P FORWARD ACCEPT -c 0 0 -P OUTPUT ACCEPT -c 103 13208      ‘-L’과 ‘-S’ 옵션의 차이점은 설명으로는 조회와 출력이 차이지만, 산출 형식(output format)이 다릅니다. ‘-L’ 옵션은 reference, 즉 참조용이지만, ‘-S’ 옵션은 reusable output, 즉 재사용을 위한 출력입니다. ‘-S’ 옵션은 iptables-save 방식으로 생성되어 iptables-apply 및 iptables-restore에 다시 사용할 수 있습니다.    3. iptables 용어  3.1. target     target은 packet이 rule과 일치할 때 실행될 명령입니다.   각각의 target에 대한 설명은 아래와 같습니다.            ACCEPT :: packet을 허용합니다.       DROP :: packet을 차단하고, 사용자에게 오류 메시지를 보내지 않습니다.       REJECT :: packet을 차단하고, 사용자에게 오류 메시지를 보냅니다.       LOG :: packet을 syslog에 기록합니다.       RETURN :: chain 통과를 중지하고, 이전(호출) chain의 다음 rule에 따라 다시 시작합니다.       QUEUE :: application에 packet을 대기(queue)시키는 데 사용합니다.       SNAT :: 소스 네트워크 주소 변환(source network address translation)에 사용되며, 이는 packet의 IP 헤더에 source IP address를 다시 쓰는 데 사용됩니다.       DNAT :: 대상 네트워크 주소 변환(destination network address translation)에 사용되며, 이는 packet의 destination IP address를 다시 쓰는 데 사용됩니다.           3.2. chain     chain은 순차적으로 검사하는 rule 세트이며, packet이 rule 중 하나와 일치하면 관련 작업이 실행되고, chain의 나머지 rule은 확인하지 않습니다.   filter table에는 미리 정의된 INPUT, FORWARD, OUTPUT 등 3개의 chain이 존재하며, 그 외에 PREROUTING, POSTROUTING 등 2개의 chain이 존재합니다.   INPUT, FORWARD, OUTPUT chain은 기본 chain으로 삭제가 안 되고 영구적으로 사용되며, ‘-N’ 옵션으로 사용자가 chain을 구성하여 사용할 수 있습니다.   chain은 network 통신(IP packet)에 미리 설정한 rule을 적용하여 target을 결정합니다.   각각의 chain에 대한 설명은 아래와 같습니다.            INPUT :: system으로 들어오는 packet의 policy rules입니다.       OUTPUT :: system에서 나가는 packet의 policy rules입니다.       FORWARD :: system에서 다른 system으로 보내는 packet의 policy rules입니다.       PREROUTING :: packet이 INPUT chain에 도달하기 전에 packet을 변경합니다.       POSTROUTING :: packet이 OUTPUT chain을 종료한 후에 packet을 변경합니다.           3.3. table     iptables는 filter, nat, mangle, raw, security 등 5개의 table이 존재합니다.        ‘-t’ 또는 ‘–table’ 옵션으로 packet matching table을 지정합니다.       filter table            기본 table이며, ‘-t’ 옵션이 없을 때 적용합니다.       filter table은 built-in chain INPUT, FORWARD, OUTPUT으로 구성됩니다.           nat table            nat table은 새로운 연결을 생성하는 packet이 발견되면 참조됩니다.       nat table은 built-in chain PREROUTING, INPUT, OUTPUT, POSTROUTING으로 구성됩니다.       IPv6 NAT는 kernel 3.7부터 지원합니다.           mangle table            mangle table은 특수한 packet 변경에 사용됩니다.       mangle table은 kernel 2.4.17까지 built-in chain PREROUTING, OUTPUT으로 구성되었다가, kernel 2.4.18부터 built-in chain INPUT, FORWARD, POSTROUTING도 추가 구성되었습니다.           raw table            raw table은 NOTRACK 대상과 함께 연결 추적의 제외 구성 시에 사용됩니다.       raw table은 built-in chain PREROUTING, OUTPUT으로 구성됩니다.           security table            security table은 SELinux와 같은 linux 보안 module에 의해 구현되는 Mandatory Access Control(MAC) 네트워킹 rule에 사용됩니다.       security table은 filter table 다음에 호출되기 때문에, security table MAC rule이므로 filter table의 Discretionary Access Control(DAC)보다 나중에 적용됩니다.       security table은 built-in chain INPUT, FORWARD, OUTPUT으로 구성됩니다.           3.4. parameter     iptables의 rule 스펙(specification)을 구성하며, add, delete, insert, replace 및 append 명령어에 사용됩니다.   각각의 parameter에 대한 설명은 아래와 같습니다.            ‘-4’ 또는 ‘–ipv4’ :: iptables 및 iptables-restore에 영향이 없고, 단일 rule 파일(file)에서 iptables-restore 및 ip6tables-restore와 함께 사용하여 IPv4 및 IPv6 rule을 허용합니다.       ‘-6’ 또는 ‘–ipv6’ :: ip6tables 및 ip6tables-restore에 영향이 없고, 단일 rule 파일(file)에서 iptables-restore 및 ip6tables-restore와 함께 사용하여 IPv4 및 IPv6 rule을 허용합니다.       ‘-p’ 또는 ‘–protocol’ :: rule 또는 packet의 protocol을 확인합니다.       ‘-s’ 또는 ‘–source’ :: 출발지 network 이름, host 이름, network IP address(/mask) 또는 일반 IP address를 확인합니다.       ‘-d’ 또는 ‘–destination’ :: 목적지 network 이름, host 이름, network IP address(/mask) 또는 일반 IP address를 확인히며, 별칭(alias)은 ‘-dst’입니다.       ‘-m’ 또는 ‘–match’ :: 사용할 확장 패킷 모듈(extended packet modules)을 지정합니다.       ‘-j’ 또는 ‘–jump’ :: rule의 target을 지정합니다.       ‘-g’ 또는 ‘–goto’ :: 사용자 지정 chain으로 계속 처리되도록 명시합니다.       ‘-i’ 또는 ‘–in-interface’ :: packet을 수신할 interface를 명시합니다.       ‘-o’ 또는 ‘–out-interface’ :: packet이 전송될 interface를 명시합니다.       ‘-f’ 또는 ‘–fragment’ :: 조각난 packet의 두 번째와 그 이후 IPv4 조각만을 참조하는 rule을 명시합니다.       ‘-c’ 또는 ‘–set-counters’ :: INSERT, APPEND, REPLACE 조작 중에 rule의 packet 및 바이트 카운터(byte counters)를 초기화할 수 있습니다.           3.5. command     iptables이 수행할 작업을 지정합니다.   각각의 command에 대한 설명은 아래와 같습니다.            ‘-A’ 또는 ‘–append’ :: 선택한 chain 끝에 하나 이상의 rule을 추가합니다.       ‘-C’ 또는 ‘–check’ :: 선택한 chain의 rule과 일치하는 rule이 있는지 확인합니다.       ‘-D’ 또는 ‘–delete’ :: 선택한 chain에서 하나 이상의 rule을 삭제합니다.       ‘-I’ 또는 ‘–insert’ :: rule number로 선택한 chain에 하나 이상의 rule을 삽입합니다.       ‘-R’ 또는 ‘–replace’ :: rule number로 선택한 chain에서 rule을 교체합니다.       ‘-L’ 또는 ‘–list’ :: 선택한 chain의 모든 rule을 조회하며, chain을 선택하지 않으면 모든 chain의 rule을 조회합니다.       ‘-S’ 또는 ‘–list-rules’ :: 선택한 chain의 모든 rule을 출력하며, chain을 선택하지 않으면 모든 chain의 rule을 출력합니다.       ‘-F’ 또는 ‘–flush’ :: 선택한 chain의 rule을 모두 삭제합니다.       ‘-N’ 또는 ‘–new’ :: 사용자 정의 chain을 생성합니다.       ‘-E’ 또는 ‘–rename-chain’ :: 사용자 정의 chain의 이름을 변경합니다.       ‘-X’ 또는 ‘–delete-chain’ :: 사용자 정의 chain을 삭제합니다. 단, chain에 대한 참조(references)가 없고, rule이 없이 비어있어야 합니다.       ‘-P’ 또는 ‘–policy’ :: 사용자 정의 chain을 제외한 built-in chain의 policy를 지정한 target으로 설정합니다.       ‘-Z’ 또는 ‘–zero’ :: 모든 chain 또는 선택한 chain, chain의 지정된 rule의 packet과 byte counters를 0으로 설정합니다.           4. iptables 사용 방법  4.1. rule 추가     ‘-A’ 또는 ‘–append’ command를 사용합니다.   # iptables [table] -A chain rule-specification      상세히 설명하면 아래와 같습니다.   # iptables [-t TABLE-NAME] -A chain -m MATCH-NAME [match-options] -j TARGET-NAME [target-options]      예제 :: INPUT chain의 localhost 접속 허용 rule을 추가합니다.   # iptables -A INPUT -i lo -j ACCEPT   4.2. rule 확인     ‘-C’ 또는 ‘–check’ command를 사용합니다.   # iptables [table] -C chain rule-specification      상세히 설명하면 아래와 같습니다.   # iptables [-t TABLE-NAME] -C chain -m MATCH-NAME [match-options] -j TARGET-NAME [target-options]      예제 :: INPUT chain의 tcp 8080 포트(port) 접속 허용 rule을 확인합니다.   # iptables -C INPUT -p tcp --dport 8080 -j ACCEPT   4.3. rule 삭제     ‘-D’ 또는 ‘–delete’ command를 사용합니다.   # iptables [table] -D chain rule-specification      상세히 설명하면 아래와 같습니다.   # iptables [-t TABLE-NAME] -D chain -m MATCH-NAME [match-options] -j TARGET-NAME [target-options]      예제 :: INPUT chain의 tcp 22 port 접속 차단 rule을 삭제합니다.   # iptables -D INPUT -p tcp -m tcp --dport 22 -j REJECT   4.4. rule number로 rule 삭제     ‘-D’ 또는 ‘–delete’ command를 사용합니다.   # iptables [table] -D chain rule-number      상세히 설명하면 아래와 같습니다.   # iptables [-t TABLE-NAME] -D chain rule-number      예제 :: FORWARD chain의 rule number 1번 rule을 삭제합니다.   # iptables -D FORWARD 1   4.5. rule 삽입     ‘-I’ 또는 ‘–insert’ command를 사용합니다.   # iptables [table] -I [rule-number] rule-specification      상세히 설명하면 아래와 같습니다.   # iptables [-t TABLE-NAME] -I chain [rule-number] -m MATCH-NAME [match-options] -j TARGET-NAME [target-options]      예제 :: INPUT chain의 rule number 1번에 udp 53 port 접속 차단 rule을 삽입합니다.   # iptables -I INPUT 1 -p udp --dport 53 -j DROP   4.6. rule 교체     ‘-R’ 또는 ‘–replace’ command를 사용합니다.   # iptables [table] -R rule-number rule-specification      상세히 설명하면 아래와 같습니다.   # iptables [-t TABLE-NAME] -R chain rule-number -m MATCH-NAME [match-options] -j TARGET-NAME [target-options]      예제 :: INPUT chain의 rule number 1번에 출발지 network ‘10.0.1.0/24’ 대역의 tcp 8080 port 접속 허용 rule을 교체합니다.   # iptables -R INPUT 1 -p tcp -s 10.0.1.0/24 --dport 8080 -j ACCEPT   4.7. rule 조회     ‘-L’ 또는 ‘–list’ command를 사용합니다.   # iptables [table] -L [chain [rule-number]] [options]      상세히 설명하면 아래와 같습니다.   # iptables [-t TABLE-NAME] -L [chain [rule-number]] [options]      예제 :: 모든 rule을 service 이름으로 조회합니다.   # iptables -L      예제 :: 모든 rule을 port number로 조회합니다.   # iptables -L -n   4.8. rule 모두 삭제     ‘-F’ 또는 ‘–flush’ command를 사용합니다.   # iptables [table] -F [chain [rule-number]] [options]      상세히 설명하면 아래와 같습니다.   # iptables [-t TABLE-NAME] -F [chain [rule-number]] [options]      예제 :: 모든 chain의 rule을 삭제합니다.   # iptables -F      예제 :: FORWARD chain의 모든 rule을 삭제합니다.   # iptables -F FORWARD   4.9. rule 출력     ‘-S’ 또는 ‘–list-rules’ command를 사용합니다.   # iptables [table] -S [chain [rule-number]]      상세히 설명하면 아래와 같습니다.   # iptables [-t TABLE-NAME] -S [chain [rule-number]]      예제 :: OUTPUT chain의 rule number 1번 rule을 출력합니다.   # iptables -S OUTPUT 1   4.10. 사용자 정의 chain 생성     ‘-N’ 또는 ‘–new’ command를 사용합니다.   # iptables [table] -N chain      상세히 설명하면 아래와 같습니다.   # iptables [-t TABLE-NAME] -N chain      예제 :: ‘lindarex-chain-income’이라는 이름의 사용자 정의 chain을 생성합니다.   # iptables -N lindarex-chain-income   4.11. 사용자 정의 chain 이름 변경     ‘-E’ 또는 ‘–rename-chain’ command를 사용합니다.   # iptables [table] -E old-chain-name new-chain-name      상세히 설명하면 아래와 같습니다.   # iptables [-t TABLE-NAME] -E old-chain-name new-chain-name      예제 :: 생성한 ‘lindarex-chain-income’이라는 이름의 사용자 정의 chain의 이름을 ‘lindarex-chain-income-new’로 변경합니다.   # iptables -E lindarex-chain-income lindarex-chain-income-new   4.12. 사용자 정의 chain 삭제     ‘-X’ 또는 ‘–delete-chain’ command를 사용합니다.   # iptables [table] -X [chain]      상세히 설명하면 아래와 같습니다.   # iptables [-t TABLE-NAME] -X [chain]      예제 :: 생성한 ‘lindarex-chain-income-new’라는 이름의 사용자 정의 chain을 삭제합니다.   # iptables -X lindarex-chain-income-new   4.13. built-in chain의 policy target 설정     ‘-P’ 또는 ‘–policy’ command를 사용합니다.   # iptables [table] -P chain target      상세히 설명하면 아래와 같습니다.   # iptables [-t TABLE-NAME] -P chain -j TARGET-NAME [target-options]      예제 :: OUTPUT chain의 policy를 DROP으로 설정합니다.   # iptables -P FORWARD DROP   4.14. packet과 byte counters를 0으로 설정     ‘-Z’ 또는 ‘–zero’ command를 사용합니다.   # iptables [table] -Z [chain [rule-number]] [options]      상세히 설명하면 아래와 같습니다.   # iptables [-t TABLE-NAME] -Z [chain [rule-number]] [options]      예제 :: FORWARD chain의 모든 rule의 counters를 0으로 설정합니다.   # iptables -Z FORWARD   마무리(CONCLUSION)  ubuntu 환경에 iptables 설정을 완료했습니다.    iptables 설정을 통해 서버(server) 보안을 많이 향상할 수 있지만, 잘못 설정할 경우에는 접속을 못 하는 경우가 발생할 수 있으니 신중하게 설정해야 합니다.    다음 포스트에서는 우분투(Ubuntu) 서버(Server) 초기 설정하기를 소개하겠습니다.   참고(REFERENCES)     https://ko.wikipedia.org/wiki/iptables   https://www.linuxtopia.org/index.html   https://fedoraproject.org/wiki/How_to_edit_iptables_rules   https://linux.die.net/man/8/iptables  ","categories": ["ubuntu"],
        "tags": ["iptables","firewall","nat","open source software","open source","oss","ubuntu"],
        "url": "https://lindarex.github.io/ubuntu/ubuntu-iptables-setting/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 서버(Server) 초기 설정하기",
        "excerpt":"이 포스트에서는 우분투(ubuntu) 서버(server) 설치 후 초기 설정하는 방법을 소개합니다.   선행조건(PREREQUISITE)     ubuntu 환경이 필요합니다.      Ubuntu 설치 방법은 우분투(Ubuntu) 서버(Server) 16.04 설치하기 또는 우분투(Ubuntu) 서버(Server) 18.04 설치하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   Ubuntu 18.04.4 LTS (Bionic Beaver) Server (64-bit)   요약(SUMMARY)     리눅스 버전 조회   관리자(root) 계정 활성화   패키지 업데이트   유용한 패키지 설치   사용자 계정 생성   시스템 언어 설정   시스템 시간 설정   내용(CONTENTS)     아래 설정은 ubuntu server 16.04, ubuntu server 18.04에 모두 동일하게 적용됩니다.   1. 리눅스(linux) 버전 조회      아래 예제는 buntu 18.04의 정보입니다.   $ grep . /etc/*-release /etc/lsb-release:DISTRIB_ID=Ubuntu /etc/lsb-release:DISTRIB_RELEASE=18.04 /etc/lsb-release:DISTRIB_CODENAME=bionic /etc/lsb-release:DISTRIB_DESCRIPTION=\"Ubuntu 18.04.4 LTS\" /etc/os-release:NAME=\"Ubuntu\" /etc/os-release:VERSION=\"18.04.4 LTS (Bionic Beaver)\" /etc/os-release:ID=ubuntu /etc/os-release:ID_LIKE=debian /etc/os-release:PRETTY_NAME=\"Ubuntu 18.04.4 LTS\" /etc/os-release:VERSION_ID=\"18.04\" /etc/os-release:HOME_URL=\"https://www.ubuntu.com/\" /etc/os-release:SUPPORT_URL=\"https://help.ubuntu.com/\" /etc/os-release:BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\" /etc/os-release:PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" /etc/os-release:VERSION_CODENAME=bionic /etc/os-release:UBUNTU_CODENAME=bionic  ​     아래 예제는 buntu 16.04의 정보입니다.   $ grep . /etc/*-release /etc/lsb-release:DISTRIB_ID=Ubuntu /etc/lsb-release:DISTRIB_RELEASE=16.04 /etc/lsb-release:DISTRIB_CODENAME=xenial /etc/lsb-release:DISTRIB_DESCRIPTION=\"Ubuntu 16.04.5 LTS\" /etc/os-release:NAME=\"Ubuntu\" /etc/os-release:VERSION=\"16.04.5 LTS (Xenial Xerus)\" /etc/os-release:ID=ubuntu /etc/os-release:ID_LIKE=debian /etc/os-release:PRETTY_NAME=\"Ubuntu 16.04.5 LTS\" /etc/os-release:VERSION_ID=\"16.04\" /etc/os-release:HOME_URL=\"http://www.ubuntu.com/\" /etc/os-release:SUPPORT_URL=\"http://help.ubuntu.com/\" /etc/os-release:BUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\" /etc/os-release:VERSION_CODENAME=xenial /etc/os-release:UBUNTU_CODENAME=xenial   2. 관리자(root) 계정 활성화     ubuntu를 비롯한 linux를 설치하면 기본적으로 관리자(root) 계정이 생성됩니다.   root 계정은 비밀번호(password)를 설정하기 전까지 비활성화 상태이기 때문에, password를 설정하여 root 계정을 활성화합니다.   2.1. root 계정 password 설정     ‘passwd’ 명령어(command)로 root password를 설정합니다.   password를 2번 동일하게 입력하면 아래와 같이 password가 갱신되었다는 메시지를 확인할 수 있습니다.   $ sudo passwd root [sudo] password for rex: Enter new UNIX password: Retype new UNIX password: passwd: password updated successfully   2.2. root 계정 활성화 확인     ‘su -‘ command를 사용하여 root 계정으로 사용자 계정 전환에 성공하면, root 계정이 정상적으로 활성화된 것입니다.   rex@lindarex:~$ su - root Password: root@lindarex:~#      ‘exit’ 또는 ‘logout’ command로 root 계정에서 로그아웃(logout)을 할 수 있습니다.   root@lindarex:~# exit logout rex@lindarex:~$      ‘sudo’(substitute user do)는 현재 계정에서 다른 계정, 즉 슈퍼 유저(superuser)로서 관리자(root) 권한을 가진 계정으로 프로그램(program)을 구동할 수 있도록 하는 command입니다.    ‘sudo -i’는 root 계정으로 로그인(login)하며 ‘/root’ 디렉터리(directory)로 이동하는 command이고, ‘sudo -s’는 현재 directory를 유지하며 root 계정으로 login 하는 command입니다.    ‘sudo’, ‘sudo -i’, ‘sudo -s’ command는 현재 계정의 password를 요구하고, 전환된 계정의 환경변수는 적용하지 않습니다.       ‘su’(substitute user, switch user)는 현재 계정에서 log out 하지 않고 다른 계정(기본값은 root 계정)으로 전환하며 현재 계정의 환경변수를 유지하는 command입니다.    ‘su -‘는 ‘su’와 동일하게 계정을 전환하며, 전환된 계정의 환경변수를 적용하는 command입니다.    ‘su’, ‘su -‘ command는 전환될 계정의 password를 요구합니다.       ‘sudo’ command는 현재 계정에서 root 계정의 권한만 빌리기 때문에 작업 내역은 현재 계정으로 남고,    ‘su’ command는 현재 계정에서 root 계정으로 전환하기 때문에 작업 내역은 root 계정으로 저장됩니다.       ‘sudo’ command에 대한 자세한 정보는 https://ko.wikipedia.org/wiki/Sudo와 [https://zetawiki.com/wiki/리눅스_sudo\\,su_차이점](https://zetawiki.com/wiki/%EB%A6%AC%EB%88%85%EC%8A%A4_sudo,_su%EC%B0%A8%EC%9D%B4%EC%A0%90){: target=”_blank”}을 확인해 주시기 바랍니다.    3. 패키지(package) 업데이트(update)     ‘apt update’ command로 package 인덱스를 update합니다.   $ sudo apt update [sudo] password for rex:      ‘apt upgrade’ command로 업그레이드 가능한 모든 package를 update합니다.   $ sudo apt upgrade -y [sudo] password for rex:      ubuntu를 재시작합니다.   $ sudo reboot [sudo] password for rex:   4. 유용한 package 설치      ‘apt install’ command로 arp, ifconfig, netstat,  rarp 등 네트워크 제어 command를 포함한 net-tools package를 설치합니다.   $ sudo apt install net-tools -y [sudo] password for rex:      ‘apt install’ command로 압축 program인 unzip package를 설치합니다.   $ sudo apt install unzip -y [sudo] password for rex:      ‘apt install’ command로 tree 구조로 디렉터리를 조회할 수 있는 tree package를 설치합니다.   $ sudo apt install tree -y [sudo] password for rex:   5. 사용자(user) 계정 생성     ‘adduser’ command로 user 계정을 추가합니다.   $ sudo adduser rex2 [sudo] password for rex:      root 계정의 password 설정 시와 동일하게, ‘passwd’ command로 user 계정의 password를 설정합니다.   $ sudo passwd rex2 [sudo] password for rex:   ​- ‘visudo’ command로 생성한 user 계정에 root(sudo) 권한을 추가합니다.      ‘/etc/sudoers’를 vi로 수정할 수도 있지만, 설정 유효성과 문법 체크를 위해 ‘visudo’ command로 수정하시기 바랍니다.    $ sudo visudo [sudo] password for rex:   ---------------------------------------------------------------------------------------------------- # # This file MUST be edited with the 'visudo' command as root. # # Please consider adding local content in /etc/sudoers.d/ instead of # directly modifying this file. # # See the man page for details on how to write a sudoers file. # Defaults        env_reset Defaults        mail_badpass Defaults        secure_path=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin\"  # Host alias specification  # User alias specification  # Cmnd alias specification  # User privilege specification root    ALL=(ALL:ALL) ALL rex     ALL=(ALL:ALL) ALL rex2    ALL=(ALL:ALL) ALL  # Members of the admin group may gain root privileges %admin ALL=(ALL) ALL  # Allow members of group sudo to execute any command %sudo   ALL=(ALL:ALL) ALL  # See sudoers(5) for more information on \"#include\" directives:  #includedir /etc/sudoers.d ----------------------------------------------------------------------------------------------------   ​- ‘visudo’ command로 생성한 user 계정에 root(sudo) 권한 추가와 함께 password 입력 없이 사용할 때는 아래와 같이 설정합니다.      sudo password 입력 없이 사용하면, 보안상 안전하지 않을 수 있으니 주의하시기 바랍니다.    $ sudo visudo [sudo] password for rex:   ---------------------------------------------------------------------------------------------------- ~ # User privilege specification root    ALL=(ALL:ALL) ALL rex     ALL=NOPASSWD:ALL rex2    ALL=NOPASSWD:ALL ~ ----------------------------------------------------------------------------------------------------   6. 시스템(system) 언어(locale) 설정      ‘locale’ command로 현재 locale 설정을 조회합니다.   $ locale LANG=en_US.UTF-8 LANGUAGE= LC_CTYPE=\"en_US.UTF-8\" LC_NUMERIC=\"en_US.UTF-8\" LC_TIME=\"en_US.UTF-8\" LC_COLLATE=\"en_US.UTF-8\" LC_MONETARY=\"en_US.UTF-8\" LC_MESSAGES=\"en_US.UTF-8\" LC_PAPER=\"en_US.UTF-8\" LC_NAME=\"en_US.UTF-8\" LC_ADDRESS=\"en_US.UTF-8\" LC_TELEPHONE=\"en_US.UTF-8\" LC_MEASUREMENT=\"en_US.UTF-8\" LC_IDENTIFICATION=\"en_US.UTF-8\" LC_ALL=      ‘locale -a’ command로 설치되어 있는 locale을 조회합니다.   $ locale -a C C.UTF-8 en_US.utf8 POSIX      ‘apt install’ command로 한국어(Korean) package를 설치합니다.   $ sudo apt install language-pack-ko -y [sudo] password for rex:      Korean 설정을 합니다.   $ sudo vi /etc/default/locale [sudo] password for rex:   ---------------------------------------------------------------------------------------------------- LANG=\"ko_KR.UTF-8\" LANGUAGE=\"ko_KR:ko:en_US:en\" ----------------------------------------------------------------------------------------------------      설정 후 재접속하여 한글 출력을 확인합니다.   $ sudo ls [sudo] rex의 암호: 죄송합니다만, 다시 시도하십시오. [sudo] rex의 암호:  ​     ‘locale -a’ command로 설치된 Korean locale을 조회합니다.   $ locale -a C C.UTF-8 en_US.utf8 ko_KR.utf8 POSIX   7. 시스템(system) 시간(timezone) 설정      ‘date’ command로 현재 시각을 조회합니다.   $ date Mon Mar 14 07:29:26 UTC 2020      ‘timedatectl’ command로 자세한 timezone 설정을 조회합니다.   $ timedatectl                       Local time: Mon 2020-03-14 07:26:52 UTC                   Universal time: Mon 2020-03-14 07:26:52 UTC                         RTC time: Mon 2020-03-14 07:26:53                        Time zone: Etc/UTC (UTC, +0000)        System clock synchronized: yes systemd-timesyncd.service active: yes                  RTC in local TZ: no      ‘timedatectl list-timezones’ command로 사용 가능한 timezone 목록을 조회합니다.   $ timedatectl list-timezones Africa/Abidjan Africa/Accra Africa/Addis_Ababa ~ Pacific/Tongatapu Pacific/Wake Pacific/Wallis UTC      ‘timedatectl set-timezone’ command로 ‘Asia/Seoul’ timezone으로 설정합니다.   $ sudo timedatectl set-timezone Asia/Seoul      ‘date’ command로 설정한 timezone이 적용된 현재 시각을 조회합니다.   $ date Mon Mar 14 16:35:27 KST 2020      ‘timedatectl’ command로 ‘Asia/Seoul’으로 설정한 timezone 설정을 조회합니다.   $ timedatectl                       Local time: Mon 2020-03-14 16:35:29 KST                   Universal time: Mon 2020-03-14 07:35:29 UTC                         RTC time: Mon 2020-03-14 07:35:30                        Time zone: Asia/Seoul (KST, +0900)        System clock synchronized: yes systemd-timesyncd.service active: yes                  RTC in local TZ: no   마무리(CONCLUSION)  ubuntu server 설치 후 초기 설정을 완료했습니다.    다음 포스트에서는 ubuntu 보안 강화를 위한 오픈소스(open source) 소프트웨어인 Fail2ban을 소개하겠습니다.   참고(REFERENCES)     https://ubuntu.com/   https://ubuntu.com/tutorials/tutorial-install-ubuntu-server#1-overview  ","categories": ["ubuntu"],
        "tags": ["ubuntu","open source software","open source","oss"],
        "url": "https://lindarex.github.io/ubuntu/ubuntu-initial-setting/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 WAR 파일로 젠킨스(Jenkins) 설치하기",
        "excerpt":"젠킨스(Jenkins)를 설치하는 방법은 다양합니다.    이 포스트에서는 우분투(ubuntu) 환경에서 WAR 파일(file)로 jenkins를 설치하는 방법을 소개합니다.      패키지(package)로 jenkins를 설치하는 방법은 우분투(Ubuntu) 환경에 패키지(Package)로 젠킨스(Jenkins) 설치하기 포스트를 참고하시기 바랍니다.    선행조건(PREREQUISITE)     ubuntu 환경에 Java가 설치되어 있어야 합니다.            Java 8 or Java 11 버전(version)이 필요합니다.           방화벽 설정이 필요합니다.            TCP 8080 포트가 개방되어 있어야 합니다.              Java 설치 방법은 우분투(Ubuntu) 환경에 OpenJDK(Java) 설치하기 포스트를 참고하시기 바랍니다.       jenkins version에 따른 Java version에 대한 자세한 정보는 https://jenkins.io/doc/administration/requirements/java/를 확인해 주시기 바랍니다.       방화벽 설정 방법은 우분투(Ubuntu) 환경에 방화벽(Firewalld) 설치 및 설정하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.1 build-15018445)   Ubuntu 18.04.4 LTS (Bionic Beaver) Server (64-bit)   jenkins 2.204.5   OpenJDK 1.8.0_242   요약(SUMMARY)     작업 공간(workspace) 생성   jenkins war file 내려받기   java 명령어로 jenkins 실행   웹브라우저로 jenkins 접속   내용(CONTENTS)  1. workspace 생성     로그인한 사용자의 ubuntu 홈 디렉터리(directory) 아래에 ‘workspace’ directory를 생성합니다.   $ mkdir -p ${HOME}/workspace   2. jenkins war file 내려받기     위에서 생성한 workspace directory에 wget 명령어(command)로 jenkins war file을 내려받습니다.   $ wget -P ${HOME}/workspace http://mirrors.jenkins.io/war-stable/latest/jenkins.war --2020-03-19 04:25:52--  http://mirrors.jenkins.io/war-stable/latest/jenkins.war Resolving mirrors.jenkins.io (mirrors.jenkins.io)... 52.202.51.185 Connecting to mirrors.jenkins.io (mirrors.jenkins.io)|52.202.51.185|:80... connected. HTTP request sent, awaiting response... 302 Found Location: http://ftp-nyc.osuosl.org/pub/jenkins/war-stable/2.204.5/jenkins.war [following] --2020-03-19 04:25:53--  http://ftp-nyc.osuosl.org/pub/jenkins/war-stable/2.204.5/jenkins.war Resolving ftp-nyc.osuosl.org (ftp-nyc.osuosl.org)... 64.50.233.100, 2600:3404:200:237::2 Connecting to ftp-nyc.osuosl.org (ftp-nyc.osuosl.org)|64.50.233.100|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 63466030 (61M) [application/x-java-archive] Saving to: ‘/home/rex/workspace/jenkins.war’  jenkins.war                                              100%[=================================================================================================================================&gt;]  60.53M  31.0MB/s    in 2.0s  2020-03-19 04:25:55 (31.0 MB/s) - ‘/home/rex/workspace/jenkins.war’ saved [63466030/63466030]      jenkins.war file을 확인합니다.   $ ll ${HOME}/workspace total 61988 drwxrwxr-x 2 rex rex     4096 Mar 19 04:25 ./ drwxr-xr-x 5 rex rex     4096 Mar 19 04:25 ../ -rw-rw-r-- 1 rex rex 63466030 Mar  8 06:20 jenkins.war   3. java command로 jenkins 실행   $ java -jar ${HOME}/workspace/jenkins.war --httpPort=8080 Running from: /home/rex/workspace/jenkins.war webroot: $user.home/.jenkins 2020-03-19 05:28:15.555+0000 [id=1]\tINFO\torg.eclipse.jetty.util.log.Log#initialized: Logging initialized @516ms to org.eclipse.jetty.util.log.JavaUtilLog 2020-03-19 05:28:15.689+0000 [id=1]\tINFO\twinstone.Logger#logInternal: Beginning extraction from war file 2020-03-19 05:28:16.774+0000 [id=1]\tWARNING\to.e.j.s.handler.ContextHandler#setContextPath: Empty contextPath 2020-03-19 05:28:16.841+0000 [id=1]\tINFO\torg.eclipse.jetty.server.Server#doStart: jetty-9.4.z-SNAPSHOT; built: 2019-05-02T00:04:53.875Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08 2020-03-19 05:28:17.214+0000 [id=1]\tINFO\to.e.j.w.StandardDescriptorProcessor#visitServlet: NO JSP Support for /, did not find org.eclipse.jetty.jsp.JettyJspServlet 2020-03-19 05:28:17.291+0000 [id=1]\tINFO\to.e.j.s.s.DefaultSessionIdManager#doStart: DefaultSessionIdManager workerName=node0 2020-03-19 05:28:17.292+0000 [id=1]\tINFO\to.e.j.s.s.DefaultSessionIdManager#doStart: No SessionScavenger set, using defaults 2020-03-19 05:28:17.295+0000 [id=1]\tINFO\to.e.j.server.session.HouseKeeper#startScavenging: node0 Scavenging every 600000ms 2020-03-19 05:28:17.757+0000 [id=1]\tINFO\thudson.WebAppMain#contextInitialized: Jenkins home directory: /home/rex/.jenkins found at: $user.home/.jenkins 2020-03-19 05:28:17.930+0000 [id=1]\tINFO\to.e.j.s.handler.ContextHandler#doStart: Started w.@4a9e6faf{Jenkins v2.204.5,/,file:///home/rex/.jenkins/war/,AVAILABLE}{/home/rex/.jenkins/war} 2020-03-19 05:28:17.974+0000 [id=1]\tINFO\to.e.j.server.AbstractConnector#doStart: Started ServerConnector@7a5ceedd{HTTP/1.1,[http/1.1]}{0.0.0.0:8080} 2020-03-19 05:28:17.977+0000 [id=1]\tINFO\torg.eclipse.jetty.server.Server#doStart: Started @2938ms 2020-03-19 05:28:17.981+0000 [id=20]\tINFO\twinstone.Logger#logInternal: Winstone Servlet Engine v4.0 running: controlPort=disabled 2020-03-19 05:28:19.005+0000 [id=25]\tINFO\tjenkins.InitReactorRunner$1#onAttained: Started initialization 2020-03-19 05:28:19.050+0000 [id=25]\tINFO\tjenkins.InitReactorRunner$1#onAttained: Listed all plugins 2020-03-19 05:28:20.675+0000 [id=25]\tINFO\tjenkins.InitReactorRunner$1#onAttained: Prepared all plugins 2020-03-19 05:28:20.689+0000 [id=25]\tINFO\tjenkins.InitReactorRunner$1#onAttained: Started all plugins 2020-03-19 05:28:20.716+0000 [id=26]\tINFO\tjenkins.InitReactorRunner$1#onAttained: Augmented all extensions 2020-03-19 05:28:21.991+0000 [id=26]\tINFO\tjenkins.InitReactorRunner$1#onAttained: Loaded all jobs 2020-03-19 05:28:22.027+0000 [id=39]\tINFO\thudson.model.AsyncPeriodicWork#lambda$doRun$0: Started Download metadata 2020-03-19 05:28:22.050+0000 [id=39]\tINFO\thudson.util.Retrier#start: Attempt #1 to do the action check updates server 2020-03-19 05:28:23.246+0000 [id=26]\tINFO\to.s.c.s.AbstractApplicationContext#prepareRefresh: Refreshing org.springframework.web.context.support.StaticWebApplicationContext@2f650e69: display name [Root WebApplicationContext]; startup date [Thu Mar 19 04:28:23 KST 2020]; root of context hierarchy 2020-03-19 05:28:23.250+0000 [id=26]\tINFO\to.s.c.s.AbstractApplicationContext#obtainFreshBeanFactory: Bean factory for application context [org.springframework.web.context.support.StaticWebApplicationContext@2f650e69]: org.springframework.beans.factory.support.DefaultListableBeanFactory@252ba0b0 2020-03-19 05:28:23.274+0000 [id=26]\tINFO\to.s.b.f.s.DefaultListableBeanFactory#preInstantiateSingletons: Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@252ba0b0: defining beans [authenticationManager]; root of factory hierarchy 2020-03-19 05:28:23.532+0000 [id=26]\tINFO\to.s.c.s.AbstractApplicationContext#prepareRefresh: Refreshing org.springframework.web.context.support.StaticWebApplicationContext@17aa246c: display name [Root WebApplicationContext]; startup date [Thu Mar 19 04:28:23 KST 2020]; root of context hierarchy 2020-03-19 05:28:23.533+0000 [id=26]\tINFO\to.s.c.s.AbstractApplicationContext#obtainFreshBeanFactory: Bean factory for application context [org.springframework.web.context.support.StaticWebApplicationContext@17aa246c]: org.springframework.beans.factory.support.DefaultListableBeanFactory@45e8b062 2020-03-19 05:28:23.534+0000 [id=26]\tINFO\to.s.b.f.s.DefaultListableBeanFactory#preInstantiateSingletons: Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@45e8b062: defining beans [filter,legacy]; root of factory hierarchy 2020-03-19 05:28:23.817+0000 [id=26]\tINFO\tjenkins.install.SetupWizard#init:  ************************************************************* ************************************************************* *************************************************************  Jenkins initial setup is required. An admin user has been created and a password generated. Please use the following password to proceed to installation:  cqqb445c1bad4f219e3737661dd2740d  This may also be found at: /home/rex/.jenkins/secrets/initialAdminPassword  ************************************************************* ************************************************************* *************************************************************  2020-03-19 05:28:33.139+0000 [id=39]\tINFO\thudson.model.UpdateSite#updateData: Obtained the latest update center data file for UpdateSource default 2020-03-19 05:28:33.770+0000 [id=26]\tINFO\thudson.model.UpdateSite#updateData: Obtained the latest update center data file for UpdateSource default 2020-03-19 05:28:34.144+0000 [id=26]\tINFO\tjenkins.InitReactorRunner$1#onAttained: Completed initialization 2020-03-19 05:28:34.162+0000 [id=19]\tINFO\thudson.WebAppMain$3#run: Jenkins is fully up and running 2020-03-19 05:28:34.796+0000 [id=39]\tINFO\th.m.DownloadService$Downloadable#load: Obtained the updated data file for hudson.tasks.Maven.MavenInstaller 2020-03-19 05:28:34.797+0000 [id=39]\tINFO\thudson.util.Retrier#start: Performed the action check updates server successfully at the attempt #1 2020-03-19 05:28:34.800+0000 [id=39]\tINFO\thudson.model.AsyncPeriodicWork#lambda$doRun$0: Finished Download metadata. 12,771 ms      위 콘솔 로그에서 비밀번호(‘cqqb445c1bad4f219e3737661dd2740d’)는 jenkins 초기 설정할 때 필요합니다.    4. 웹브라우저로 jenkins 접속     http://[MY-IP]:8080   마무리(CONCLUSION)  ubuntu 환경에 war file로 jenkins 설치를 완료했습니다.    jenkins 초기 설정은 젠킨스(Jenkins) 초기 설정하기 포스트를 참고하시기 바랍니다.    다음 포스트에서는 GitHub Webhook으로 젠킨스(Jenkins) Job을 실행(자동화)하는 방법을 소개하겠습니다.   참고(REFERENCES)     https://jenkins.io/   https://jenkins.io/doc/pipeline/tour/getting-started/  ","categories": ["jenkins"],
        "tags": ["jenkins","ci","continuous integration","open source software","open source","oss","ubuntu"],
        "url": "https://lindarex.github.io/jenkins/ubuntu-jenkins-war-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 패키지(Package)로 BIND(BIND9) 설치하기",
        "excerpt":"BIND(bind9, berkeley internet name domain)는 안정성과 고품질로 유닉스(unix) 및 리눅스(linux)에서 널리 사용되며, 모든 기능을 갖춘 매우 유연한 DNS(dns, domain name server) 시스템(system)입니다.    bind9은 MPL(mozilla public license) 2.0 라이선스(license)가 적용되는 오픈소스(open source) 소프트웨어이며, 인터넷에 dns 정보를 게시 할 수 있을 뿐만 아니라 사용자의 dns 쿼리를 처리할 수 있습니다.     bind9은 내부 dns 서버(server)를 설정하여, server가 개인 호스트(host) 이름과 개인 IP 주소(address)를 확인하는 데 사용합니다. 이를 통해 내부 host 이름과 개인 IP address를 중앙에서 관리할 수 ​​있으며, 이는 사용자 환경이 몇 개 이상의 host로 확장될 때 필요합니다.    이 포스트에서는 우분투(ubuntu) 환경에서 package로 bind9을 설치하는 방법을 소개합니다.      server 구성 및 인프라(Infrastructure) 관리 시에 중요한 부분은, dns를 설정하여 네트워크(network) 인터페이스와 IP address를 쉽게 검색할 방법을 유지하는 것입니다.    IP address 대신 FQDN(fully qualified domain name)을 사용하여 network address를 지정하면, 애플리케이션(application) 및 서비스(service) 구성이 쉬워지고 구성 파일의 유지 관리가 향상됩니다.    선행조건(PREREQUISITE)     ubuntu 환경이 필요합니다.      ubuntu 설치 방법은 우분투(Ubuntu) 서버(Server) 16.04 설치하기 또는 우분투(Ubuntu) 서버(Server) 18.04 설치하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.2 build-15785246)   Ubuntu 18.04.4 LTS (Bionic Beaver) Server (64-bit)   BIND 9.11.3   요약(SUMMARY)     apt 명령어로 bind9 설치   bind9 설치 확인   systemctl 명령어로 bind9 서비스 관리   (선택사항) apt 명령어로 bind9 삭제   내용(CONTENTS)  1. apt 명령어로 bind9 설치  $ sudo apt update &amp;&amp; sudo apt install bind9 bind9utils bind9-doc -y   2. bind9 설치 확인  $ named -V BIND 9.11.3-1ubuntu1.11-Ubuntu (Extended Support Version) &lt;id:a375815&gt; running on Linux x86_64 4.15.0-91-generic #92-Ubuntu SMP Fri Feb 28 11:09:48 UTC 2020 built by make with '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/usr/include' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--disable-silent-rules' '--libdir=/usr/lib/x86_64-linux-gnu' '--libexecdir=/usr/lib/x86_64-linux-gnu' '--disable-maintainer-mode' '--disable-dependency-tracking' '--libdir=/usr/lib/x86_64-linux-gnu' '--sysconfdir=/etc/bind' '--with-python=python3' '--localstatedir=/' '--enable-threads' '--enable-largefile' '--with-libtool' '--enable-shared' '--enable-static' '--with-gost=no' '--with-openssl=/usr' '--with-gssapi=/usr' '--with-libjson=/usr' '--without-lmdb' '--with-gnu-ld' '--with-geoip=/usr' '--with-atf=no' '--enable-ipv6' '--enable-rrl' '--enable-filter-aaaa' '--enable-native-pkcs11' '--with-pkcs11=/usr/lib/softhsm/libsofthsm2.so' '--with-randomdev=/dev/urandom' '--with-eddsa=no' 'build_alias=x86_64-linux-gnu''CFLAGS=-g -O2 -fdebug-prefix-map=/build/bind9-uW3Pyl/bind9-9.11.3+dfsg=. -fstack-protector-strong -Wformat -Werror=format-security -fno-strict-aliasing -fno-delete-null-pointer-checks -DNO_VERSION_DATE -DDIG_SIGCHASE' 'LDFLAGS=-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' compiled by GCC 7.4.0 compiled with OpenSSL version: OpenSSL 1.1.1  11 Sep 2018 linked to OpenSSL version: OpenSSL 1.1.1  11 Sep 2018 compiled with libxml2 version: 2.9.4 linked to libxml2 version: 20904 compiled with libjson-c version: 0.12.1 linked to libjson-c version: 0.12.1 compiled with zlib version: 1.2.11 linked to zlib version: 1.2.11 threads support is enabled   3. systemctl 명령어로 bind9 서비스(service) 관리  3.1. bind9 service 설정 반영  $ sudo systemctl daemon-reload   3.2. bind9 service 시작  $ sudo systemctl start bind9.service   3.3. bind9 service 중지  $ sudo systemctl stop bind9.service   3.4. bind9 service 재시작  $ sudo systemctl restart bind9.service   3.5. bind9 service 설정 재적용  $ sudo systemctl reload bind9.service   3.6. bind9 service 상태 조회  $ sudo systemctl status bind9.service   3.7. bind9 service 활성화(부팅 시 자동 시작)  $ sudo systemctl enable bind9.service   3.8. bind9 service 비활성화  $ sudo systemctl disable bind9.service   3.9. bind9 service 및 관련 프로세스 모두 중지  $ sudo systemctl kill bind9.service   4. (선택사항) apt 명령어로 bind9 삭제     ’–auto-remove’ 옵션을 추가하면, 사용하지 않는 관련 package를 모두 삭제합니다.   4.1. apt remove 명령어로 bind9 삭제     설정 파일을 유지하며 bind9을 삭제합니다.   $ sudo apt remove bind9* $ sudo apt remove --auto-remove bind9*   4.2. apt purge 명령어로 bind9 삭제     설정 파일과 함께 bind9을 삭제합니다. (단, 사용자 홈 디렉터리의 설정 파일은 유지됩니다.)   $ sudo apt purge bind9* $ sudo apt purge --auto-remove bind9*   마무리(CONCLUSION)  ubuntu 환경에 package로 bind9 설치를 완료했습니다.    bind9 설정은 우분투(Ubuntu) 환경에 BIND(BIND9) 설정하기 포스트를 참고하시기 바랍니다.   참고(REFERENCES)     https://www.isc.org/bind/   https://www.linuxbabe.com/ubuntu/set-up-authoritative-dns-server-ubuntu-18-04-bind9   https://www.digitalocean.com/community/tutorials/how-to-configure-bind-as-a-private-network-dns-server-on-ubuntu-18-04   https://www.linuxtechi.com/install-configure-bind-9-dns-server-ubuntu-debian/  ","categories": ["bind9"],
        "tags": ["bind9","dns","open source software","open source","oss","ubuntu"],
        "url": "https://lindarex.github.io/bind9/ubuntu-bind9-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 BIND(BIND9) 설정하기",
        "excerpt":"BIND(bind9, berkeley internet name domain)는 DNS(domain name system)를 제공하는 서버(server), 즉 네임 서버(name server) 기능을 갖춘 오픈소스(open source) 소프트웨어입니다.    bind9 설정은 인프라(infrastructure) 환경과 용도에 따라 다양하지만, 이 포스트에서는 간단한 구성으로 ubuntu 환경에서 bind9을 설정하는 방법을 소개합니다.      포스트 작성 전에 DNS server와 name server, DNS에 대해 간략히 설명합니다.    DNS server는 도메인(domain) 이름과 관련 기록을 관리, 유지, 처리하는 name server의 일종이며, DNS 쿼리(query)에 응답하는 server입니다.     name server는 네이밍(naming) 서비스를 제공하는 모든 server인데, 일반적으로 로컬 DNS server로 알려져 있으며, DNS server를 찾는 데 사용됩니다.    name server는 domain 이름을 IP 주소(address)로 변환하며, 이를 통해 사용자는 웹 사이트의 실제 IP address 대신 domain 이름을 입력하여 웹 사이트에 액세스할 수 있습니다.     DNS는 호스트(host) 이름을 IP address로 변환하는 분산 시스템입니다.    DNS는 주로 TCP/IP 네트워킹 프로토콜(protocol) 제품군과 함께 domain 이름을 확인하는 데 사용되지만, 다른 용도로도 사용되기도 합니다.    즉, DNS server는 기본적으로 name server 유형이지만, 모든 name server가 DNS server인 것은 아닙니다.   그리고 DNS server가 가장 일반적인 name server이므로, name server와 DNS server는 서로 바꿔 사용할 수 있습니다.       DNS server와 name server, DNS에 대한 자세한 정보는 https://www.quora.com/What-is-the-difference-if-any-between-DNS-server-and-name-server를 확인해 주시기 바랍니다.    선행조건(PREREQUISITE)     ubuntu 환경에 bind9이 설치되어 있어야 합니다.   방화벽 설정이 필요합니다.            TCP 및 UDP 53 포트가 개방되어 있어야 합니다.              bind9 설치 방법은 우분투(Ubuntu) 환경에 패키지(Package)로 BIND(BIND9) 설치하기 포스트를 참고하시기 바랍니다.       방화벽 설정 방법은 우분투(Ubuntu) 환경에 방화벽(Firewalld) 설치 및 설정하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.2 build-15785246)   Ubuntu 18.04.4 LTS (Bionic Beaver) Server (64-bit)   BIND 9.11.3      아래 VM(virtual machine) 정보는 예시입니다. 고정 IP 설정은 이 포스트에서 다루지 않습니다.       bind9을 설치한 VM 정보            Host :: ns       Private FQDN :: ns.lindarex.local       Private IP address :: 10.0.1.32           bind9 테스트를 위한 외부 VM 정보            Host :: test       Private FQDN :: test.lindarex.local       Private IP address :: 10.0.1.31              FQDN(fully qualified domain name)이란 전체 주소 도메인 이름으로 host 이름과 domain 이름을 포함합니다.    요약(SUMMARY)     bind9 DNS 서버 VM 호스트 설정   bind9 DNS 서버 설정   bind9 로컬 DNS 서버 및 영역(ZONE) 설정   bind9 설정 확인   네트워크 명령어로 bind9 DNS 서버 VM에서 DNS 확인   네트워크 명령어로 외부 VM에서 DNS 확인   내용(CONTENTS)  1. DNS 서버 VM 호스트 설정  1.1. /etc/hosts 설정  $ sudo vi /etc/hosts   -------------------------------------------------------------------------------- 127.0.0.1 localhost 127.0.1.1 ns.lindarex.local 10.0.1.32 ns.lindarex.local  # The following lines are desirable for IPv6 capable hosts ::1     ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters --------------------------------------------------------------------------------   1.2. /etc/hostname 설정  $ sudo vi /etc/hostname   -------------------------------------------------------------------------------- ns.lindarex.local --------------------------------------------------------------------------------      host 설정 후 VM 재시작이 필요합니다.    2. bind9 DNS 서버 설정  $ sudo vi /etc/bind/named.conf.options   -------------------------------------------------------------------------------- options { \tdirectory \"/var/cache/bind\"; \tdnssec-validation auto; \tauth-nxdomain no;     listen-on port 53 { localhost; 10.0.1.0/24; };     allow-query { any; };     forwarders { 8.8.8.8; };     recursion yes; }; --------------------------------------------------------------------------------           위 설정을 설명합니다.         'directory \"/var/cache/bind\";'   // 기본값은 '/var/cache/bind'입니다.   // server의 작업 디렉터리(directory)를 정의하며 절대 경로(path)입니다.             'dnssec-validation auto;'   // 기본값은 'auto'입니다.   // 'auto'로 지정하면, DNSSEC 유효성 검사가 활성화되고 DNS root zone에 기본 trust anchor가 사용됩니다.             'auth-nxdomain no;'   // 기본값은 'no'입니다.   // 오래된 DNS 소프트웨어를 사용한다면 'yes'로 설정합니다.             'listen-on port 53 { localhost; 10.0.1.0/24; };'   // server가 query에 응답할 인터페이스(interface)와 포트(port)를 지정합니다.             'allow-query { any; };'   // DNS query를 할 수 있는 host를 지정합니다.             'forwarders { 8.8.8.8; };'   // 포워딩(forwarding)에 사용할 IP address를 지정합니다.             'recursion yes;'   // 기본값은 'yes'이며, 재귀(recursive) query를 활성화합니다.              DNSSEC(domain name system security extensions)에 대한 자세한 정보는 https://ko.wikipedia.org/wiki/DNSSEC와 https://한국인터넷정보센터.한국/jsp/resources/dns/dnssecInfo/dnssecInfo.jsp를 확인해 주시기 바랍니다.    3. bind9 로컬 DNS 서버 및 영역(ZONE) 설정  3.1. /etc/bind/named.conf.local 설정     정방향(forward) 및 역방향(reverse) 영역(zone)을 지정합니다.      forward zone(forward lookup zone)은 host 이름 또는 FQDN에 대한 IP address를 관리하는 DNS zone이고, reverse zone(reverse lookup zone)은 IP address에 대응하는 host 이름 또는 FQDN을 관리하는 DNS zone입니다.    $ sudo vi /etc/bind/named.conf.local   -------------------------------------------------------------------------------- // FORWARD ZONE zone \"lindarex.local\" IN {   type master;   file \"lindarex.local.zone\"; };  // REVERSE ZONE zone \"1.0.10.in-addr.arpa\" IN {   type master;   file \"lindarex.local.zone.rev\"; }; --------------------------------------------------------------------------------           위 설정을 설명합니다.         'zone \"lindarex.local\" IN {'   // forward zone의 domain 이름을 지정합니다.   // 'IN'은 클래스(class)를 명시한 것으로 인터넷(internet)을 지정한 것입니다.   // 지정하지 않으면 기본값은 internet이지만, 예제를 위해 지정했습니다.   // 'hesiod', 'CHAOS' class 등이 존재합니다.             'type master;'   // zone type을 'master'로 지정합니다.   // 'primary'와 같은 의미입니다.   // slave (또는 secondary), mirror, delegation-only, forward, hint, redirect, static-stub, stub 등이 존재합니다.             'file \"lindarex.local.zone\";'   // zone 파일(file)의 path와 이름을 지정합니다.   // 절대 path로 지정하지 않으면, 'named.conf.options'의 directory 설정을 루트(root) directory로 사용합니다.             'zone \"1.0.10.in-addr.arpa\" IN {'   // reverse zone의 IP 영역을 지정합니다.   // 역순으로 기재하는 것을 주의합니다.                  위 예제는 서브넷 마스크(subnet mask)가 ‘10.0.1.0/24’이기 때문에 ‘1.0.10.in-addr.arpa’으로 설정합니다. 만약 subnet mask가 ‘10.0.1.0/16’ 이라면, ‘0.10.in-addr.arpa’으로 설정합니다.            3.2. /var/cache/bind/lindarex.local.zone 생성     forward zone file을 생성합니다.   $ sudo vi /var/cache/bind/lindarex.local.zone   -------------------------------------------------------------------------------- $TTL\t86400 @\tIN\tSOA\tns.lindarex.local. root.ns.lindarex.local. ( \t\t\t      1\t\t; Serial \t\t\t 604800\t\t; Refresh \t\t\t  86400\t\t; Retry \t\t\t2419200\t\t; Expire \t\t\t  86400 )\t; Negative Cache TTL ; @\tIN\tNS\tns.lindarex.local. ns\tIN\tA\t10.0.1.32 --------------------------------------------------------------------------------           위 설정을 설명합니다.         '$TTL\t86400'   // resolver가 record를 캐시(cache) 할 시간(초, seconds)을 정의합니다.   // TTL(time-to-live)을 '0'으로 설정하면 record를 cache 하지 않습니다.             '@\tIN\tSOA\tns.lindarex.local. root.ns.lindarex.local. ('   // SOA(start of authority)는 zone(domain)에 대한 전역(global) 매개변수(parameters)와 zone 이름을 정의합니다.   // zone file에는 하나의 SOA RR(resource records)만 허용됩니다.             '1\t\t; Serial'   // serial number를 정의합니다.   // 1에서 4294967295 사이의 부호 없는 32비트(bit) 값(최대 2147483647)이며, 10자리 필드로 정의됩니다.    // 이 값은 zone file의 RR이 업데이트될 때 증가해야 합니다.             '604800\t\t; Refresh'   // 32 bit 값의 refresh seconds를 정의합니다.   // slave가 master DNS SOA RR을 읽어서 master에서 zone을 새로 고치려고 시도하는 시간을 설정합니다.             '86400\t\t; Retry'   // 32 bit 값의 update retry seconds를 정의합니다.   // 'refresh' 만료 또는 NOTIFY 메시지 수신, slave가 master에 연결하지 못하는 경우의 재시도 간격을 지정합니다.             '2419200\t\t; Expire'   // expire seconds를 정의합니다.   // 32 bit 값이며, zone 데이터(data)가 더는 권한이 없는 경우를 나타냅니다.   // slave(secondary) server에서만 사용됩니다.             '86400 )\t; Negative Cache TTL'   // negative cache TTL seconds를 정의합니다.   // 32 bit 값이며, 이전에는 'nx = nxdomain ttl'로 정의했었습니다.   // 'NAME ERROR = NXDOMAIN' 결과는 resolver에 의해 cache 되고, 허용하는 최댓값은 3시간(10800 seconds)입니다.             '@\tIN\tNS\tns.lindarex.local.'   // DNS generic record format(textual format)입니다.   // - 일반적으로 'owner-name', 'class', 'type', 'type-specific-data'로 구성됩니다.   // - 'owner-name'   //    + record가 속한 zone file에서 노드(node)의 소유자 이름 또는 레이블(label)입니다.   //    + 위 예제에서는 '@'이며, '$ORIGIN'을 대체합니다.   // - 'class'   //    + protocol 패밀리 또는 protocol 인스턴스(instance)를 정의합니다.   //    + 16 bit 값이며, 위 예제에서는 'IN'이며, internet protocol입니다.   // - 'type'   //    + 'type-specific-data'의 값을 결정하는 RR 유형이며, 위 예제에서는 'NS'입니다.    //    + 'NS'는 name server를 의미합니다.   //    + SOA record로 정의된 domain 또는 하위(sub) domain의 권한 있는 name server를 지정합니다.   // - 'type-specific-data'   //    + 'class'와 'type' 값에 의해 정의됩니다.   //    + 위 예제에서는 'ns.lindarex.local.'입니다.             'ns\tIN\tA\t10.0.1.32'   // - 'owner-name'   //    + 위 예제에서는 'ns'이며, host 이름을 지정합니다.   // - 'class'   //    + 위 예제에서는 'IN'이며, internet protocol입니다.   // - 'type'   //    + 위 예제에서는 'A'입니다.   //    + 'A'는 IPv4 address record, host의 IPv4 address입니다.   // - 'type-specific-data'   //    + 위 예제에서는 '10.0.1.32'입니다.           3.3. /var/cache/bind/lindarex.local.zone.rev 생성     reverse zone file을 생성합니다.   $ sudo vi /var/cache/bind/lindarex.local.zone.rev   -------------------------------------------------------------------------------- $TTL\t86400 @\tIN\tSOA\tns.lindarex.local. root.ns.lindarex.local. ( \t\t\t      2\t\t; Serial \t\t\t 604800\t\t; Refresh \t\t\t  86400\t\t; Retry \t\t\t2419200\t\t; Expire \t\t\t  86400 )\t; Negative Cache TTL ; @\tIN\tNS\tns. 32\tIN\tPTR\tns.lindarex.local. --------------------------------------------------------------------------------      forward zone file 설정과 다른 부분만 설명합니다.       '@\tIN\tNS\tns.'   // - 'owner-name'   //    + 위 예제에서는 '@'이며, '$ORIGIN'을 대체합니다.   // - 'class'   //    + 위 예제에서는 'IN'이며, internet protocol입니다.   // - 'type'   //    + 위 예제에서는 'NS'이며, name server입니다.   // - 'type-specific-data'   //    + 위 예제에서는 'ns.'이며, host 이름을 지정합니다.             '32\tIN\tPTR\tns.lindarex.local.'   // - 'owner-name'   //    + 위 예제에서는 '32'입니다.   //    + bind9 DNS server VM의 IP address가 '10.0.1.32'이고,    //      subnet mask가 '10.0.1.0/24'이기 때문에 역순으로 '32'를 지정합니다.   //    + 만약 subnet mask가 '10.0.1.0/16'이면, '32.1'로 설정해야 합니다.   // - 'class'   //    + 위 예제에서는 'IN'이며, internet protocol입니다.   // - 'type'   //    + 위 예제에서는 'PTR'입니다.   //    + 'PTR'은 pointer, reverse DNS입니다   // - 'type-specific-data'   //    + 위 예제에서는 'ns.lindarex.local.'입니다.           4. bind9 설정 확인  4.1. bind9 설정 file 문법 확인  $ named-checkconf      아무 결과가 나오지 않아야 문법(syntax) 오류가 없는 것입니다.    4.2. bind9 zone file syntax 확인     forward zone file의 syntax를 확인합니다.   $ named-checkzone lindarex.local /var/cache/bind/lindarex.local.zone zone lindarex.local/IN: loaded serial 1 OK      reverse zone file의 syntax를 확인합니다.   $ named-checkzone 10.0.1.32 /var/cache/bind/lindarex.local.zone.rev zone 10.0.1.32/IN: loaded serial 2 OK      bind9 설정에 syntax 오류가 없다면, bind9을 재시작합니다.   $ sudo systemctl restart bind9.service   5. 네트워크 명령어로 bind9 DNS 서버 VM에서 DNS 확인      아래 명령어는 bind9을 설치한 VM(10.0.1.32)에서 실행합니다.   5.1. nslookup 명령어(command)로 확인  $ nslookup ns.lindarex.local Server:\t\t127.0.0.53 Address:\t127.0.0.53#53  Non-authoritative answer: Name:\tns.lindarex.local Address: 127.0.1.1 Name:\tns.lindarex.local Address: 10.0.1.32   $ nslookup ns.lindarex.local localhost.localdomain Server:\t\tlocalhost.localdomain Address:\t::1#53  Name:\tns.lindarex.local Address: 10.0.1.32      nslookup command에 대한 자세한 정보는 https://ko.wikipedia.org/wiki/Nslookup를 확인해 주시기 바랍니다.    5.2. dig command로 확인  $ dig ns.lindarex.local  ; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.11-Ubuntu &lt;&lt;&gt;&gt; ns.lindarex.local ;; global options: +cmd ;; Got answer: ;; WARNING: .local is reserved for Multicast DNS ;; You are currently testing what happens when an mDNS query is leaked to DNS ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 36078 ;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1  ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 65494 ;; QUESTION SECTION: ;ns.lindarex.local.\t\tIN\tA  ;; ANSWER SECTION: ns.lindarex.local.\t0\tIN\tA\t127.0.1.1 ns.lindarex.local.\t0\tIN\tA\t10.0.1.32  ;; Query time: 0 msec ;; SERVER: 127.0.0.53#53(127.0.0.53) ;; WHEN: Sat Apr 04 05:17:52 UTC 2020 ;; MSG SIZE  rcvd: 78   $ dig -x 10.0.1.32  ; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.11-Ubuntu &lt;&lt;&gt;&gt; -x 10.0.1.32 ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 14862 ;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1  ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 65494 ;; QUESTION SECTION: ;32.1.0.10.in-addr.arpa.\t\tIN\tPTR  ;; ANSWER SECTION: 32.1.0.10.in-addr.arpa.\t0\tIN\tPTR\tns.lindarex.local.  ;; Query time: 0 msec ;; SERVER: 127.0.0.53#53(127.0.0.53) ;; WHEN: Sat Apr 04 05:18:04 UTC 2020 ;; MSG SIZE  rcvd: 100      dig command에 대한 자세한 정보는 https://ko.wikipedia.org/wiki/Dig를 확인해 주시기 바랍니다.    6. 네트워크 명령어로 외부 VM에서 DNS 확인      아래 command는 외부 VM(10.0.1.0/24)에서 실행합니다.   외부 VM에 name server 설정이 필요합니다.      name server 설정 방법은 우분투(Ubuntu) 환경에 네임 서버(Name server) 설정하기 포스트를 참고하시기 바랍니다.    6.1. nslookup command로 확인  $ nslookup ns.lindarex.local Server:\t\t10.0.1.32 Address:\t10.0.1.32#53  Name:\tns.lindarex.local Address: 10.0.1.32   $ nslookup 10.0.1.32 32.1.0.10.in-addr.arpa\tname = ns.lindarex.local.    6.2. dig command로 확인  $ dig ns.lindarex.local  ; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.11-Ubuntu &lt;&lt;&gt;&gt; ns.lindarex.local ;; global options: +cmd ;; Got answer: ;; WARNING: .local is reserved for Multicast DNS ;; You are currently testing what happens when an mDNS query is leaked to DNS ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 37938 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 1, ADDITIONAL: 1  ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ; COOKIE: 4d8ecf3a8c01a1e3c63e18595e8ac359738e244f6d3d1ec2 (good) ;; QUESTION SECTION: ;ns.lindarex.local.\t\tIN\tA  ;; ANSWER SECTION: ns.lindarex.local.\t86400\tIN\tA\t10.0.1.32  ;; AUTHORITY SECTION: lindarex.local.\t\t86400\tIN\tNS\tns.lindarex.local.  ;; Query time: 0 msec ;; SERVER: 10.0.1.32#53(10.0.1.32) ;; WHEN: Sat Apr 04 05:51:22 UTC 2020 ;; MSG SIZE  rcvd: 104   $ dig -x 10.0.1.32  ; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.11-Ubuntu &lt;&lt;&gt;&gt; -x 10.0.1.32 ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 18625 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 1  ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ; COOKIE: 37c4348db0ce59742913765d5e8ac375010a08556676faca (good) ;; QUESTION SECTION: ;32.1.0.10.in-addr.arpa.\t\tIN\tPTR  ;; ANSWER SECTION: 32.1.0.10.in-addr.arpa.\t86400\tIN\tPTR\tns.lindarex.local.  ;; AUTHORITY SECTION: 1.0.10.in-addr.arpa.\t86400\tIN\tNS\tns.  ;; Query time: 0 msec ;; SERVER: 10.0.1.32#53(10.0.1.32) ;; WHEN: Sat Apr 04 05:51:49 UTC 2020 ;; MSG SIZE  rcvd: 144   마무리(CONCLUSION)  ubuntu 환경에 bind9 설정을 완료했습니다.    일반적으로 엔지니어가 아닌 이상 DNS 설정을 할 기회는 드물지만, bind9 설정을 통해 네트워크 용어와 개념을 조금은 익숙해지길 바라며 이 포스트를 작성했습니다.    bind9 설정에 대한 더 자세한 내용은 아래 참고 페이지를 확인해 주시기 바랍니다.   참고(REFERENCES)     https://wiki.debian.org/Bind9   https://bind9.readthedocs.io/en/latest/index.html   https://www.zytrax.com/books/dns/ch8/  ","categories": ["bind9"],
        "tags": ["bind9","dns","open source software","open source","oss","ubuntu"],
        "url": "https://lindarex.github.io/bind9/ubuntu-bind9-setting/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "GitHub Webhook으로 젠킨스(Jenkins) Job을 실행(자동화)하는 방법",
        "excerpt":"이 포스트에서는 GitHub Webhook으로 젠킨스(Jenkins) Job을 실행(자동화)하는 방법을 소개합니다.   선행조건(PREREQUISITE)     외부 통신이 가능한 jenkins가 설치되어 있어야 합니다.   github 계정이 필요합니다.      jenkins 설치 방법은 우분투(Ubuntu) 환경에 패키지로 젠킨스(Jenkins) 설치하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     jenkins 2.222.3   Chrome v81.0.4044.129(공식 빌드) (64비트)   Firefox Browser DEVELOPER v76.0b8 (64-비트)   요약(SUMMARY)     github personal access token 생성   (선택사항) github repository 생성   jenkins 설정   github webhook 설정   테스트   내용(CONTENTS)  1. github personal access token 생성      github에 접속 후 ‘Settings’ 페이지 왼쪽 아래의 ‘Developer settings’ 메뉴를 선택합니다.         왼쪽 아래의 ‘Personal access tokens’ 메뉴를 선택합니다.         오른쪽 위의 ‘Generate new token’ 버튼을 선택합니다.         github 계정의 비밀번호를 입력합니다.         ‘New personal access token’ 페이지에서 아래와 같이 설정합니다.            Note :: lindarex-github-access-token       repo :: 체크       admin:repo_hook :: 체크              Note는 임의로 입력합니다.          아래의 ‘Generate token’ 버튼을 선택하여 github personal access token을 생성합니다.         생성한 github personal access token을 확인하고, token 값을 보관합니다.      2. (선택사항) github repository 생성      ‘Repositories’ 페이지로 이동하여 오른쪽 위의 ‘New’ 버튼을 선택합니다.         repository 생성 페이지에서 아래와 같이 설정하고, 아래의 ‘Create repository’ 버튼을 선택하여 repository를 생성합니다.            Repository name :: lindarex-jenkins-webhook-test       Initialize this repository with a README :: 체크              Repository name은 임의로 입력합니다.       README 생성 설정은 테스트의 편의를 위함이니 체크하지 않아도 무관합니다.          생성한 repository를 확인합니다.      3. jenkins 설정  3.1. (선택사항) jenkins 플러그인 설치      github webhook 연동을 위해 ‘Github Integeration Plugin’이 필요합니다.       jenkins에 로그인하여 왼쪽의 ‘Jenkins 관리’ 메뉴를 선택합니다.         ‘플러그인 관리’ 메뉴를 선택합니다.         ‘설치 가능’ 탭을 선택합니다.         ‘Github Integeration Plugin’을 체크하고, 아래의 ‘지금 다운로드하고 재시작 후 설치하기’를 선택합니다.         ‘플러그인 설치/업그레이드 중’ 페이지로 이동된 후, 설치 진행을 확인합니다.         아래와 같이 내려받기를 완료하면, 아래의 ‘설치가 끝나고 실행중인 작업이 없으면 Jenkins 재시작’을 체크합니다.         jenkins가 재시작됩니다.      3.2. jenkins credential 설정  3.2.1. github personal access token을 위한 jenkins credential 생성      jenkins에 로그인하여 왼쪽의 ‘Credentials’ 메뉴를 선택합니다.         왼쪽의 ‘System’ 메뉴를 선택합니다.         ‘Global credentials (unrestricted)’를 선택하여 ‘Add Credentials’ 버튼을 선택합니다.         credential 생성 페이지로 이동합니다.         아래와 같이 설정하고, 아래의 ‘OK’ 버튼을 선택하여 저장합니다.            Kind :: Secret text       Scope :: Global (Jenkins, nodes, items, all child items, etc)       Secret :: {GITHUB PERSONAL ACCESS TOKEN}       ID :: lindarex-github-access-token       Description :: lindarex github access token              {GITHUB PERSONAL ACCESS TOKEN} 값은 위에서 생성한 후 복사한 lindarex-github-access-token 값입니다.       ID와 Description은 임의로 입력합니다.          생성한 jenkins credential을 확인합니다.      3.2.2. github account를 위한 jenkins credential 생성      ‘3.2.1.’을 참고하여 Jenkins &gt; Credentials &gt; System &gt; Global credentials (unrestricted) &gt; Add Credentials 순서로 jenkins credential을 추가 생성합니다.   아래와 같이 설정하고, 아래의 ‘OK’ 버튼을 선택하여 저장합니다.            Kind :: Username with password       Scope :: Global (Jenkins, nodes, items, all child items, etc)       Username :: {GITHUB ACCOUNT NAME}       Password :: {GITHUB ACCOUNT PASSWORD}       ID :: lindarex-github-account       Description :: lindarex github account              Username과 Password는 github 계정 정보입니다.       Username은 github 로그인 시 입력하는 이메일 주소가 아닌, github profile의 ‘Name’ 값입니다.       ID와 Description은 임의로 입력합니다.          생성한 jenkins credential을 확인합니다.      3.3. github server 설정      jenkins 첫 페이지에서 왼쪽의 ‘Jenkins 관리’ 메뉴를 선택합니다.         ‘시스템 설정’ 메뉴를 선택합니다.         GitHub 설정 영역의 ‘Add GitHub Server’를 선택합니다.         아래와 같이 설정하고, 아래의 ‘저장’ 버튼을 선택하여 저장합니다.            Name :: lindarex-github-server       API URL :: https://api.github.com       Credentials :: lindarex-github-access-token       Manage hooks :: 체크              Name은 임의로 입력합니다.       Credentials는 github personal access token을 위해 생성한 jenkins credential 값입니다.       3.4. jenkins job 설정      jenkins 첫 페이지에서 왼쪽의 ‘새로운 Item’ 메뉴를 선택합니다.         아래와 같이 설정하고, 아래의 ‘OK’ 버튼을 선택하여 새로운 item(job)을 생성합니다.            Item name :: github-webhook       Freestyle project 선택              Item name은 임의로 입력합니다.          ‘소스 코드 관리’와 ‘빌드 유발’ 영역을 아래와 같이 설정하고, 아래의 ‘저장’ 버튼을 선택하여 저장합니다.            ‘소스 코드 관리’ &gt; ‘Git’ :: 체크                    Repositories &gt; Repository URL\t:: https://github.com/lindarex/lindarex-jenkins-webhook-test           Repositories &gt; Credentials :: lindarex-github-account           Branches to build &gt; Branch Specifier (blank for ‘any’) :: ‘*/master’           Repository browser :: ‘(자동)’                       ‘빌드 유발’ &gt; ‘GitHub hook trigger for GITScm polling’ :: 체크              Repositories &gt; Repository URL은 위에서 생성한 github repository 주소입니다.       Repositories &gt; Credentials는 github account를 위해 생성한 jenkins credential 값입니다.       4. github webhook 설정      위에서 생성한 github repository로 이동한 후, 오른쪽 위의 ‘Settings’ 메뉴를 선택합니다.         왼쪽의 ‘Webhooks’ 메뉴를 선택합니다.         오른쪽 위의 ‘Add webhook’ 버튼을 선택합니다.         아래와 같이 설정하고, 아래의 ‘Add webhook’ 버튼을 선택하여 저장합니다.            Payload URL :: [JENKINS_URL]/github-webhook/       Content type :: application/json              Payload URL 입력 시, ‘[JENKINS_URL]/github-webhook/’과 같이 마지막 ‘/’를 꼭 입력하시기 바랍니다.          저장 후 정상적으로 연동되면, 목록의 Payload URL 앞에 녹색 아이콘이 표시됩니다.      Payload URL 앞에 녹색 아이콘이 아니라면 오류가 발생한 것이며, Payload URL 오타 또는 github과 jenkins 간의 통신 오류가 원인입니다.       5. 테스트      위에서 생성한 github repository로 이동합니다.         테스트를 위해 github repository의 README.md 파일을 수정합니다.         ‘Commit changes’ 버튼을 선택하여 수정 사항을 적용(PUSH)합니다.         jenkins에 접속하면, 왼쪽 아래에 ‘빌드 실행 상태’에 github webhook 설정을 한 job이 실행하는 것을 확인할 수 있습니다.         빌드가 완료된 후 해당 job을 선택합니다.         왼쪽 아래의 ‘Build History’ 영역에서 완료한 빌드 번호를 선택한 후, ‘Console Output’을 선택합니다.         README.md 파일을 수정한 내역과 함께 정상적으로 github webhook이 연동된 것을 확인할 수 있습니다.      마무리(CONCLUSION)  github webhook으로 jenkins job을 실행(자동화)하는 방법을 소개했습니다.    현업에서는 github에서 제공하는 webhook을 사용해서 ‘Pull Request’ 발생에 따라 빌드뿐만 아니라 테스트, 코드 분석, 리뷰, 알람 등을 적용하여 개발 프로세스를 최대한 자동화합니다.    하지만 어디까지나 개발 서버와 연동하는 수준이며, 운영 서버는 보안 및 승인 등의 이유로 github webhook을 연동하는 경우는 거의 없습니다.    그리고 github webhook을 적용하기 위해서는 github와 jenkins 간의 통신이 가능해야 하므로 jenkins는 외부망에 위치해야 합니다. 그래서 내부망 또는 폐쇄망에서는 github webhook을 사용할 수 없는 단점이 있습니다.    하지만 상당수의 많은 프로젝트에서, 특히 DevOps를 지향하는 사이트에서는 자동화된 CI/CD(continuous integration/continuous delivery 또는 continuous deployment) 구축을 위해 github webhook 연동을 필수로 설정합니다.   참고(REFERENCES)     https://developer.github.com/webhooks/   https://devcenter.bitrise.io/webhooks/adding-a-github-webhook/   https://dzone.com/articles/adding-a-github-webhook-in-your-jenkins-pipeline  ","categories": ["jenkins"],
        "tags": ["jenkins","github","webhook","ci","continuous integration","open source software","open source","oss"],
        "url": "https://lindarex.github.io/jenkins/jenkins-github-webhook-setting/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 네임 서버(Name server) 설정하기",
        "excerpt":"네임 서버(name server)는 인터넷에서 도메인 네임 서비스(domain name service)를 제공하는 server를 의미합니다.    도메인으로 IP를 조회할 때, 로컬 캐시 조회, ‘/etc/hosts’ 조회, DNS 조회 순으로 진행되며, 이 중 ‘/etc/hosts’ 설정은 개별적으로 수정해야 하기 때문에 협업 시 불편하고 일괄 관리에 어려움이 있습니다.    이 포스트에서는 Ubuntu 환경에서 위 과정의 DNS 조회를 위한 name server를 설정하는 방법을 소개합니다.   선행조건(PREREQUISITE)     Ubuntu 환경이 필요합니다.   방화벽 설정이 필요합니다.            TCP 및 UDP 53 포트가 개방되어 있어야 합니다.              방화벽 설정 방법은 우분투(Ubuntu) 환경에 방화벽(Firewalld) 설치 및 설정하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VMware® Workstation 15 Pro (15.5.2 build-15785246)   Ubuntu 18.04.4 LTS (Bionic Beaver) Server (64-bit)   Ubuntu 16.04.6 LTS (Xenial Xerus) Server (64-bit)   요약(SUMMARY)     Ubuntu 16.04 환경에 name server 설정   Ubuntu 18.04 환경에 name server 설정   (선택사항) 네트워크 명령어로 DNS 확인   내용(CONTENTS)  1. Ubuntu 16.04 환경에 name server 설정  1.1. name server 조회     ‘/etc/resolv.conf’ 파일(file) 확인   $ cat /etc/resolv.conf   # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) #     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN nameserver 10.0.1.2 search localdomain      ‘/etc/resolv.conf’ file은 서버를 재부팅하면 설정이 초기화되기 때문에 조회 용도로만 사용합니다.    1.2. name server 설정     ‘/etc/resolvconf/resolv.conf.d/head’ file을 설정합니다.   $ sudo vi /etc/resolvconf/resolv.conf.d/head   # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) #     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN nameserver 10.0.1.32      위 스크립트는 name server(10.0.1.32)를 추가한 예제입니다.    1.3. resolvconf 서비스 재시작     위에서 설정한 name server를 적용하기 위해 resolvconf 서비스(service)를 재시작합니다.   $ sudo systemctl restart resolvconf      resolvconf service가 정상적으로 재시작되었는지 확인합니다.   $ sudo systemctl status resolvconf ● resolvconf.service - Nameserver information manager    Loaded: loaded (/lib/systemd/system/resolvconf.service; enabled; vendor preset: enabled)    Active: active (exited) since Mon 2020-05-11 15:25:52 KST; 8s ago      Docs: man:resolvconf(8)   Process: 2586 ExecStop=/sbin/resolvconf --disable-updates (code=exited, status=0/SUCCESS)   Process: 2602 ExecStart=/sbin/resolvconf --enable-updates (code=exited, status=0/SUCCESS)   Process: 2596 ExecStartPre=/bin/touch /run/resolvconf/postponed-update (code=exited, status=0/SUCCESS)   Process: 2593 ExecStartPre=/bin/mkdir -p /run/resolvconf/interface (code=exited, status=0/SUCCESS)  Main PID: 2602 (code=exited, status=0/SUCCESS)  May 11 15:25:52 lindarex systemd[1]: Stopped Nameserver information manager. May 11 15:25:52 lindarex systemd[1]: Starting Nameserver information manager... May 11 15:25:52 lindarex systemd[1]: Started Nameserver information manager.   1.4. 설정된 name server 조회     ‘/etc/resolv.conf’ file을 확인합니다.   $ cat /etc/resolv.conf   # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) #     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN nameserver 10.0.1.32 nameserver 10.0.1.2 search localdomain   2. Ubuntu 18.04 환경에 name server 설정  2.1. name server 조회     ‘/etc/resolv.conf’ file을 확인합니다.   $ cat /etc/resolv.conf   # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) #     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN # 127.0.0.53 is the systemd-resolved stub resolver. # run \"systemd-resolve --status\" to see details about the actual nameservers.  nameserver 127.0.0.53 search localdomain options edns0      systemd-resolve command로 확인합니다.   $ sudo systemd-resolve --status Global           DNSSEC NTA: 10.in-addr.arpa                       16.172.in-addr.arpa                       168.192.in-addr.arpa                       17.172.in-addr.arpa                       18.172.in-addr.arpa                       19.172.in-addr.arpa                       20.172.in-addr.arpa                       21.172.in-addr.arpa                       22.172.in-addr.arpa                       23.172.in-addr.arpa                       24.172.in-addr.arpa                       25.172.in-addr.arpa                       26.172.in-addr.arpa                       27.172.in-addr.arpa                       28.172.in-addr.arpa                       29.172.in-addr.arpa                       30.172.in-addr.arpa                       31.172.in-addr.arpa                       corp                       d.f.ip6.arpa                       home                       internal                       intranet                       lan                       local                       private                       test  Link 2 (ens33)       Current Scopes: DNS        LLMNR setting: yes MulticastDNS setting: no       DNSSEC setting: no     DNSSEC supported: no          DNS Servers: 10.0.1.2           DNS Domain: localdomain   2.2. resolvconf 패키지(package) 설치     Ubuntu 18.04에 resolvconf package를 설치하여 Ubuntu 16.04와 동일한 방법으로 name server를 설정합니다.   $ sudo apt install resolvconf -y      resolvconf package 설치 완료 후 resolvconf service 상태를 조회합니다.   $ sudo systemctl status resolvconf ● resolvconf.service - Nameserver information manager    Loaded: loaded (/lib/systemd/system/resolvconf.service; enabled; vendor preset: enabled)    Active: inactive (dead)      Docs: man:resolvconf(8)   2.3. name server 설정     ‘/etc/resolvconf/resolv.conf.d/head’ file을 설정합니다.   $ sudo vi /etc/resolvconf/resolv.conf.d/head   # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) #     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN # 127.0.0.53 is the systemd-resolved stub resolver. # run \"systemd-resolve --status\" to see details about the actual nameservers. nameserver 10.0.1.32      위 스크립트는 name server(10.0.1.32)를 추가한 예제입니다.    2.4. resolvconf service 재시작     위에서 설정한 name server를 적용하기 위해 resolvconf service를 재시작합니다.   $ sudo systemctl restart resolvconf      resolvconf service가 정상적으로 재시작되었는지 확인합니다.   $ sudo systemctl status resolvconf ● resolvconf.service - Nameserver information manager    Loaded: loaded (/lib/systemd/system/resolvconf.service; enabled; vendor preset: enabled)    Active: active (exited) since Mon 2020-05-11 06:49:25 UTC; 1s ago      Docs: man:resolvconf(8)   Process: 15117 ExecStart=/sbin/resolvconf --enable-updates (code=exited, status=0/SUCCESS)   Process: 15107 ExecStartPre=/bin/touch /run/resolvconf/postponed-update (code=exited, status=0/SUCCESS)   Process: 15105 ExecStartPre=/bin/mkdir -p /run/resolvconf/interface (code=exited, status=0/SUCCESS)  Main PID: 15117 (code=exited, status=0/SUCCESS)  May 11 06:49:25 lindarex systemd[1]: Starting Nameserver information manager... May 11 06:49:25 lindarex systemd[1]: Started Nameserver information manager.   2.5. 설정된 name server 조회     ‘/etc/resolv.conf’ file을 확인합니다.   $ cat /etc/resolv.conf   # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) #     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN # 127.0.0.53 is the systemd-resolved stub resolver. # run \"systemd-resolve --status\" to see details about the actual nameservers.  nameserver 10.0.1.32 nameserver 127.0.0.53 search localdomain options edns0      systemd-resolve command로 확인합니다.   $ sudo systemd-resolve --status Global          DNS Servers: 10.0.1.32           DNS Domain: localdomain           DNSSEC NTA: 10.in-addr.arpa                       16.172.in-addr.arpa                       168.192.in-addr.arpa                       17.172.in-addr.arpa                       18.172.in-addr.arpa                       19.172.in-addr.arpa                       20.172.in-addr.arpa                       21.172.in-addr.arpa                       22.172.in-addr.arpa                       23.172.in-addr.arpa                       24.172.in-addr.arpa                       25.172.in-addr.arpa                       26.172.in-addr.arpa                       27.172.in-addr.arpa                       28.172.in-addr.arpa                       29.172.in-addr.arpa                       30.172.in-addr.arpa                       31.172.in-addr.arpa                       corp                       d.f.ip6.arpa                       home                       internal                       intranet                       lan                       local                       private                       test  Link 2 (ens33)       Current Scopes: DNS        LLMNR setting: yes MulticastDNS setting: no       DNSSEC setting: no     DNSSEC supported: no          DNS Servers: 10.0.1.2           DNS Domain: localdomain   3. (선택사항) 네트워크 명령어로 DNS 확인     아래 command는 DNS 서버 설정이 되어 있는 VM(10.0.1.32)이 있다는 가정하에 실행합니다.      bind9 설치 방법은 우분투(Ubuntu) 환경에 패키지(Package)로 BIND(BIND9) 설치하기 포스트를 참고하시기 바랍니다.    $ nslookup ns.lindarex.local Server:   10.0.1.32 Address:  10.0.1.32#53  Name: ns.lindarex.local Address: 10.0.1.32   $ nslookup www.lindarex.local Server:   10.0.1.32 Address:  10.0.1.32#53  Name: www.lindarex.local Address: 10.0.1.32   $ nslookup 10.0.1.32 Server:   10.0.1.32 Address:  10.0.1.32#53  32.1.0.10.in-addr.arpa  name = ns.lindarex.local. 32.1.0.10.in-addr.arpa  name = www.lindarex.local.   $ dig ns.lindarex.local  ; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Ubuntu &lt;&lt;&gt;&gt; ns.lindarex.local ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 32791 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 1, ADDITIONAL: 1  ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;ns.lindarex.local.   IN  A  ;; ANSWER SECTION: ns.lindarex.local.  86400 IN  A 10.0.1.32  ;; AUTHORITY SECTION: lindarex.local.   86400 IN  NS  ns.lindarex.local.  ;; Query time: 0 msec ;; SERVER: 10.0.1.32#53(10.0.1.32) ;; WHEN: Mon May 11 16:29:44 KST 2020 ;; MSG SIZE  rcvd: 76   $ dig www.lindarex.local  ; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Ubuntu &lt;&lt;&gt;&gt; www.lindarex.local ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 59631 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 1, ADDITIONAL: 2  ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;www.lindarex.local.    IN  A  ;; ANSWER SECTION: www.lindarex.local. 86400 IN  A 10.0.1.32  ;; AUTHORITY SECTION: lindarex.local.   86400 IN  NS  ns.lindarex.local.  ;; ADDITIONAL SECTION: ns.lindarex.local.  86400 IN  A 10.0.1.32  ;; Query time: 0 msec ;; SERVER: 10.0.1.32#53(10.0.1.32) ;; WHEN: Mon May 11 16:30:44 KST 2020 ;; MSG SIZE  rcvd: 96   $ dig -x 10.0.1.32  ; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Ubuntu &lt;&lt;&gt;&gt; -x 10.0.1.32 ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 18085 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 1  ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;32.1.0.10.in-addr.arpa.    IN  PTR  ;; ANSWER SECTION: 32.1.0.10.in-addr.arpa. 86400 IN  PTR www.lindarex.local. 32.1.0.10.in-addr.arpa. 86400 IN  PTR ns.lindarex.local.  ;; AUTHORITY SECTION: 1.0.10.in-addr.arpa.  86400 IN  NS  ns.  ;; Query time: 0 msec ;; SERVER: 10.0.1.32#53(10.0.1.32) ;; WHEN: Mon May 11 16:30:20 KST 2020 ;; MSG SIZE  rcvd: 116   마무리(CONCLUSION)  Ubuntu 환경에 DNS 조회를 위한 name server 설정을 완료했습니다.    내부 네트워크(network)의 name server 설정으로 network 관리가 용이하고 보안을 강화할 수 있습니다.   개발자 입장에서는 name server 설정을 통해 host file 변경 없이 내부 도메인을 사용할 수 있고, IP 변경에 신경 쓰지 않아도 되는 이점이 있습니다.    현업에서는 name server를 주로 보안을 위해 사용하기 때문에, 개발자가 크게 신경 쓸 부분이 아닙니다. 하지만, DNS, name server에 대한 기본 개념은 이해하고 있어야 합니다.    더 자세한 내용은 아래 참고 페이지를 확인해 주시기 바랍니다.   참고(REFERENCES)     https://datawookie.netlify.app/blog/2018/10/dns-on-ubuntu-18.04/   https://www.tecmint.com/set-permanent-dns-nameservers-in-ubuntu-debian/   https://wiki.archlinux.org/index.php/Domain_name_resolution  ","categories": ["ubuntu"],
        "tags": ["nameserver","name server","dns","ubuntu"],
        "url": "https://lindarex.github.io/ubuntu/ubuntu-nameserver-setting/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 패키지(Package)로 Fail2ban 설치하기",
        "excerpt":"Fail2ban(fail2ban)은 침입 차단 소프트웨어 프레임워크(framework)로, 파이썬(python) 언어로 개발되었습니다.   로그(log) 파일과 iptables를 이용하여 접속 시도를 확인하고 차단하여 무차별 대입 공격으로부터 시스템을 보호합니다.   패킷 제어 시스템이나 로컬 방화벽(iptables 또는 TCP wrapper)과의 인터페이스를 갖는 POSIX 시스템에서 사용할 수 있으며, GNU General Public License(GNU GPL 또는 GPL) v2+가 적용된 오픈소스(open source) 소프트웨어로 배포(release)됩니다.   이 포스트에서는 우분투(ubuntu) 환경에서 package로 fail2ban을 설치하는 방법을 소개합니다.      침입 차단 소프트웨어(Intrusion Prevention Systems (IPS), Intrusion Detection and Prevention Systems (IDPS))에 대한 자세한 정보는 https://ko.wikipedia.org/wiki/침입_차단_시스템을 확인해 주시기 바랍니다.       무차별 대입 공격(brute-force attack)에 대한 자세한 정보는 https://ko.wikipedia.org/wiki/무차별_대입_공격을 확인해 주시기 바랍니다.       iptables에 대한 자세한 정보는 https://ko.wikipedia.org/wiki/Iptables를 확인해 주시기 바랍니다.       TCP wrapper에 대한 자세한 정보는 https://ko.wikipedia.org/wiki/TCP_래퍼를 확인해 주시기 바랍니다.       POSIX 시스템에 대한 자세한 정보는 https://ko.wikipedia.org/wiki/POSIX를 확인해 주시기 바랍니다.    선행조건(PREREQUISITE)     ubuntu 환경이 필요합니다.      ubuntu 설치 방법은 우분투(Ubuntu) 서버(Server) 16.04 설치하기 또는 우분투(Ubuntu) 서버(Server) 18.04 설치하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     AWS EC2 Instance Ubuntu Server 18.04 LTS (HVM), SSD Volume Type   Ubuntu 18.04.5 LTS (GNU/Linux 5.4.0-1029-aws x86_64)   Fail2Ban v0.10.2   요약(SUMMARY)     apt 명령어로 fail2ban 설치   fail2ban 설치 확인   systemctl 명령어로 fail2ban 서비스 관리   (선택사항) apt 명령어로 fail2ban 삭제   fail2ban 설정   fail2ban 사용 방법   내용(CONTENTS)  1. apt 명령어로 fail2ban 설치  $ sudo apt update &amp;&amp; sudo apt install fail2ban -y   2. fail2ban 설치 확인  $ fail2ban-client -V Fail2Ban v0.10.2  Copyright (c) 2004-2008 Cyril Jaquier, 2008- Fail2Ban Contributors Copyright of modifications held by their respective authors. Licensed under the GNU General Public License v2 (GPL).   3. systemctl 명령어로 fail2ban 서비스(service) 관리  3.1. fail2ban service 설정 반영  $ sudo systemctl daemon-reload   3.2. fail2ban service 시작  $ sudo systemctl start fail2ban.service   3.3. fail2ban service 중지  $ sudo systemctl stop fail2ban.service   3.4. fail2ban service 재시작  $ sudo systemctl restart fail2ban.service   3.5. fail2ban service 설정 재적용  $ sudo systemctl reload fail2ban.service   3.6. fail2ban service 상태 조회  $ sudo systemctl status fail2ban.service   3.7. fail2ban service 활성화(부팅 시 자동 시작)  $ sudo systemctl enable fail2ban.service   3.8. fail2ban service 비활성화  $ sudo systemctl disable fail2ban.service   3.9. fail2ban service 및 관련 프로세스 모두 중지  $ sudo systemctl kill fail2ban.service   4. (선택사항) apt 명령어로 fail2ban 삭제     ’–auto-remove’ 옵션을 추가하면, 사용하지 않는 관련 package를 모두 삭제합니다.   4.1. apt remove 명령어로 fail2ban 삭제     설정 파일을 유지하며 fail2ban을 삭제합니다.   $ sudo apt remove fail2ban* $ sudo apt remove --auto-remove fail2ban*   4.2. apt purge 명령어로 fail2ban 삭제     설정 파일과 함께 fail2ban을 삭제합니다. (단, 사용자 홈 디렉터리의 설정 파일은 유지됩니다.)   $ sudo apt purge fail2ban* $ sudo apt purge --auto-remove fail2ban*   5. fail2ban 설정  $ sudo vi /etc/fail2ban/jail.conf   -------------------------------------------------------------------------------- [DEFAULT] # 차단 예외 IPs, 추가 시 스페이스로 구분 ignoreip = 127.0.0.1/8  # 접속 차단 시간, 1분 = 60, 아래 예제는 1일, -1 설정 시 영구 차단 bantime = 86400  # 설정 시간 내 maxretry 설정만큼 접속 실패 시 차단 findtime = 86400  # 최대 허용 횟수 maxretry = 3  # 차단 방법, firewalld 사용 시 'firewallcmd-new', iptables 사용 시 'iptables-multiport' banaction = iptables-multiport   [sshd] enabled = true --------------------------------------------------------------------------------      ‘/etc/fail2ban/jail.conf’ 파일은 업데이트 시 초기화되므로, ‘/etc/fail2ban/jail.d/*.conf’ 또는 ‘/etc/fail2ban/jail.local’ 파일 생성 후 설정하는 것을 추천합니다.       설정 반영을 위해 아래 명령어로 fail2ban을 재시작합니다.   $ sudo systemctl restart fail2ban.service   6. fail2ban 사용 방법  6.1. 차단 목록 조회  $ sudo fail2ban-client status   Status |- Number of jail:\t1 `- Jail list:\tsshd   $ sudo fail2ban-client status sshd   Status for the jail: sshd |- Filter |  |- Currently failed:\t3 |  |- Total failed:\t23 |  `- File list:\t/var/log/auth.log `- Actions    |- Currently banned:\t0    |- Total banned:\t0    `- Banned IP list:   6.2. 차단 설정  $ sudo fail2ban-client set ${JAIL_NAME} banip ${차단 IP}   $ sudo fail2ban-client set sshd banip 192.168.0.11   192.168.0.11   6.3. 차단 해제  $ sudo fail2ban-client set ${JAIL_NAME} unbanip ${차단 해제 IP}   $ sudo fail2ban-client set sshd unbanip 192.168.0.11   192.168.0.11   6.4. log 조회  $ cat /var/log/fail2ban.log   2020-10-29 05:15:09,537 fail2ban.server         [1588]: INFO    -------------------------------------------------- 2020-10-29 05:15:09,537 fail2ban.server         [1588]: INFO    Starting Fail2ban v0.10.2 2020-10-29 05:15:09,548 fail2ban.database       [1588]: INFO    Connected to fail2ban persistent database '/var/lib/fail2ban/fail2ban.sqlite3' 2020-10-29 05:15:09,552 fail2ban.database       [1588]: WARNING New database created. Version '2' 2020-10-29 05:15:09,553 fail2ban.jail           [1588]: INFO    Creating new jail 'sshd' 2020-10-29 05:15:09,584 fail2ban.jail           [1588]: INFO    Jail 'sshd' uses pyinotify {} 2020-10-29 05:15:09,587 fail2ban.jail           [1588]: INFO    Initiated 'pyinotify' backend 2020-10-29 05:15:09,588 fail2ban.filter         [1588]: INFO      maxLines: 1 2020-10-29 05:15:09,614 fail2ban.server         [1588]: INFO    Jail sshd is not a JournalFilter instance 2020-10-29 05:15:09,615 fail2ban.filter         [1588]: INFO    Added logfile: '/var/log/auth.log' (pos = 0, hash = 25791bcc640f76a6b3ead17d55a5f0e66465461c) 2020-10-29 05:15:09,619 fail2ban.filter         [1588]: INFO      encoding: UTF-8 2020-10-29 05:15:09,619 fail2ban.filter         [1588]: INFO      maxRetry: 5 2020-10-29 05:15:09,619 fail2ban.filter         [1588]: INFO      findtime: 600 2020-10-29 05:15:09,619 fail2ban.actions        [1588]: INFO      banTime: 600 2020-10-29 05:15:09,621 fail2ban.jail           [1588]: INFO    Jail 'sshd' started 2020-10-29 05:15:27,281 fail2ban.filter         [1588]: INFO    [sshd] Found 193.112.23.105 - 2020-10-29 05:10:15 2020-10-29 05:15:27,281 fail2ban.filter         [1588]: INFO    [sshd] Found 159.89.1.242 - 2020-10-29 05:10:18 2020-10-29 05:15:27,281 fail2ban.filter         [1588]: INFO    [sshd] Found 51.15.94.14 - 2020-10-29 05:14:53 2020-10-29 05:15:27,282 fail2ban.filter         [1588]: INFO    [sshd] Found 122.51.238.27 - 2020-10-29 05:15:27 2020-10-29 05:26:15,805 fail2ban.filter         [1588]: INFO    [sshd] Found 141.98.10.211 - 2020-10-29 05:26:15 2020-10-29 05:26:19,199 fail2ban.filter         [1588]: INFO    [sshd] Found 141.98.10.212 - 2020-10-29 05:26:19 2020-10-29 05:26:26,824 fail2ban.filter         [1588]: INFO    [sshd] Found 141.98.10.214 - 2020-10-29 05:26:26 2020-10-29 05:26:31,431 fail2ban.filter         [1588]: INFO    [sshd] Found 141.98.10.209 - 2020-10-29 05:26:31 2020-10-29 05:26:35,751 fail2ban.filter         [1588]: INFO    [sshd] Found 141.98.10.210 - 2020-10-29 05:26:35 2020-10-29 05:26:39,393 fail2ban.filter         [1588]: INFO    [sshd] Found 141.98.10.211 - 2020-10-29 05:26:39 2020-10-29 05:26:49,698 fail2ban.filter         [1588]: INFO    [sshd] Found 141.98.10.213 - 2020-10-29 05:26:49 2020-10-29 05:26:55,779 fail2ban.filter         [1588]: INFO    [sshd] Found 141.98.10.214 - 2020-10-29 05:26:55 2020-10-29 05:26:59,919 fail2ban.filter         [1588]: INFO    [sshd] Found 141.98.10.209 - 2020-10-29 05:26:59 2020-10-29 05:27:29,170 fail2ban.filter         [1588]: INFO    [sshd] Found 200.73.128.64 - 2020-10-29 05:27:29 2020-10-29 05:27:49,164 fail2ban.filter         [1588]: INFO    [sshd] Found 137.74.5.81 - 2020-10-29 05:27:49 2020-10-29 05:35:28,107 fail2ban.filter         [1588]: INFO    [sshd] Found 91.74.129.82 - 2020-10-29 05:35:28 2020-10-29 05:36:05,060 fail2ban.filter         [1588]: INFO    [sshd] Found 179.184.0.112 - 2020-10-29 05:36:04 2020-10-29 05:36:49,781 fail2ban.filter         [1588]: INFO    [sshd] Found 161.35.200.233 - 2020-10-29 05:36:49 2020-10-29 05:41:38,714 fail2ban.filter         [1588]: INFO    [sshd] Found 60.248.199.194 - 2020-10-29 05:41:38 2020-10-29 05:41:45,894 fail2ban.filter         [1588]: INFO    [sshd] Found 151.80.176.191 - 2020-10-29 05:41:45 2020-10-29 05:46:13,683 fail2ban.filter         [1588]: INFO    [sshd] Found 122.51.130.21 - 2020-10-29 05:46:13 2020-10-29 05:52:53,319 fail2ban.filter         [1588]: INFO    [sshd] Found 51.83.41.120 - 2020-10-29 05:52:48 2020-10-29 05:55:49,062 fail2ban.filter         [1588]: INFO    [sshd] Found 161.35.162.11 - 2020-10-29 05:55:49 2020-10-29 06:14:12,583 fail2ban.actions        [1588]: NOTICE  [sshd] Ban 192.168.0.11 2020-10-29 06:15:31,272 fail2ban.actions        [1588]: NOTICE  [sshd] Unban 192.168.0.11   마무리(CONCLUSION)  ubuntu 환경에 package로 fail2ban 설치를 완료했습니다.   fail2ban은 설치만으로 데몬(daemon)으로 실행되면서 기본 설정까지 완료됩니다.   fail2ban은 IPv6 미지원과 분산 무차별 대입 공격에 취약한 단점이 있지만, 기본 설정으로 대부분의 brute force 공격을 막을 수 있습니다.   기본 설정에는 Apache HTTP Server, Lighttpd, sshd, vsftpd, qmail, Postfix 그리고 Courier 메일 서버를 위한 필터가 제공됩니다.   필터는 python 정규식으로 정의되며, 관리자는 필터를 커스터마이징 하여 더 강력한 보안 설정을 할 수 있습니다.   참고(REFERENCES)     https://ko.wikipedia.org/wiki/Fail2ban   https://www.fail2ban.org/   http://www.fail2ban.org/wiki/index.php/Fail2Ban  ","categories": ["fail2ban"],
        "tags": ["fail2ban","firewall","open source software","open source","oss","ubuntu"],
        "url": "https://lindarex.github.io/fail2ban/ubuntu-fail2ban-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"},{
        "title": "우분투(Ubuntu) 환경에 패키지(Package)로 SCM Manager v2 설치하기",
        "excerpt":"SCM Manager v2(이하 scm manager)는 개별적으로 확장 가능한 가벼운 소스 코드 관리 도구로, MIT 라이선스(license)가 적용된 오픈소스(open source) 소프트웨어입니다.    scm manager는 다양한 플러그인과 Level 3 RESTful WebService를 제공하고, Git과 Mercurial, Subversion 저장소(repository)를 지원합니다.    scm manager는 웹 서버(web server), 데이터베이스(database) 등의 종속성이 없고, 다양한 환경(Ubuntu, CentOS, Fedora, Windows, MacOS X, Docker, Kubernetes)에 설치할 수 있습니다.    이 포스트에서는 우분투(ubuntu) 환경에 package로 scm manager를 설치하는 방법을 소개합니다.   선행조건(PREREQUISITE)     ubuntu 환경이 필요합니다.   방화벽 설정이 필요합니다.            TCP 8080 포트가 개방되어 있어야 합니다.              ubuntu 설치 방법은 우분투(Ubuntu) 서버(Server) 16.04 설치하기 또는 우분투(Ubuntu) 서버(Server) 18.04 설치하기 포스트를 참고하시기 바랍니다.       방화벽 설정 방법은 우분투(Ubuntu) 환경에 방화벽(Firewalld) 설치 및 설정하기 포스트를 참고하시기 바랍니다.    테스트 환경(TEST ENVIRONMENT)     VULTR High Frequency Cloud Compute (256 GB NVMe, 3 CPU, 8192MB Memory, 4000GB Bandwidth)   Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-136-generic x86_64)   SCM Manager 2.14.1   요약(SUMMARY)     scm manager debian packages repository 설정   apt install 명령어로 scm manager 설치   (선택사항) scm manager 설정   systemctl 명령어로 scm manager 관리   웹브라우저로 scm manager 접속   내용(CONTENTS)  1. scm manager debian packages repository key 추가  $ sudo apt-key adv --recv-keys --keyserver hkps://keys.openpgp.org 0x975922F193B07D6E Executing: /tmp/apt-key-gpghome.mDvF5MxIa1/gpg.1.sh --recv-keys --keyserver hkps://keys.openpgp.org 0x975922F193B07D6E gpg: key 975922F193B07D6E: \"SCM Packages (signing key for packages.scm-manager.org) &lt;scm-team@cloudogu.com&gt;\" not changed gpg: Total number processed: 1 gpg:              unchanged: 1   2. scm manager debian packages repository 추가  $ echo 'deb [arch=all] https://packages.scm-manager.org/repository/apt-v2-releases/ stable main' | sudo tee /etc/apt/sources.list.d/scm-manager.list   3. apt install 명령어로 scm manager 설치  3.1. package 업데이트  $ sudo apt-get update   3.2. scm manager 설치  $ sudo apt-get install scm-server -y   4. (선택사항) scm manager 설정      scm manager UI의 포트를 설정합니다.   $ sudo vi /etc/default/scm-server   ---------------------------------------------------------------------------------------------------- PORT=8080 ----------------------------------------------------------------------------------------------------   5. systemctl 명령어로 scm manager 관리  5.1. scm manager 설정 반영  $ sudo systemctl daemon-reload   5.2. scm manager 시작  $ sudo systemctl start scm-server   5.3. scm manager 중지  $ sudo systemctl stop scm-server   5.4. scm manager 재시작  $ sudo systemctl restart scm-server   5.5. scm manager 설정 재적용  $ sudo systemctl reload scm-server   5.6. scm manager 상태 조회  $ sudo systemctl status scm-server   5.7. scm manager 활성화(부팅 시 자동 시작)  $ sudo systemctl enable scm-server   5.8. scm manager 비활성화  $ sudo systemctl disable scm-server   5.9. scm manager 및 관련 프로세스 모두 중지  $ sudo systemctl kill scm-server   6. 웹브라우저로 scm manager 접속     http://[MY-IP]:8080/scm      SCM Manager의 기본 계정은 ‘scmadmin’이며, 비밀번호는 ‘scmadmin’ 입니다.    마무리(CONCLUSION)  ubuntu 환경에 package로 scm manager 설치를 완료했습니다.    SCM Manager의 자세한 내용은 아래 페이지를 확인해 주시기 바랍니다.   참고(REFERENCES)     https://www.scm-manager.org/  ","categories": ["scm-manager"],
        "tags": ["scm manager","v2","version control","source code management","open source software","open source","oss","ubuntu"],
        "url": "https://lindarex.github.io/scm-manager/ubuntu-scm-manager-v2-installation/",
        "teaser":"https://lindarex.github.io/assets/images/LindaRex_LOGO.jpg"}]
